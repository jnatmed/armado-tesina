{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7389a834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Analizando dataset: SHUTTLE\n",
      "âŒ Error al analizar shuttle: \"None of [Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'], dtype='object')] are in the [columns]\"\n",
      "\n",
      "ðŸ” Analizando dataset: WDBC\n",
      "ðŸŽ¯ Valores Ãºnicos del target: ['B', 'M']\n",
      "ðŸ“Š DistribuciÃ³n de clases:\n",
      "   - B: 357 (62.74%)\n",
      "   - M: 212 (37.26%)\n",
      "âœ… Clase minoritaria real: M\n",
      "âš ï¸ Clase configurada como minoritaria: M\n",
      "\n",
      "ðŸ” Analizando dataset: GLASS\n",
      "ðŸŽ¯ Valores Ãºnicos del target: [2, 1, 7, 3, 5, 6]\n",
      "ðŸ“Š DistribuciÃ³n de clases:\n",
      "   - 2: 76 (35.51%)\n",
      "   - 1: 70 (32.71%)\n",
      "   - 7: 29 (13.55%)\n",
      "   - 3: 17 (7.94%)\n",
      "   - 5: 13 (6.07%)\n",
      "   - 6: 9 (4.21%)\n",
      "âœ… Clase minoritaria real: 6\n",
      "âš ï¸ Clase configurada como minoritaria: 6\n",
      "\n",
      "ðŸ” Analizando dataset: HEART\n",
      "ðŸŽ¯ Valores Ãºnicos del target: [0, 1, 2, 3, 4]\n",
      "ðŸ“Š DistribuciÃ³n de clases:\n",
      "   - 0: 164 (54.13%)\n",
      "   - 1: 55 (18.15%)\n",
      "   - 2: 36 (11.88%)\n",
      "   - 3: 35 (11.55%)\n",
      "   - 4: 13 (4.29%)\n",
      "âœ… Clase minoritaria real: 4\n",
      "âš ï¸ Clase configurada como minoritaria: 4\n",
      "\n",
      "ðŸ” Analizando dataset: IRIS\n",
      "ðŸŽ¯ Valores Ãºnicos del target: ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
      "ðŸ“Š DistribuciÃ³n de clases:\n",
      "   - Iris-setosa: 50 (33.33%)\n",
      "   - Iris-versicolor: 50 (33.33%)\n",
      "   - Iris-virginica: 50 (33.33%)\n",
      "âœ… Clase minoritaria real: Iris-setosa\n",
      "âš ï¸ Clase configurada como minoritaria: None\n",
      "â„¹ï¸ No se definiÃ³ clase minoritaria (modo multiclase o imagen).\n",
      "\n",
      "ðŸ” Analizando dataset: ECOLI\n",
      "ðŸŽ¯ Valores Ãºnicos del target: ['cp', 'im', 'pp', 'imU', 'om', 'omL', 'imL', 'imS']\n",
      "ðŸ“Š DistribuciÃ³n de clases:\n",
      "   - cp: 143 (42.56%)\n",
      "   - im: 77 (22.92%)\n",
      "   - pp: 52 (15.48%)\n",
      "   - imU: 35 (10.42%)\n",
      "   - om: 20 (5.95%)\n",
      "   - omL: 5 (1.49%)\n",
      "   - imL: 2 (0.6%)\n",
      "   - imS: 2 (0.6%)\n",
      "âœ… Clase minoritaria real: imL\n",
      "âš ï¸ Clase configurada como minoritaria: imL\n",
      "\n",
      "ðŸ“ AnÃ¡lisis guardado en: resultados/reporte_distribucion_2025-11-10_0103.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from config_datasets import config_datasets\n",
    "from cargar_dataset import cargar_dataset, graficar_distribucion_clases\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "# Crear carpeta de resultados si no existe\n",
    "Path(\"resultados\").mkdir(exist_ok=True)\n",
    "Path(\"figuras\").mkdir(exist_ok=True)\n",
    "\n",
    "# Nombre de archivo con fecha y hora\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "nombre_archivo = f\"resultados/reporte_distribucion_{timestamp}.txt\"\n",
    "\n",
    "# Lista de lÃ­neas para guardar en el archivo\n",
    "lineas_resultado = []\n",
    "\n",
    "for nombre, cfg in config_datasets.items():\n",
    "    lineas_resultado.append(f\"\\nðŸ” Analizando dataset: {nombre.upper()}\")\n",
    "    print(f\"\\nðŸ” Analizando dataset: {nombre.upper()}\")\n",
    "    try:\n",
    "        # Cargar el dataset (multiclase)\n",
    "        X, y, _ = cargar_dataset(\n",
    "            path=cfg[\"path\"],\n",
    "            clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "            col_features=cfg.get(\"col_features\"),\n",
    "            col_target=cfg.get(\"col_target\"),\n",
    "            sep=cfg.get(\"sep\", \",\"),\n",
    "            header=cfg.get(\"header\"),\n",
    "            binarizar=cfg.get(\"binarizar\", False),\n",
    "            tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "            dataset_name=cfg.get(\"dataset_name\")\n",
    "        )\n",
    "\n",
    "\n",
    "        # Contar clases originales\n",
    "        conteo = pd.Series(y).value_counts()\n",
    "        clase_min_real = conteo.idxmin()\n",
    "        total = conteo.sum()\n",
    "        proporcion = (conteo / total * 100).round(2)\n",
    "\n",
    "        # Mostrar\n",
    "        print(\"ðŸŽ¯ Valores Ãºnicos del target:\", list(conteo.index))\n",
    "        print(\"ðŸ“Š DistribuciÃ³n de clases:\")\n",
    "        # Guardar resultados\n",
    "        lineas_resultado.append(f\"ðŸŽ¯ Valores Ãºnicos del target: {list(conteo.index)}\")\n",
    "        lineas_resultado.append(\"ðŸ“Š DistribuciÃ³n de clases:\")\n",
    "\n",
    "        for clase, count in conteo.items():\n",
    "            print(f\"   - {clase}: {count} ({proporcion[clase]}%)\")\n",
    "            lineas_resultado.append(f\"   - {clase}: {count} ({proporcion[clase]}%)\")\n",
    "\n",
    "        print(f\"âœ… Clase minoritaria real: {clase_min_real}\")\n",
    "        print(f\"âš ï¸ Clase configurada como minoritaria: {cfg['clase_minoria']}\")\n",
    "        lineas_resultado.append(f\"âœ… Clase minoritaria real: {clase_min_real}\")\n",
    "        lineas_resultado.append(f\"âš ï¸ Clase configurada como minoritaria: {cfg.get('clase_minoria')}\")\n",
    "\n",
    "        if \"clase_minoria\" in cfg and cfg[\"clase_minoria\"] is not None:\n",
    "            if clase_min_real != cfg[\"clase_minoria\"]:\n",
    "                print(\"ðŸš¨ POSIBLE ERROR DE CONFIGURACIÃ“N â—\")\n",
    "                lineas_resultado.append(\"ðŸš¨ POSIBLE ERROR DE CONFIGURACIÃ“N â—\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No se definiÃ³ clase minoritaria (modo multiclase o imagen).\")\n",
    "            lineas_resultado.append(\"â„¹ï¸ No se definiÃ³ clase minoritaria (modo multiclase o imagen).\")\n",
    "\n",
    "        # ðŸ”½ Agregar grÃ¡fico descriptivo por dataset\n",
    "        nombre_figura = f\"figuras/{nombre.lower()}_distribucion_{timestamp}.png\"\n",
    "        graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=nombre_figura)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error al analizar {nombre}: {e}\")\n",
    "        lineas_resultado.append(f\"âŒ Error al analizar {nombre}: {e}\")\n",
    "\n",
    "# Guardar a archivo\n",
    "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lineas_resultado))\n",
    "\n",
    "print(f\"\\nðŸ“ AnÃ¡lisis guardado en: {nombre_archivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bcce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# --- Utilidades numÃ©ricas seguras ---\n",
    "def es_numerica(serie):\n",
    "    return np.issubdtype(serie.dtype, np.number)\n",
    "\n",
    "def obtener_matriz_correlacion_segura(df_numerico):\n",
    "    # Evita NaNs en correlaciÃ³n\n",
    "    df_sin_nan = df_numerico.fillna(df_numerico.median(numeric_only=True))\n",
    "    return df_sin_nan.corr().values, list(df_numerico.columns)\n",
    "\n",
    "# --- MÃ©tricas de calidad del dataset ---\n",
    "def calcular_metricas_basicas_dataframe(X_df, y_series):\n",
    "    metricas = {}\n",
    "\n",
    "    # Filtrado numÃ©rico\n",
    "    columnas_numericas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    X_num = X_df[columnas_numericas].copy()\n",
    "\n",
    "    # TamaÃ±os bÃ¡sicos\n",
    "    metricas[\"cantidad_muestras\"] = int(X_df.shape[0])\n",
    "    metricas[\"cantidad_atributos\"] = int(X_df.shape[1])\n",
    "\n",
    "    # Faltantes y duplicados\n",
    "    total_celdas = int(X_df.shape[0] * X_df.shape[1])\n",
    "    cantidad_faltantes = int(X_df.isnull().sum().sum())\n",
    "    metricas[\"porcentaje_faltantes\"] = float((cantidad_faltantes / total_celdas) * 100.0)\n",
    "\n",
    "    cantidad_duplicados = int(X_df.duplicated().sum())\n",
    "    metricas[\"porcentaje_duplicados\"] = float((cantidad_duplicados / X_df.shape[0]) * 100.0)\n",
    "\n",
    "    # DistribuciÃ³n de clases\n",
    "    valores_unicos, conteos = np.unique(y_series, return_counts=True)\n",
    "    cantidad_clases = int(len(valores_unicos))\n",
    "    indice_min = int(np.argmin(conteos))\n",
    "    indice_max = int(np.argmax(conteos))\n",
    "    clase_min = valores_unicos[indice_min]\n",
    "    clase_max = valores_unicos[indice_max]\n",
    "    n_min = int(conteos[indice_min])\n",
    "    n_max = int(conteos[indice_max])\n",
    "\n",
    "    metricas[\"cantidad_clases\"] = cantidad_clases\n",
    "    metricas[\"clase_minima_real\"] = str(clase_min)\n",
    "    metricas[\"clase_mayoritaria_real\"] = str(clase_max)\n",
    "    metricas[\"n_min\"] = n_min\n",
    "    metricas[\"n_max\"] = n_max\n",
    "    metricas[\"ratio_desequilibrio_max\"] = float(n_max / max(1, n_min))\n",
    "\n",
    "    # EntropÃ­a de clases y tamaÃ±o efectivo\n",
    "    proporciones = conteos / conteos.sum()\n",
    "    entropia = float(-(proporciones * np.log(proporciones + 1e-12)).sum())\n",
    "    metricas[\"entropia_clases\"] = entropia\n",
    "    metricas[\"tamano_efectivo_clases\"] = float(np.exp(entropia))  # Effective Number of Classes\n",
    "\n",
    "    # Rango de escalas por columna (para sugerir scaler)\n",
    "    rangos = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna()\n",
    "        if col.shape[0] > 0:\n",
    "            valor_min = float(np.min(col))\n",
    "            valor_max = float(np.max(col))\n",
    "            rango = float(valor_max - valor_min)\n",
    "            rangos.append(rango)\n",
    "    if len(rangos) > 0:\n",
    "        metricas[\"rango_mediano_variables\"] = float(np.median(rangos))\n",
    "        metricas[\"rango_maximo_variables\"] = float(np.max(rangos))\n",
    "    else:\n",
    "        metricas[\"rango_mediano_variables\"] = 0.0\n",
    "        metricas[\"rango_maximo_variables\"] = 0.0\n",
    "\n",
    "    # AsimetrÃ­a y curtosis agregadas\n",
    "    skew_acumulado = []\n",
    "    kurt_acumulado = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        if col.shape[0] > 3:\n",
    "            skew_acumulado.append(float(skew(col)))\n",
    "            kurt_acumulado.append(float(kurtosis(col, fisher=True)))\n",
    "    if len(skew_acumulado) > 0:\n",
    "        metricas[\"asimetria_mediana\"] = float(np.median(skew_acumulado))\n",
    "        metricas[\"curtosis_mediana\"] = float(np.median(kurt_acumulado))\n",
    "    else:\n",
    "        metricas[\"asimetria_mediana\"] = 0.0\n",
    "        metricas[\"curtosis_mediana\"] = 0.0\n",
    "\n",
    "    # Normalidad (Shapiro-Wilk sobre una muestra si N>5000)\n",
    "    # Devuelve porcentaje de variables con p>0.05 (no se rechaza normalidad)\n",
    "    porcentaje_normalidad = 0.0\n",
    "    variables_evaluadas = 0\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        tam_col = col.shape[0]\n",
    "        if tam_col > 3:\n",
    "            if tam_col > 5000:\n",
    "                # muestreo simple para evitar lÃ­mites de Shapiro\n",
    "                col = col.sample(5000, random_state=123).astype(float)\n",
    "            try:\n",
    "                estadistico, pvalor = shapiro(col.values)\n",
    "                variables_evaluadas += 1\n",
    "                if pvalor > 0.05:\n",
    "                    porcentaje_normalidad += 1.0\n",
    "            except Exception:\n",
    "                # si Shapiro falla en alguna variable, la omitimos\n",
    "                pass\n",
    "    if variables_evaluadas > 0:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = float((porcentaje_normalidad / variables_evaluadas) * 100.0)\n",
    "    else:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = 0.0\n",
    "\n",
    "    # CorrelaciÃ³n: mÃ¡ximo |r| y % de pares con |r| > 0.9\n",
    "    if X_num.shape[1] >= 2:\n",
    "        matriz_corr, nombres = obtener_matriz_correlacion_segura(X_num)\n",
    "        valores_superior = []\n",
    "        cantidad_altamente_correl = 0\n",
    "        total_pares = 0\n",
    "\n",
    "        for i in range(len(nombres)):\n",
    "            for j in range(i + 1, len(nombres)):\n",
    "                r = float(matriz_corr[i, j])\n",
    "                valores_superior.append(abs(r))\n",
    "                total_pares += 1\n",
    "                if abs(r) > 0.9:\n",
    "                    cantidad_altamente_correl += 1\n",
    "\n",
    "        if len(valores_superior) > 0:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = float(np.max(valores_superior))\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = float((cantidad_altamente_correl / total_pares) * 100.0)\n",
    "        else:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "    else:\n",
    "        metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "        metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "\n",
    "    # Sugerencia de escalado (reglas simples y explÃ­citas)\n",
    "    sugerencia_escalado = \"MinMaxScaler\"\n",
    "    if metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] > 60.0 and metricas[\"correlacion_absoluta_maxima\"] < 0.95:\n",
    "        sugerencia_escalado = \"StandardScaler\"\n",
    "    if metricas[\"asimetria_mediana\"] > 1.0 or metricas[\"curtosis_mediana\"] > 1.0:\n",
    "        sugerencia_escalado = \"RobustScaler\"\n",
    "    metricas[\"sugerencia_escalado\"] = sugerencia_escalado\n",
    "\n",
    "    return metricas\n",
    "\n",
    "# --- DetecciÃ³n de outliers por IQR ---\n",
    "def calcular_porcentaje_outliers_por_variable(X_df):\n",
    "    resultado = {}\n",
    "    for nombre_columna in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_columna]):\n",
    "            serie = X_df[nombre_columna].dropna().astype(float)\n",
    "            if serie.shape[0] > 0:\n",
    "                q1 = float(np.percentile(serie, 25))\n",
    "                q3 = float(np.percentile(serie, 75))\n",
    "                iqr = float(q3 - q1)\n",
    "                limite_inferior = q1 - 1.5 * iqr\n",
    "                limite_superior = q3 + 1.5 * iqr\n",
    "\n",
    "                cantidad = int(serie.shape[0])\n",
    "                cantidad_out = 0\n",
    "                indice = 0\n",
    "                valores = serie.values\n",
    "                while indice < cantidad:\n",
    "                    valor_actual = float(valores[indice])\n",
    "                    if valor_actual < limite_inferior or valor_actual > limite_superior:\n",
    "                        cantidad_out += 1\n",
    "                    indice += 1\n",
    "                porcentaje = float((cantidad_out / cantidad) * 100.0)\n",
    "                resultado[nombre_columna] = porcentaje\n",
    "    return resultado\n",
    "\n",
    "# --- GrÃ¡ficos simples y guardado ---\n",
    "def graficar_histograma_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    # Toma las primeras N numÃ©ricas para no explotar la figura\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    columnas = columnas[:max_columnas]\n",
    "    for nombre_col in columnas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        plt.hist(valores, bins=30)\n",
    "        plt.title(f\"{nombre_dataset} Â· Histograma: {nombre_col}\")\n",
    "        plt.xlabel(nombre_col)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        ruta_salida = f\"{ruta_base}/{nombre_dataset}_hist_{nombre_col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_boxplot_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas.append(c)\n",
    "    # Limitar cantidad para no generar demasiadas figuras\n",
    "    if len(columnas) > max_columnas:\n",
    "        columnas = columnas[:max_columnas]\n",
    "\n",
    "    for nombre_col in columnas:\n",
    "        # Preparar datos\n",
    "        serie = X_df[nombre_col].dropna()\n",
    "        valores = serie.values.astype(float)\n",
    "\n",
    "        # Crear figura\n",
    "        plt.figure()\n",
    "        # 1) NO pasamos labels/tick_labels (evita deprecations/errores entre versiones)\n",
    "        try:\n",
    "            plt.boxplot(valores, vert=True, showfliers=True)\n",
    "        except Exception as e:\n",
    "            # Fallback: dibujar un punto para no romper el loop\n",
    "            plt.plot([1], [valores[0] if valores.size>0 else 0], marker=\"o\")\n",
    "\n",
    "        # 2) Setear el tick X manualmente (compatible 100%)\n",
    "        plt.xticks([1], [nombre_col])\n",
    "\n",
    "        # 3) TÃ­tulo y guardado\n",
    "        plt.title(f\"{nombre_dataset} Â· Boxplot: {nombre_col}\")\n",
    "        ruta_salida = f\"{ruta_base}/{nombre_dataset}_box_{nombre_col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def graficar_mapa_correlaciones(X_df, nombre_dataset, ruta_base):\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    if len(columnas) < 2:\n",
    "        return\n",
    "    X_num = X_df[columnas].fillna(X_df[columnas].median(numeric_only=True))\n",
    "    matriz_corr = X_num.corr().values\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(matriz_corr, aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(columnas)), columnas, rotation=90)\n",
    "    plt.yticks(np.arange(len(columnas)), columnas)\n",
    "    plt.title(f\"{nombre_dataset} Â· Matriz de correlaciones (Pearson)\")\n",
    "    plt.tight_layout()\n",
    "    ruta_salida = f\"{ruta_base}/{nombre_dataset}_correlaciones.png\"\n",
    "    plt.savefig(ruta_salida, dpi=140)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1268d60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: SHUTTLE\n",
      "âŒ Error al analizar shuttle: \"None of [Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'], dtype='object')] are in the [columns]\"\n",
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: WDBC\n",
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: GLASS\n",
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: HEART\n",
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: IRIS\n",
      "â„¹ï¸ No se definiÃ³ clase minoritaria (modo multiclase o imagen).\n",
      "\n",
      "ðŸ”Ž AnÃ¡lisis extendido: ECOLI\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ruta_metricas_csv = f\"resultados/metricas_eda_{timestamp}.csv\"\n",
    "encabezados_csv = [\n",
    "    \"dataset\",\"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "    \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "    \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "    \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "    \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "    \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\"\n",
    "]\n",
    "with open(ruta_metricas_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "    escritor = csv.writer(fcsv)\n",
    "    escritor.writerow(encabezados_csv)\n",
    "\n",
    "for nombre, cfg in config_datasets.items():\n",
    "    lineas_resultado.append(f\"\\nðŸ”Ž AnÃ¡lisis extendido: {nombre.upper()}\")\n",
    "    print(f\"\\nðŸ”Ž AnÃ¡lisis extendido: {nombre.upper()}\")\n",
    "    try:\n",
    "        # === Cargar dataset (tu misma funciÃ³n) ===\n",
    "        X, y, _ = cargar_dataset(\n",
    "            path=cfg[\"path\"],\n",
    "            clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "            col_features=cfg.get(\"col_features\"),\n",
    "            col_target=cfg.get(\"col_target\"),\n",
    "            sep=cfg.get(\"sep\", \",\"),\n",
    "            header=cfg.get(\"header\"),\n",
    "            binarizar=cfg.get(\"binarizar\", False),\n",
    "            tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "            dataset_name=cfg.get(\"dataset_name\")\n",
    "        )\n",
    "\n",
    "        # === Reporte de clases (ya lo haces) ===\n",
    "        conteo = pd.Series(y).value_counts()\n",
    "        clase_min_real = conteo.idxmin()\n",
    "        total = int(conteo.sum())\n",
    "        proporcion = (conteo / total * 100).round(2)\n",
    "\n",
    "        lineas_resultado.append(f\"ðŸŽ¯ Valores Ãºnicos del target: {list(conteo.index)}\")\n",
    "        lineas_resultado.append(\"ðŸ“Š DistribuciÃ³n de clases:\")\n",
    "        for clase, count in conteo.items():\n",
    "            lineas_resultado.append(f\"   - {clase}: {int(count)} ({float(proporcion[clase])}%)\")\n",
    "\n",
    "        lineas_resultado.append(f\"âœ… Clase minoritaria real: {clase_min_real}\")\n",
    "        lineas_resultado.append(f\"âš ï¸ Clase configurada como minoritaria: {cfg.get('clase_minoria')}\")\n",
    "\n",
    "        if \"clase_minoria\" in cfg and cfg[\"clase_minoria\"] is not None:\n",
    "            if clase_min_real != cfg[\"clase_minoria\"]:\n",
    "                aviso = \"ðŸš¨ POSIBLE ERROR DE CONFIGURACIÃ“N â—\"\n",
    "                print(aviso)\n",
    "                lineas_resultado.append(aviso)\n",
    "        else:\n",
    "            info = \"â„¹ï¸ No se definiÃ³ clase minoritaria (modo multiclase o imagen).\"\n",
    "            print(info)\n",
    "            lineas_resultado.append(info)\n",
    "\n",
    "        # === MÃ©tricas de calidad globales ===\n",
    "        X_df = pd.DataFrame(X, columns=[f\"col_{i}\" for i in range(X.shape[1])]) if not isinstance(X, pd.DataFrame) else X.copy()\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        metricas = calcular_metricas_basicas_dataframe(X_df, y_series)\n",
    "        lineas_resultado.append(f\"ðŸ§ª MÃ©tricas clave: {metricas}\")\n",
    "\n",
    "        # Guardar mÃ©tricas al CSV\n",
    "        with open(ruta_metricas_csv, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "            escritor = csv.writer(fcsv)\n",
    "            fila = [\n",
    "                nombre,\n",
    "                metricas[\"cantidad_muestras\"],\n",
    "                metricas[\"cantidad_atributos\"],\n",
    "                metricas[\"cantidad_clases\"],\n",
    "                metricas[\"clase_minima_real\"],\n",
    "                metricas[\"clase_mayoritaria_real\"],\n",
    "                metricas[\"n_min\"],\n",
    "                metricas[\"n_max\"],\n",
    "                round(metricas[\"ratio_desequilibrio_max\"], 4),\n",
    "                round(metricas[\"entropia_clases\"], 4),\n",
    "                round(metricas[\"tamano_efectivo_clases\"], 4),\n",
    "                round(metricas[\"porcentaje_faltantes\"], 2),\n",
    "                round(metricas[\"porcentaje_duplicados\"], 2),\n",
    "                round(metricas[\"asimetria_mediana\"], 3),\n",
    "                round(metricas[\"curtosis_mediana\"], 3),\n",
    "                round(metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"], 2),\n",
    "                round(metricas[\"correlacion_absoluta_maxima\"], 3),\n",
    "                round(metricas[\"porcentaje_pares_correlacion_mayor_0_9\"], 2),\n",
    "                round(metricas[\"rango_mediano_variables\"], 3),\n",
    "                round(metricas[\"rango_maximo_variables\"], 3),\n",
    "                metricas[\"sugerencia_escalado\"]\n",
    "            ]\n",
    "            escritor.writerow(fila)\n",
    "\n",
    "        # === Outliers por IQR (resumen) ===\n",
    "        porcentajes_outliers = calcular_porcentaje_outliers_por_variable(X_df)\n",
    "        # Reporta top 8 variables con mayor % de outliers\n",
    "        pares_ordenados = sorted(porcentajes_outliers.items(), key=lambda p: p[1], reverse=True)\n",
    "        top_out = pares_ordenados[:8]\n",
    "        lineas_resultado.append(\"ðŸ“Œ Top variables con mayor porcentaje de outliers (IQR):\")\n",
    "        for nombre_col, pct in top_out:\n",
    "            lineas_resultado.append(f\"   - {nombre_col}: {round(pct,2)}%\")\n",
    "\n",
    "        # === Figuras: hist, box, correlaciÃ³n ===\n",
    "        carpeta_figuras_dataset = f\"figuras/{nombre.lower()}_{timestamp}\"\n",
    "        Path(carpeta_figuras_dataset).mkdir(exist_ok=True)\n",
    "\n",
    "        graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=f\"{carpeta_figuras_dataset}/{nombre.lower()}_clases.png\")\n",
    "        graficar_histograma_variables(X_df, nombre, carpeta_figuras_dataset, max_columnas=12)\n",
    "        graficar_boxplot_variables(X_df, nombre, carpeta_figuras_dataset, max_columnas=12)\n",
    "        graficar_mapa_correlaciones(X_df, nombre, carpeta_figuras_dataset)\n",
    "\n",
    "    except Exception as e:\n",
    "        mensaje_error = f\"âŒ Error al analizar {nombre}: {e}\"\n",
    "        print(mensaje_error)\n",
    "        lineas_resultado.append(mensaje_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187fdcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Generando reporte: SHUTTLE\n",
      "   Â· ERROR en shuttle: \"None of [Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'], dtype='object')] are in the [columns]\"\n",
      "\n",
      "ðŸ”Ž Generando reporte: WDBC\n",
      "   Â· OK -> resultados_reporte_unificado_2025-11-10_0104\\reporte_wdbc.html\n",
      "\n",
      "ðŸ”Ž Generando reporte: GLASS\n",
      "   Â· OK -> resultados_reporte_unificado_2025-11-10_0104\\reporte_glass.html\n",
      "\n",
      "ðŸ”Ž Generando reporte: HEART\n",
      "   Â· OK -> resultados_reporte_unificado_2025-11-10_0104\\reporte_heart.html\n",
      "\n",
      "ðŸ”Ž Generando reporte: IRIS\n",
      "   Â· OK -> resultados_reporte_unificado_2025-11-10_0104\\reporte_iris.html\n",
      "\n",
      "ðŸ”Ž Generando reporte: ECOLI\n",
      "   Â· OK -> resultados_reporte_unificado_2025-11-10_0104\\reporte_ecoli.html\n",
      "\n",
      "âœ… Reporte unificado generado.\n",
      "   - Ãndice: resultados_reporte_unificado_2025-11-10_0104\\index.html\n",
      "   - MÃ©tricas consolidadas: resultados_reporte_unificado_2025-11-10_0104\\metricas_eda_consolidado.csv\n",
      "   - Figuras por dataset en: resultados_reporte_unificado_2025-11-10_0104\\figuras\n"
     ]
    }
   ],
   "source": [
    "# === REPORTE VISUAL UNIFICADO (HTML) ===\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ConfiguraciÃ³n general de salida\n",
    "# -------------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "carpeta_resultados = Path(f\"resultados_reporte_unificado_{timestamp}\")\n",
    "carpeta_figuras = carpeta_resultados / \"figuras\"\n",
    "carpeta_resultados.mkdir(exist_ok=True, parents=True)\n",
    "carpeta_figuras.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utilidades base\n",
    "# -------------------------------------------------------------------\n",
    "def es_numerica(serie):\n",
    "    return np.issubdtype(serie.dtype, np.number)\n",
    "\n",
    "def obtener_matriz_correlacion_segura(df_numerico):\n",
    "    df_sin_nan = df_numerico.fillna(df_numerico.median(numeric_only=True))\n",
    "    return df_sin_nan.corr().values, list(df_numerico.columns)\n",
    "\n",
    "def calcular_metricas_basicas_dataframe(X_df, y_series):\n",
    "    metricas = {}\n",
    "    columnas_numericas = []\n",
    "    for nombre_col in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_col]):\n",
    "            columnas_numericas.append(nombre_col)\n",
    "    X_num = X_df[columnas_numericas].copy()\n",
    "\n",
    "    metricas[\"cantidad_muestras\"] = int(X_df.shape[0])\n",
    "    metricas[\"cantidad_atributos\"] = int(X_df.shape[1])\n",
    "\n",
    "    total_celdas = int(X_df.shape[0] * X_df.shape[1])\n",
    "    cantidad_faltantes = int(X_df.isnull().sum().sum())\n",
    "    metricas[\"porcentaje_faltantes\"] = float((cantidad_faltantes / total_celdas) * 100.0) if total_celdas > 0 else 0.0\n",
    "\n",
    "    cantidad_duplicados = int(X_df.duplicated().sum())\n",
    "    metricas[\"porcentaje_duplicados\"] = float((cantidad_duplicados / X_df.shape[0]) * 100.0) if X_df.shape[0] > 0 else 0.0\n",
    "\n",
    "    valores_unicos, conteos = np.unique(y_series, return_counts=True)\n",
    "    cantidad_clases = int(len(valores_unicos))\n",
    "    indice_min = int(np.argmin(conteos))\n",
    "    indice_max = int(np.argmax(conteos))\n",
    "    clase_min = valores_unicos[indice_min] if cantidad_clases > 0 else \"\"\n",
    "    clase_max = valores_unicos[indice_max] if cantidad_clases > 0 else \"\"\n",
    "    n_min = int(conteos[indice_min]) if cantidad_clases > 0 else 0\n",
    "    n_max = int(conteos[indice_max]) if cantidad_clases > 0 else 0\n",
    "\n",
    "    metricas[\"cantidad_clases\"] = cantidad_clases\n",
    "    metricas[\"clase_minima_real\"] = str(clase_min)\n",
    "    metricas[\"clase_mayoritaria_real\"] = str(clase_max)\n",
    "    metricas[\"n_min\"] = n_min\n",
    "    metricas[\"n_max\"] = n_max\n",
    "    metricas[\"ratio_desequilibrio_max\"] = float(n_max / max(1, n_min)) if n_min > 0 else float(\"inf\")\n",
    "\n",
    "    proporciones = conteos / conteos.sum() if conteos.sum() > 0 else np.array([1.0])\n",
    "    entropia = float(-(proporciones * np.log(proporciones + 1e-12)).sum())\n",
    "    metricas[\"entropia_clases\"] = entropia\n",
    "    metricas[\"tamano_efectivo_clases\"] = float(np.exp(entropia))\n",
    "\n",
    "    rangos = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna()\n",
    "        if col.shape[0] > 0:\n",
    "            valor_min = float(np.min(col))\n",
    "            valor_max = float(np.max(col))\n",
    "            rango = float(valor_max - valor_min)\n",
    "            rangos.append(rango)\n",
    "    if len(rangos) > 0:\n",
    "        metricas[\"rango_mediano_variables\"] = float(np.median(rangos))\n",
    "        metricas[\"rango_maximo_variables\"] = float(np.max(rangos))\n",
    "    else:\n",
    "        metricas[\"rango_mediano_variables\"] = 0.0\n",
    "        metricas[\"rango_maximo_variables\"] = 0.0\n",
    "\n",
    "    skew_acumulado = []\n",
    "    kurt_acumulado = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        if col.shape[0] > 3:\n",
    "            skew_acumulado.append(float(skew(col)))\n",
    "            kurt_acumulado.append(float(kurtosis(col, fisher=True)))\n",
    "    metricas[\"asimetria_mediana\"] = float(np.median(skew_acumulado)) if len(skew_acumulado) > 0 else 0.0\n",
    "    metricas[\"curtosis_mediana\"] = float(np.median(kurt_acumulado)) if len(kurt_acumulado) > 0 else 0.0\n",
    "\n",
    "    porcentaje_normalidad = 0.0\n",
    "    variables_evaluadas = 0\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        tam_col = col.shape[0]\n",
    "        if tam_col > 3:\n",
    "            if tam_col > 5000:\n",
    "                col = col.sample(5000, random_state=123).astype(float)\n",
    "            try:\n",
    "                estadistico, pvalor = shapiro(col.values)\n",
    "                variables_evaluadas += 1\n",
    "                if pvalor > 0.05:\n",
    "                    porcentaje_normalidad += 1.0\n",
    "            except Exception:\n",
    "                pass\n",
    "    if variables_evaluadas > 0:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = float((porcentaje_normalidad / variables_evaluadas) * 100.0)\n",
    "    else:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = 0.0\n",
    "\n",
    "    if X_num.shape[1] >= 2:\n",
    "        matriz_corr, nombres = obtener_matriz_correlacion_segura(X_num)\n",
    "        valores_superior = []\n",
    "        cantidad_altamente_correl = 0\n",
    "        total_pares = 0\n",
    "        for i in range(len(nombres)):\n",
    "            for j in range(i + 1, len(nombres)):\n",
    "                r = float(matriz_corr[i, j])\n",
    "                valores_superior.append(abs(r))\n",
    "                total_pares += 1\n",
    "                if abs(r) > 0.9:\n",
    "                    cantidad_altamente_correl += 1\n",
    "        if len(valores_superior) > 0:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = float(np.max(valores_superior))\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = float((cantidad_altamente_correl / total_pares) * 100.0)\n",
    "        else:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "    else:\n",
    "        metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "        metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "\n",
    "    sugerencia_escalado = \"MinMaxScaler\"\n",
    "    if metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] > 60.0 and metricas[\"correlacion_absoluta_maxima\"] < 0.95:\n",
    "        sugerencia_escalado = \"StandardScaler\"\n",
    "    if metricas[\"asimetria_mediana\"] > 1.0 or metricas[\"curtosis_mediana\"] > 1.0:\n",
    "        sugerencia_escalado = \"RobustScaler\"\n",
    "    metricas[\"sugerencia_escalado\"] = sugerencia_escalado\n",
    "\n",
    "    return metricas\n",
    "\n",
    "def calcular_porcentaje_outliers_por_variable(X_df):\n",
    "    resultado = {}\n",
    "    for nombre_columna in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_columna]):\n",
    "            serie = X_df[nombre_columna].dropna().astype(float)\n",
    "            if serie.shape[0] > 0:\n",
    "                q1 = float(np.percentile(serie, 25))\n",
    "                q3 = float(np.percentile(serie, 75))\n",
    "                iqr = float(q3 - q1)\n",
    "                limite_inferior = q1 - 1.5 * iqr\n",
    "                limite_superior = q3 + 1.5 * iqr\n",
    "                cantidad = int(serie.shape[0])\n",
    "                cantidad_out = 0\n",
    "                indice = 0\n",
    "                valores = serie.values\n",
    "                while indice < cantidad:\n",
    "                    valor_actual = float(valores[indice])\n",
    "                    if valor_actual < limite_inferior or valor_actual > limite_superior:\n",
    "                        cantidad_out += 1\n",
    "                    indice += 1\n",
    "                porcentaje = float((cantidad_out / cantidad) * 100.0)\n",
    "                resultado[nombre_columna] = porcentaje\n",
    "    return resultado\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GrÃ¡ficos (solo Matplotlib) â€” compatibles con cualquier versiÃ³n\n",
    "# -------------------------------------------------------------------\n",
    "def graficar_histograma_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas_numericas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas_numericas.append(c)\n",
    "    if len(columnas_numericas) > max_columnas:\n",
    "        columnas_numericas = columnas_numericas[:max_columnas]\n",
    "    for nombre_col in columnas_numericas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        plt.hist(valores, bins=30)\n",
    "        plt.title(f\"{nombre_dataset} Â· Histograma: {nombre_col}\")\n",
    "        plt.xlabel(nombre_col)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_hist_{nombre_col}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_boxplot_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas.append(c)\n",
    "    if len(columnas) > max_columnas:\n",
    "        columnas = columnas[:max_columnas]\n",
    "    for nombre_col in columnas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        try:\n",
    "            plt.boxplot(valores, vert=True, showfliers=True)  # sin labels/tick_labels\n",
    "        except Exception:\n",
    "            if valores.size > 0:\n",
    "                plt.plot([1], [valores[0]], marker=\"o\")\n",
    "        plt.xticks([1], [nombre_col])  # seteamos el tick manualmente\n",
    "        plt.title(f\"{nombre_dataset} Â· Boxplot: {nombre_col}\")\n",
    "        ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_box_{nombre_col}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_mapa_correlaciones(X_df, nombre_dataset, ruta_base):\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    if len(columnas) < 2:\n",
    "        return\n",
    "    X_num = X_df[columnas].fillna(X_df[columnas].median(numeric_only=True))\n",
    "    matriz_corr = X_num.corr().values\n",
    "    plt.figure()\n",
    "    plt.imshow(matriz_corr, aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    # ticks compatibles con cualquier versiÃ³n\n",
    "    posiciones = np.arange(len(columnas))\n",
    "    plt.xticks(posiciones, columnas, rotation=90)\n",
    "    plt.yticks(posiciones, columnas)\n",
    "    plt.title(f\"{nombre_dataset} Â· Matriz de correlaciones (Pearson)\")\n",
    "    plt.tight_layout()\n",
    "    ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_correlaciones.png\")\n",
    "    plt.savefig(ruta_salida, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PCA (varianza explicada)\n",
    "# -------------------------------------------------------------------\n",
    "def calcular_varianza_explicada_pca(X_df, k=5):\n",
    "    columnas_num = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas_num.append(c)\n",
    "    if len(columnas_num) < 2:\n",
    "        return []\n",
    "    X_num = X_df[columnas_num].fillna(X_df[columnas_num].median(numeric_only=True)).astype(float)\n",
    "    componentes = int(min(k, X_num.shape[1]))\n",
    "    pca = PCA(n_components=componentes, random_state=123)\n",
    "    pca.fit(X_num)\n",
    "    lista_var = []\n",
    "    for v in pca.explained_variance_ratio_:\n",
    "        lista_var.append(float(v))\n",
    "    return lista_var\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GeneraciÃ³n del reporte HTML por dataset\n",
    "# -------------------------------------------------------------------\n",
    "def generar_html_dataset(nombre_dataset, resumen_clases, metricas, varianza_pca, rutas_figuras, ruta_html_salida):\n",
    "    # ConstrucciÃ³n manual del HTML (sin librerÃ­as externas)\n",
    "    html = []\n",
    "    html.append(\"<html><head><meta charset='utf-8'><title>Reporte \" + str(nombre_dataset) + \"</title>\")\n",
    "    html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;max-width:1100px;margin:24px auto;padding:0 12px;}h1{margin-bottom:6px;}h2{margin-top:28px;}table{border-collapse:collapse;width:100%;}table,th,td{border:1px solid #ddd;}th,td{padding:8px;text-align:left;}code{background:#f5f5f5;padding:2px 4px;border-radius:4px;}</style>\")\n",
    "    html.append(\"</head><body>\")\n",
    "    html.append(\"<h1>Reporte de anÃ¡lisis â€” \" + str(nombre_dataset) + \"</h1>\")\n",
    "\n",
    "    # Resumen de clases\n",
    "    html.append(\"<h2>DistribuciÃ³n de clases</h2>\")\n",
    "    html.append(\"<table><thead><tr><th>Clase</th><th>Conteo</th><th>ProporciÃ³n (%)</th></tr></thead><tbody>\")\n",
    "    for fila in resumen_clases:  # lista de tuplas (clase, n, pct)\n",
    "        html.append(\"<tr><td>\" + str(fila[0]) + \"</td><td>\" + str(fila[1]) + \"</td><td>\" + str(round(fila[2],2)) + \"</td></tr>\")\n",
    "    html.append(\"</tbody></table>\")\n",
    "\n",
    "    # MÃ©tricas clave\n",
    "    html.append(\"<h2>MÃ©tricas clave del dataset</h2>\")\n",
    "    html.append(\"<table><tbody>\")\n",
    "    for k in [\n",
    "        \"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "        \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "        \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "        \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "        \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "        \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\"\n",
    "    ]:\n",
    "        html.append(\"<tr><th>\"+str(k)+\"</th><td>\"+str(metricas.get(k,\"\"))+\"</td></tr>\")\n",
    "    html.append(\"</tbody></table>\")\n",
    "\n",
    "    # PCA\n",
    "    html.append(\"<h2>PCA â€” Varianza explicada</h2>\")\n",
    "    if len(varianza_pca) > 0:\n",
    "        suma_2 = sum(varianza_pca[:2])\n",
    "        suma_3 = sum(varianza_pca[:3])\n",
    "        html.append(\"<p>PC1: <b>\"+str(round(varianza_pca[0],3))+\"</b> Â· PC1+PC2: <b>\"+str(round(suma_2,3))+\"</b> Â· PC1+PC2+PC3: <b>\"+str(round(suma_3,3))+\"</b></p>\")\n",
    "        html.append(\"<table><thead><tr><th>Componente</th><th>Varianza explicada</th></tr></thead><tbody>\")\n",
    "        indice = 0\n",
    "        while indice < len(varianza_pca):\n",
    "            html.append(\"<tr><td>PC\"+str(indice+1)+\"</td><td>\"+str(round(varianza_pca[indice],6))+\"</td></tr>\")\n",
    "            indice += 1\n",
    "        html.append(\"</tbody></table>\")\n",
    "    else:\n",
    "        html.append(\"<p>Insuficientes columnas numÃ©ricas para PCA.</p>\")\n",
    "\n",
    "    # ImÃ¡genes\n",
    "    html.append(\"<h2>Figuras</h2>\")\n",
    "    for titulo, ruta in rutas_figuras:\n",
    "        rel = os.path.relpath(ruta, start=str(carpeta_resultados))\n",
    "        html.append(\"<h3>\"+titulo+\"</h3>\")\n",
    "        html.append(\"<img src='\"+rel+\"' style='max-width:100%;height:auto;border:1px solid #ddd;padding:4px;' />\")\n",
    "\n",
    "    html.append(\"</body></html>\")\n",
    "\n",
    "    with open(ruta_html_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Orquestador principal\n",
    "# -------------------------------------------------------------------\n",
    "def generar_reporte_unificado(config_datasets, cargar_dataset, graficar_distribucion_clases):\n",
    "    ruta_index = carpeta_resultados / \"index.html\"\n",
    "    filas_index = []\n",
    "    filas_index.append(\"<html><head><meta charset='utf-8'><title>Reporte unificado</title>\")\n",
    "    filas_index.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;max-width:900px;margin:24px auto;padding:0 12px;}ul{line-height:1.8}</style>\")\n",
    "    filas_index.append(\"</head><body>\")\n",
    "    filas_index.append(\"<h1>Reporte unificado â€” AnÃ¡lisis exploratorio</h1>\")\n",
    "    filas_index.append(\"<ul>\")\n",
    "\n",
    "    # CSV de mÃ©tricas consolidado\n",
    "    ruta_metricas_csv = carpeta_resultados / \"metricas_eda_consolidado.csv\"\n",
    "    with open(ruta_metricas_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "        escritor = csv.writer(fcsv)\n",
    "        escritor.writerow([\n",
    "            \"dataset\",\"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "            \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "            \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "            \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "            \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "            \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\",\n",
    "            \"pca_pc1\",\"pca_pc1_pc2\",\"pca_pc1_pc2_pc3\"\n",
    "        ])\n",
    "\n",
    "    # Loop datasets\n",
    "    for nombre, cfg in config_datasets.items():\n",
    "        print(f\"\\nðŸ”Ž Generando reporte: {nombre.upper()}\")\n",
    "        try:\n",
    "            X, y, _ = cargar_dataset(\n",
    "                path=cfg[\"path\"],\n",
    "                clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "                col_features=cfg.get(\"col_features\"),\n",
    "                col_target=cfg.get(\"col_target\"),\n",
    "                sep=cfg.get(\"sep\", \",\"),\n",
    "                header=cfg.get(\"header\"),\n",
    "                binarizar=cfg.get(\"binarizar\", False),\n",
    "                tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "                dataset_name=cfg.get(\"dataset_name\")\n",
    "            )\n",
    "\n",
    "            # DataFrames seguros\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_df = X.copy()\n",
    "            else:\n",
    "                columnas_genericas = []\n",
    "                indice_col = 0\n",
    "                while indice_col < X.shape[1]:\n",
    "                    columnas_genericas.append(f\"col_{indice_col}\")\n",
    "                    indice_col += 1\n",
    "                X_df = pd.DataFrame(X, columns=columnas_genericas)\n",
    "            y_series = pd.Series(y)\n",
    "\n",
    "            # Resumen de clases para tabla HTML\n",
    "            conteo = pd.Series(y).value_counts()\n",
    "            total = int(conteo.sum())\n",
    "            resumen_clases = []\n",
    "            for clase, n in conteo.items():\n",
    "                porcentaje = float((n / max(1,total)) * 100.0)\n",
    "                resumen_clases.append((clase, int(n), porcentaje))\n",
    "\n",
    "            # MÃ©tricas\n",
    "            metricas = calcular_metricas_basicas_dataframe(X_df, y_series)\n",
    "\n",
    "            # PCA\n",
    "            var_pca = calcular_varianza_explicada_pca(X_df, k=5)\n",
    "            pc1 = float(var_pca[0]) if len(var_pca) > 0 else 0.0\n",
    "            pc12 = float(sum(var_pca[:2])) if len(var_pca) >= 2 else pc1\n",
    "            pc123 = float(sum(var_pca[:3])) if len(var_pca) >= 3 else pc12\n",
    "\n",
    "            # Guardar en CSV consolidado\n",
    "            with open(ruta_metricas_csv, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "                escritor = csv.writer(fcsv)\n",
    "                escritor.writerow([\n",
    "                    nombre,\n",
    "                    metricas[\"cantidad_muestras\"],\n",
    "                    metricas[\"cantidad_atributos\"],\n",
    "                    metricas[\"cantidad_clases\"],\n",
    "                    metricas[\"clase_minima_real\"],\n",
    "                    metricas[\"clase_mayoritaria_real\"],\n",
    "                    metricas[\"n_min\"],\n",
    "                    metricas[\"n_max\"],\n",
    "                    round(metricas[\"ratio_desequilibrio_max\"], 6) if math.isfinite(metricas[\"ratio_desequilibrio_max\"]) else \"inf\",\n",
    "                    round(metricas[\"entropia_clases\"], 6),\n",
    "                    round(metricas[\"tamano_efectivo_clases\"], 6),\n",
    "                    round(metricas[\"porcentaje_faltantes\"], 6),\n",
    "                    round(metricas[\"porcentaje_duplicados\"], 6),\n",
    "                    round(metricas[\"asimetria_mediana\"], 6),\n",
    "                    round(metricas[\"curtosis_mediana\"], 6),\n",
    "                    round(metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"], 6),\n",
    "                    round(metricas[\"correlacion_absoluta_maxima\"], 6),\n",
    "                    round(metricas[\"porcentaje_pares_correlacion_mayor_0_9\"], 6),\n",
    "                    round(metricas[\"rango_mediano_variables\"], 6),\n",
    "                    round(metricas[\"rango_maximo_variables\"], 6),\n",
    "                    metricas[\"sugerencia_escalado\"],\n",
    "                    round(pc1, 6),\n",
    "                    round(pc12, 6),\n",
    "                    round(pc123, 6)\n",
    "                ])\n",
    "\n",
    "            # Carpetas y figuras\n",
    "            carpeta_dataset = carpeta_figuras / nombre.lower()\n",
    "            carpeta_dataset.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            ruta_fig_clases = os.path.join(str(carpeta_dataset), f\"{nombre.lower()}_clases.png\")\n",
    "            graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=ruta_fig_clases)\n",
    "\n",
    "            graficar_histograma_variables(X_df, nombre, str(carpeta_dataset), max_columnas=12)\n",
    "            graficar_boxplot_variables(X_df, nombre, str(carpeta_dataset), max_columnas=12)\n",
    "            graficar_mapa_correlaciones(X_df, nombre, str(carpeta_dataset))\n",
    "\n",
    "            # Recolectar rutas de imÃ¡genes para el HTML\n",
    "            rutas_figuras = []\n",
    "            rutas_figuras.append((\"DistribuciÃ³n de clases\", ruta_fig_clases))\n",
    "\n",
    "            # Agregar hist y box generados (primeras 12 numÃ©ricas)\n",
    "            columnas_numericas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "            limite = min(12, len(columnas_numericas))\n",
    "            indice = 0\n",
    "            while indice < limite:\n",
    "                col = columnas_numericas[indice]\n",
    "                rutas_figuras.append((f\"Histograma: {col}\", os.path.join(str(carpeta_dataset), f\"{nombre}_hist_{col}.png\")))\n",
    "                rutas_figuras.append((f\"Boxplot: {col}\", os.path.join(str(carpeta_dataset), f\"{nombre}_box_{col}.png\")))\n",
    "                indice += 1\n",
    "\n",
    "            # Correlaciones (si hubo)\n",
    "            if len(columnas_numericas) >= 2:\n",
    "                rutas_figuras.append((\"Matriz de correlaciones\", os.path.join(str(carpeta_dataset), f\"{nombre}_correlaciones.png\")))\n",
    "\n",
    "            # HTML por dataset\n",
    "            ruta_html_dataset = carpeta_resultados / f\"reporte_{nombre.lower()}.html\"\n",
    "            generar_html_dataset(\n",
    "                nombre_dataset=nombre,\n",
    "                resumen_clases=resumen_clases,\n",
    "                metricas=metricas,\n",
    "                varianza_pca=var_pca,\n",
    "                rutas_figuras=rutas_figuras,\n",
    "                ruta_html_salida=str(ruta_html_dataset)\n",
    "            )\n",
    "\n",
    "            # Agregar al Ã­ndice\n",
    "            rel = os.path.relpath(str(ruta_html_dataset), start=str(carpeta_resultados))\n",
    "            filas_index.append(f\"<li><a href='{rel}'>{nombre}</a></li>\")\n",
    "            print(f\"   Â· OK -> {ruta_html_dataset}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Â· ERROR en {nombre}: {e}\")\n",
    "\n",
    "    filas_index.append(\"</ul>\")\n",
    "    filas_index.append(\"<hr/><p>Carpeta de figuras: <code>\" + str(carpeta_figuras) + \"</code></p>\")\n",
    "    filas_index.append(\"</body></html>\")\n",
    "    with open(ruta_index, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(filas_index))\n",
    "\n",
    "    print(\"\\nâœ… Reporte unificado generado.\")\n",
    "    print(\"   - Ãndice:\", ruta_index)\n",
    "    print(\"   - MÃ©tricas consolidadas:\", ruta_metricas_csv)\n",
    "    print(\"   - Figuras por dataset en:\", carpeta_figuras)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# EJECUCIÃ“N: llama con tus objetos reales\n",
    "# -------------------------------------------------------------------\n",
    "# Requiere que existan en tu notebook:\n",
    "#  - config_datasets\n",
    "#  - cargar_dataset(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo)\n",
    "#  - graficar_distribucion_clases(y, nombre_dataset, guardar_en)\n",
    "generar_reporte_unificado(config_datasets, cargar_dataset, graficar_distribucion_clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31abe33a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resumen_univariado, resumen_por_clase\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Ejecutar diagnÃ³stico\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m resumen_univariado, resumen_por_clase = \u001b[43mdiagnosticar_outliers_shuttle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshuttle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdiagnosticos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m resumen_univariado.head(), resumen_por_clase\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mdiagnosticar_outliers_shuttle\u001b[39m\u001b[34m(config_datasets, nombre_dataset, ruta_salida)\u001b[39m\n\u001b[32m     13\u001b[39m Path(ruta_salida).mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m cfg = config_datasets[nombre_dataset]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m X, y, _ = \u001b[43mcargar_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclase_minoria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclase_minoria\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol_target\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinarizar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Shuttle multiclase\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtipo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtipo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabular\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpute\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedian\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# X viene como ndarray (segÃºn tu cargar_dataset) -> armo DataFrame con nombres\u001b[39;00m\n\u001b[32m     29\u001b[39m nombres_columnas = cfg.get(\u001b[33m\"\u001b[39m\u001b[33mcol_features\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\Documents\\UNLu\\armado-tesina\\codigo\\datasets\\cargar_dataset.py:48\u001b[39m, in \u001b[36mcargar_dataset\u001b[39m\u001b[34m(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo, impute, na_values, dataset_name, names)\u001b[39m\n\u001b[32m     45\u001b[39m         df.columns = esquema  \u001b[38;5;66;03m# mapeo automÃ¡tico\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# SelecciÃ³n features y target (por nombre o Ã­ndice)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m df_features = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_features\u001b[49m\u001b[43m]\u001b[49m.apply(pd.to_numeric, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     49\u001b[39m df_target = df[[col_target]] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_target, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m df[col_target]\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ImputaciÃ³n / drop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Asumo que ya tenÃ©s:\n",
    "# - config_datasets\n",
    "# - cargar_dataset(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo, impute, na_values)\n",
    "# Si tus funciones estÃ¡n en mÃ³dulos, importalas antes.\n",
    "\n",
    "def diagnosticar_outliers_shuttle(config_datasets, nombre_dataset=\"shuttle\", ruta_salida=\"diagnosticos\"):\n",
    "    Path(ruta_salida).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    cfg = config_datasets[nombre_dataset]\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=cfg[\"path\"],\n",
    "        clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "        col_features=cfg.get(\"col_features\"),\n",
    "        col_target=cfg.get(\"col_target\"),\n",
    "        sep=cfg.get(\"sep\", \",\"),\n",
    "        header=cfg.get(\"header\"),\n",
    "        binarizar=False,                 # Shuttle multiclase\n",
    "        tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "        impute=\"median\"\n",
    "    )\n",
    "\n",
    "    # X viene como ndarray (segÃºn tu cargar_dataset) -> armo DataFrame con nombres\n",
    "    nombres_columnas = cfg.get(\"col_features\")\n",
    "    df = pd.DataFrame(X, columns=nombres_columnas)\n",
    "    serie_clase = pd.Series(y, name=\"clase\")\n",
    "\n",
    "    # === 1) Resumen univariado: skew, kurtosis, % outliers por IQR ===\n",
    "    lista_filas_resumen = []\n",
    "    for nombre_columna in nombres_columnas:\n",
    "        serie = df[nombre_columna].astype(float)\n",
    "\n",
    "        q1 = float(np.percentile(serie, 25))\n",
    "        q3 = float(np.percentile(serie, 75))\n",
    "        iqr = q3 - q1\n",
    "        limite_inferior = q1 - 1.5 * iqr\n",
    "        limite_superior = q3 + 1.5 * iqr\n",
    "\n",
    "        cantidad_outliers = int(((serie < limite_inferior) | (serie > limite_superior)).sum())\n",
    "        porcentaje_outliers = 100.0 * cantidad_outliers / len(serie)\n",
    "\n",
    "        asimetria = float(pd.Series(serie).skew())\n",
    "        curtosis = float(pd.Series(serie).kurtosis())\n",
    "\n",
    "        fila = {\n",
    "            \"variable\": nombre_columna,\n",
    "            \"asimetria\": asimetria,\n",
    "            \"curtosis\": curtosis,\n",
    "            \"q1\": q1,\n",
    "            \"q3\": q3,\n",
    "            \"iqr\": iqr,\n",
    "            \"limite_inferior\": limite_inferior,\n",
    "            \"limite_superior\": limite_superior,\n",
    "            \"cantidad_outliers_IQR\": cantidad_outliers,\n",
    "            \"porcentaje_outliers_IQR\": porcentaje_outliers\n",
    "        }\n",
    "        lista_filas_resumen.append(fila)\n",
    "\n",
    "    resumen_univariado = pd.DataFrame(lista_filas_resumen).sort_values(\"porcentaje_outliers_IQR\", ascending=False)\n",
    "    resumen_univariado.to_csv(f\"{ruta_salida}/shuttle_resumen_univariado.csv\", index=False)\n",
    "\n",
    "    # === 2) GrÃ¡ficos univariados (histograma + boxplot) por variable ===\n",
    "    for nombre_columna in nombres_columnas:\n",
    "        serie = df[nombre_columna].astype(float)\n",
    "\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.hist(serie, bins=50)\n",
    "        plt.title(f\"Histograma - {nombre_columna}\")\n",
    "        plt.xlabel(nombre_columna)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{ruta_salida}/shuttle_hist_{nombre_columna}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.boxplot(serie, vert=True, showfliers=True)\n",
    "        plt.title(f\"Boxplot - {nombre_columna}\")\n",
    "        plt.ylabel(nombre_columna)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{ruta_salida}/shuttle_box_{nombre_columna}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # === 3) Outliers por clase usando IQR (no borro, solo mido) ===\n",
    "    lista_resumen_por_clase = []\n",
    "    clases_unicas = np.unique(serie_clase.values)\n",
    "    for clase_actual in clases_unicas:\n",
    "        indice_clase = (serie_clase.values == clase_actual)\n",
    "        subdf = df.loc[indice_clase]\n",
    "\n",
    "        # porcentaje promedio de outliers IQR (promedio sobre columnas)\n",
    "        porcentaje_por_variable = []\n",
    "        for nombre_columna in nombres_columnas:\n",
    "            serie = subdf[nombre_columna].astype(float)\n",
    "            if len(serie) == 0:\n",
    "                continue\n",
    "            q1 = float(np.percentile(serie, 25))\n",
    "            q3 = float(np.percentile(serie, 75))\n",
    "            iqr = q3 - q1\n",
    "            limite_inferior = q1 - 1.5 * iqr\n",
    "            limite_superior = q3 + 1.5 * iqr\n",
    "            cant = int(((serie < limite_inferior) | (serie > limite_superior)).sum())\n",
    "            porcentaje = 100.0 * cant / len(serie)\n",
    "            porcentaje_por_variable.append(porcentaje)\n",
    "        promedio_porcentaje = float(np.mean(porcentaje_por_variable)) if len(porcentaje_por_variable) > 0 else 0.0\n",
    "\n",
    "        lista_resumen_por_clase.append({\n",
    "            \"clase\": clase_actual,\n",
    "            \"muestras_clase\": int(indice_clase.sum()),\n",
    "            \"promedio_%_outliers_IQR\": promedio_porcentaje\n",
    "        })\n",
    "\n",
    "    resumen_por_clase = pd.DataFrame(lista_resumen_por_clase).sort_values(\"muestras_clase\", ascending=False)\n",
    "    resumen_por_clase.to_csv(f\"{ruta_salida}/shuttle_outliers_por_clase_IQR.csv\", index=False)\n",
    "\n",
    "    # === 4) IsolationForest por clase (solo para medir en la mayoritaria, no para borrar) ===\n",
    "    # Nota: no uses este resultado para eliminar en clases minoritarias.\n",
    "    if len(clases_unicas) > 0:\n",
    "        # identifico clase mayoritaria\n",
    "        conteo = pd.Series(serie_clase).value_counts()\n",
    "        clase_mayoritaria = conteo.idxmax()\n",
    "        indice_mayoritaria = (serie_clase.values == clase_mayoritaria)\n",
    "        X_may = df.loc[indice_mayoritaria].values.astype(float)\n",
    "\n",
    "        if X_may.shape[0] > 100:\n",
    "            modelo_iso = IsolationForest(\n",
    "                n_estimators=200,\n",
    "                contamination=0.002,  # umbral MUY estricto: ~0.2%\n",
    "                random_state=42\n",
    "            )\n",
    "            etiquetas = modelo_iso.fit_predict(X_may)\n",
    "            porcentaje_anomalias = 100.0 * (etiquetas == -1).sum() / len(etiquetas)\n",
    "\n",
    "            with open(f\"{ruta_salida}/shuttle_isolationforest_mayoritaria.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Clase mayoritaria: {clase_mayoritaria}\\n\")\n",
    "                f.write(f\"Muestras en clase mayoritaria: {len(etiquetas)}\\n\")\n",
    "                f.write(f\"Porcentaje marcado como anomalÃ­a (contamination=0.002): {porcentaje_anomalias:.4f}%\\n\")\n",
    "\n",
    "    # === 5) RecomendaciÃ³n final en texto ===\n",
    "    with open(f\"{ruta_salida}/shuttle_recomendacion.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"RecomendaciÃ³n Shuttle:\\n\")\n",
    "        f.write(\"- No eliminar outliers: los extremos reflejan transiciones reales del sistema.\\n\")\n",
    "        f.write(\"- Si el modelo lo requiere, usar RobustScaler o winsorizaciÃ³n leve (p0.1â€“p99.9) y validar.\\n\")\n",
    "        f.write(\"- Evitar IsolationForest global. Si se limpia, hacerlo solo en clase mayoritaria y evaluar impacto.\\n\")\n",
    "\n",
    "    return resumen_univariado, resumen_por_clase\n",
    "\n",
    "# Ejecutar diagnÃ³stico\n",
    "resumen_univariado, resumen_por_clase = diagnosticar_outliers_shuttle(config_datasets, \"shuttle\", \"diagnosticos\")\n",
    "resumen_univariado.head(), resumen_por_clase\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
