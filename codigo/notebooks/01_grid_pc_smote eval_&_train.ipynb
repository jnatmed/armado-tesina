{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la t√©cnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import gc, os, time                                                      # Operaciones con el sistema de archivos\n",
    "import re\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa: F401  <- necesario\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Evitar sobre-suscripci√≥n de CPU (BLAS/OpenMP)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "SCORING_PRIMARY = 'f1_macro'  # √∫nico m√©trico para Halving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc87fa1",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n de modelos con validaci√≥n cruzada estratificada\n",
    "\n",
    "Para evaluar el rendimiento de los modelos de clasificaci√≥n sobre los datasets previamente balanceados, se utiliz√≥ validaci√≥n cruzada estratificada de 5 particiones (Stratified K-Fold con *k=5*). Este m√©todo garantiza que en cada fold de entrenamiento y validaci√≥n se preserve la proporci√≥n original de clases, lo cual es especialmente importante en tareas de clasificaci√≥n multiclase con datasets balanceados artificialmente.\n",
    "\n",
    "Durante el proceso, cada modelo es entrenado y evaluado cinco veces, cada vez usando un subconjunto distinto como conjunto de prueba y el resto como conjunto de entrenamiento. Las m√©tricas calculadas en cada iteraci√≥n (F1-score macro, balanced accuracy, MCC y kappa de Cohen) se promedian para obtener un valor representativo y del rendimiento general del modelo sobre ese dataset aumentado.\n",
    "\n",
    "Este enfoque evita sobreajuste y proporciona una evaluaci√≥n m√°s confiable que una simple divisi√≥n train/test, permitiendo comparar de forma justa distintas configuraciones de sobremuestreo y modelos de clasificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e43a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Enumerando combinaciones base y aumentadas...\n",
      "üìÇ Explorando carpeta base: ../datasets/datasets_aumentados/base/\n",
      "  ‚ö™ Omitido (no es *_train.csv): ecoli_test.csv\n",
      "üìÇ Explorando carpeta cl√°sicos: ../datasets/datasets_aumentados/resampler_clasicos/\n",
      "üìÇ Explorando carpeta aumentados: ../datasets/datasets_aumentados/\n",
      "üìä Total combinaciones descubiertas: 3\n",
      "üßÆ Tareas planificadas: 3 (excluidos por dataset: 0, por pol√≠tica SVM-Shuttle‚Üë: 0)\n",
      "üì¶ Total de tareas planificadas: 3\n",
      "\n",
      "================================================================================\n",
      "üèÅ [1/3] Dataset: ecoli | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: ecoli_I0_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=4\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=40. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.08 s\n",
      "üìä F1(CV): 0.7013 | F1(Test): 0.8846\n",
      "\n",
      "================================================================================\n",
      "üèÅ [2/3] Dataset: ecoli | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_ecoli_D80_R40_Pentropia_I0_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=4\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=40. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 0.86 s\n",
      "üìä F1(CV): 0.7723 | F1(Test): 0.8954\n",
      "\n",
      "================================================================================\n",
      "üèÅ [3/3] Dataset: ecoli | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_ecoli_D80_R40_Pproporcion_I0_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=4\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=40. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 0.73 s\n",
      "üìä F1(CV): 0.7723 | F1(Test): 0.8954\n",
      "\n",
      "üìä Compilando resultados globales...\n",
      "\n",
      "üèÅ Ejecuci√≥n total completada en 4.28 s\n",
      "üìò Archivo Excel generado: ../resultados\\resultados_RS_cv_vs_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, gc, time\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "# =========================\n",
    "# Configuraci√≥n general\n",
    "# =========================\n",
    "RUTA_DATASETS_BASE = \"../datasets/datasets_aumentados/base/\"\n",
    "RUTA_DATASETS_AUMENTADOS = \"../datasets/datasets_aumentados/\"\n",
    "DIRECTORIO_SALIDA = \"../resultados\"\n",
    "os.makedirs(DIRECTORIO_SALIDA, exist_ok=True)\n",
    "RUTA_DATASETS_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "os.makedirs(RUTA_DATASETS_CLASICOS, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SCORING_REFIT = \"f1_macro\"\n",
    "SCORING_MULTIPLE = {\n",
    "    \"f1_macro\": \"f1_macro\",\n",
    "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "    \"mcc\": make_scorer(matthews_corrcoef),\n",
    "    \"cohen_kappa\": make_scorer(cohen_kappa_score),\n",
    "}\n",
    "N_ITER_BUSQUEDA_POR_DEFECTO = 40\n",
    "OMITIR_SVM_EN_SHUTTLE_AUMENTADO = True  # pol√≠tica opcional\n",
    "NOMBRE_ARCHIVO_EXCEL = os.path.join(DIRECTORIO_SALIDA, \"resultados_RS_cv_vs_test.xlsx\")\n",
    "\n",
    "# =========================\n",
    "# Estructuras de datos\n",
    "# =========================\n",
    "@dataclass\n",
    "class DatasetCombination:\n",
    "    \"\"\"Representa una combinaci√≥n base o aumentada (metadatos y paths).\"\"\"\n",
    "    dataset_logico: str          # p. ej. \"us_crime\"\n",
    "    tipo_combination: str        # \"base\" | \"aumentado\"\n",
    "    ruta_train_csv: str\n",
    "    ruta_test_csv: str\n",
    "    tecnica_aumento: str = \"base\"\n",
    "    valor_densidad: str = \"--\"\n",
    "    valor_riesgo: str = \"--\"\n",
    "    criterio_pureza: str = \"--\"\n",
    "    grado_limpieza: str = \"--\"   # nuevo: I0, I1, I5, etc.\n",
    "\n",
    "@dataclass\n",
    "class RegistroRendimiento:\n",
    "    dataset_logico: str\n",
    "    tipo_combination: str\n",
    "    nombre_modelo_aprendizaje: str\n",
    "    tecnica_aumento: str\n",
    "    valor_densidad: str\n",
    "    valor_riesgo: str\n",
    "    criterio_pureza: str\n",
    "    grado_limpieza: str\n",
    "    cantidad_train: int\n",
    "    cantidad_test: int\n",
    "    cantidad_caracteristicas: int\n",
    "    # CV (sobre el mejor candidato de RS)\n",
    "    cv_f1_macro: float\n",
    "    cv_balanced_accuracy: float\n",
    "    cv_mcc: float\n",
    "    cv_cohen_kappa: float\n",
    "    # Test (macro y adicionales)\n",
    "    test_f1_macro: float\n",
    "    test_balanced_accuracy: float\n",
    "    test_mcc: float\n",
    "    test_cohen_kappa: float\n",
    "    test_accuracy: float\n",
    "    test_f1_por_clase: str\n",
    "    test_recall_minoritario: float\n",
    "    test_gmean: float\n",
    "    test_roc_auc_macro_ovr: float\n",
    "    test_auc_pr_macro: float\n",
    "    # Auditor√≠a\n",
    "    mejores_hiperparametros: str\n",
    "    tiempo_busqueda_seg: float\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilidades de datos\n",
    "# =========================\n",
    "def cargar_matriz_caracteristicas_y_etiquetas_desde_csv(ruta_csv):\n",
    "    \"\"\"Lee un CSV y devuelve (X, y). Si existe 'target', la usa; si no, usa la √∫ltima columna como y.\"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    if \"target\" in df.columns:\n",
    "        X = df.drop(columns=[\"target\"]).to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df[\"target\"].to_numpy()\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df.iloc[:, -1].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def enumerar_combinaciones_base_y_aumentadas(\n",
    "    ruta_base,\n",
    "    ruta_clasicos,\n",
    "    ruta_aumentados,\n",
    "    verbose=True\n",
    "):\n",
    "    combinaciones = []\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) BASE: busca archivos del estilo:   dataset_I0_train.csv / dataset_I1_train.csv\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta base: {ruta_base}\")\n",
    "\n",
    "    for nombre in os.listdir(ruta_base):\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ Omitido (no es *_train.csv): {nombre}\")\n",
    "            continue\n",
    "\n",
    "        # patr√≥n: nombre_dataset_I(entero)_train.csv\n",
    "        m = re.match(r\"(.+?)_I(\\d+)_train\\.csv$\", nombre)\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ No coincide patr√≥n base con I*: {nombre}\")\n",
    "            continue\n",
    "\n",
    "        dataset_logico = m.group(1)\n",
    "        grado_limpieza = int(m.group(2))\n",
    "        ruta_train_csv = os.path.join(ruta_base, nombre)\n",
    "\n",
    "        # test asociado: SIEMPRE es dataset_logico_test.csv\n",
    "        ruta_test_csv = os.path.join(ruta_base, f\"{dataset_logico}_test.csv\")\n",
    "        if not os.path.isfile(ruta_test_csv):\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  Falta test para {nombre}, se omite.\")\n",
    "            continue\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"base\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=\"base\",\n",
    "            valor_densidad=None,\n",
    "            valor_riesgo=None,\n",
    "            criterio_pureza=None,\n",
    "            grado_limpieza=grado_limpieza,\n",
    "        ))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) CL√ÅSICOS: smote_XXX_I0_train.csv, borderline_XXX_I1_train.csv, adasyn_XXX_I0_train.csv\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta cl√°sicos: {ruta_clasicos}\")\n",
    "\n",
    "    for nombre in os.listdir(ruta_clasicos):\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            continue\n",
    "\n",
    "        # patr√≥n: tecnica_dataset_I(0|1)_train.csv\n",
    "        # ejemplo: adasyn_us_crime_I1_train.csv\n",
    "        m = re.match(r\"(.+?)_(.+?)_I(\\d+)_train\\.csv$\", nombre)\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No cumple patr√≥n cl√°sicos: {nombre}\")\n",
    "            continue\n",
    "\n",
    "        tecnica = m.group(1)\n",
    "        dataset_logico = m.group(2)\n",
    "        grado_limpieza = int(m.group(3))\n",
    "\n",
    "        ruta_train_csv = os.path.join(ruta_clasicos, nombre)\n",
    "        ruta_test_csv = os.path.join(ruta_base, f\"{dataset_logico}_test.csv\")\n",
    "\n",
    "        if not os.path.isfile(ruta_test_csv):\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No hay test base para dataset '{dataset_logico}', se omite {nombre}\")\n",
    "            continue\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"clasico\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=tecnica.lower(),\n",
    "            valor_densidad=None,\n",
    "            valor_riesgo=None,\n",
    "            criterio_pureza=None,\n",
    "            grado_limpieza=grado_limpieza,\n",
    "        ))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) PC-SMOTE: pcsmote_dataset_Dxx_Ryy_Pproporcion_I0_train.csv\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta aumentados: {ruta_aumentados}\")\n",
    "\n",
    "    for nombre in os.listdir(ruta_aumentados):\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            continue\n",
    "\n",
    "        m = re.match(\n",
    "            r\"pcsmote_(.+?)_D(\\d+)_R(\\d+)_P(.+?)_I(\\d+)_train\\.csv\",\n",
    "            nombre\n",
    "        )\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ Omitido (no es pcsmote v√°lido): {nombre}\")\n",
    "            continue\n",
    "\n",
    "        dataset_logico = m.group(1)\n",
    "        valor_densidad = int(m.group(2))\n",
    "        valor_riesgo   = int(m.group(3))\n",
    "        criterio_pureza = m.group(4)\n",
    "        grado_limpieza  = int(m.group(5))\n",
    "\n",
    "        ruta_train_csv = os.path.join(ruta_aumentados, nombre)\n",
    "        ruta_test_csv = os.path.join(ruta_base, f\"{dataset_logico}_test.csv\")\n",
    "\n",
    "        if not os.path.isfile(ruta_test_csv):\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No hay test base para dataset '{dataset_logico}', se omite {nombre}\")\n",
    "            continue\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"pcsmote\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=\"pcsmote\",\n",
    "            valor_densidad=valor_densidad,\n",
    "            valor_riesgo=valor_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            grado_limpieza=grado_limpieza,\n",
    "        ))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üìä Total combinaciones descubiertas: {len(combinaciones)}\")\n",
    "\n",
    "    return combinaciones\n",
    "\n",
    "\n",
    "def definir_configuracion_busqueda_para_dataset(X_train, nombre_dataset_logico, tipo_combination):\n",
    "    \"\"\"\n",
    "    Define la configuraci√≥n de b√∫squeda: esquema de CV (n_splits), n_iter (RandomizedSearchCV) y n_jobs.\n",
    "    Mantiene: Shuttle aumentado ‚Üí 2 folds; Shuttle o n‚â•10k ‚Üí 3 folds; resto ‚Üí 5 folds.\n",
    "    \"\"\"\n",
    "    n_muestras = X_train.shape[0]\n",
    "    es_shuttle = (nombre_dataset_logico.lower() == \"shuttle\")\n",
    "    if es_shuttle and tipo_combination == \"aumentado\":\n",
    "        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "    elif es_shuttle or n_muestras >= 10000:\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    cpu = os.cpu_count() or 4\n",
    "    n_jobs = 1 if (es_shuttle or n_muestras >= 10000) else max(1, min(4, cpu // 2))\n",
    "    n_iter = 25 if (es_shuttle or n_muestras >= 10000) else N_ITER_BUSQUEDA_POR_DEFECTO\n",
    "    return dict(cv=cv, n_jobs=n_jobs, n_iter=n_iter)\n",
    "\n",
    "# =========================\n",
    "# Registro de modelos\n",
    "# =========================\n",
    "def construir_estimador_y_espacio_svm():\n",
    "    \"\"\"Devuelve (pipeline SVM, espacio de hiperpar√°metros) con kernels linear/rbf.\"\"\"\n",
    "    est = Pipeline([('classifier', SVC(random_state=RANDOM_STATE, probability=False, max_iter=5000, cache_size=400))])\n",
    "    # espacio de busqueda de hiperparametros,\n",
    "    # combinaciones posibles para cada\n",
    "    # hiperparametro del modelo\n",
    "    space = {\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__C': loguniform(1e-3, 1e2),\n",
    "        'classifier__gamma': loguniform(1e-4, 1e0),  # ignorado cuando kernel='linear'\n",
    "        'classifier__shrinking': [True, False],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "def construir_estimador_y_espacio_regresion_logistica_saga():\n",
    "    \"\"\"Devuelve (pipeline LR con solver saga) evitando combinaciones inv√°lidas; penalizaci√≥n L1/L2.\"\"\"\n",
    "    est = Pipeline([('classifier', LogisticRegression(max_iter=5000, random_state=RANDOM_STATE, solver='saga'))])\n",
    "    # espacio de busqueda de hiperparametros,\n",
    "    # combinaciones posibles para cada\n",
    "    # hiperparametro del modelo\n",
    "    space = {\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__C': loguniform(1e-4, 1e2),\n",
    "        'classifier__fit_intercept': [True, False],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "        'classifier__tol': loguniform(1e-5, 1e-3),\n",
    "        'classifier__l1_ratio': [None],  # evitamos elasticnet para simplificar\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "def construir_estimador_y_espacio_random_forest():\n",
    "    est = Pipeline([\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1,\n",
    "            bootstrap=True,\n",
    "            oob_score=False,\n",
    "            n_estimators=150,\n",
    "            max_depth=None,\n",
    "            max_features='sqrt',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            class_weight=None,\n",
    "            criterion='gini'\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Sin espacio de b√∫squeda (lo pod√©s manejar aparte)\n",
    "    space = {}\n",
    "    return est, space\n",
    "\n",
    "\n",
    "REGISTRO_MODELOS = {\n",
    "    # \"LogisticRegression\": construir_estimador_y_espacio_regresion_logistica_saga,\n",
    "    \"RandomForest\": construir_estimador_y_espacio_random_forest,\n",
    "    # \"SVM\": construir_estimador_y_espacio_svm\n",
    "}\n",
    "ORDEN_MODELOS = [\n",
    "                #   \"LogisticRegression\",\n",
    "                  \"RandomForest\", \n",
    "                #   \"SVM\"\n",
    "                  ]  # r√°pido ‚Üí lento\n",
    "\n",
    "# =========================\n",
    "# Entrenamiento y evaluaci√≥n\n",
    "# =========================\n",
    "def ejecutar_rs_y_comparar_cv_con_test(\n",
    "    estimator,\n",
    "    space,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    configuracion_busqueda,\n",
    "    verbose=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta RandomizedSearchCV con scoring m√∫ltiple (refit=f1_macro).\n",
    "    Devuelve:\n",
    "      - mejores_params: dict de hiperpar√°metros del mejor modelo\n",
    "      - tiempo: segundos de b√∫squeda\n",
    "      - cv: m√©tricas promedio de validaci√≥n cruzada del mejor candidato\n",
    "      - test: m√©tricas sobre el conjunto de test, incluyendo m√©tricas adicionales\n",
    "    \"\"\"\n",
    "    inicio = time.perf_counter()\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=space,\n",
    "        n_iter=configuracion_busqueda[\"n_iter\"],\n",
    "        scoring=SCORING_MULTIPLE,\n",
    "        refit=SCORING_REFIT,\n",
    "        cv=configuracion_busqueda[\"cv\"],\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=configuracion_busqueda[\"n_jobs\"],\n",
    "        verbose=verbose,\n",
    "        error_score=np.nan,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    fin = time.perf_counter()\n",
    "    elapsed = float(fin - inicio)\n",
    "\n",
    "    # ===================== M√âTRICAS DE CV =====================\n",
    "    cv_res = search.cv_results_\n",
    "    idx_best = search.best_index_\n",
    "\n",
    "    cv_f1 = float(cv_res[\"mean_test_f1_macro\"][idx_best])\n",
    "    cv_bacc = float(cv_res[\"mean_test_balanced_accuracy\"][idx_best])\n",
    "    cv_mcc = float(cv_res[\"mean_test_mcc\"][idx_best])\n",
    "    cv_kappa = float(cv_res[\"mean_test_cohen_kappa\"][idx_best])\n",
    "\n",
    "    # ===================== M√âTRICAS EN TEST =====================\n",
    "    best_est = search.best_estimator_\n",
    "    y_pred = best_est.predict(X_test)\n",
    "\n",
    "    # m√©tricas ya existentes\n",
    "    test_f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    test_bacc = float(balanced_accuracy_score(y_test, y_pred))\n",
    "    test_mcc = float(matthews_corrcoef(y_test, y_pred))\n",
    "    test_kappa = float(cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "    # exactitud cl√°sica\n",
    "    test_accuracy = float(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # m√©tricas por clase (F1 y recall)\n",
    "    clases, counts = np.unique(y_test, return_counts=True)\n",
    "    f1_por_clase = f1_score(y_test, y_pred, labels=clases, average=None)\n",
    "    recall_por_clase = recall_score(y_test, y_pred, labels=clases, average=None)\n",
    "\n",
    "    # recall de la clase minoritaria (la de menor soporte en y_test)\n",
    "    idx_min = int(np.argmin(counts))\n",
    "    recall_minoritario = float(recall_por_clase[idx_min])\n",
    "\n",
    "    # G-mean macro: media geom√©trica de los recalls por clase\n",
    "    if np.any(recall_por_clase < 0):\n",
    "        test_gmean = float(\"nan\")\n",
    "    else:\n",
    "        # si alguna clase tiene recall 0, la gmean es 0\n",
    "        test_gmean = float(np.prod(recall_por_clase) ** (1.0 / len(recall_por_clase)))\n",
    "\n",
    "    # ===================== ROC AUC y AUC-PR (si hay probabilidades) =====================\n",
    "    roc_auc_macro_ovr = float(\"nan\")\n",
    "    auc_pr_macro = float(\"nan\")\n",
    "    y_proba = None\n",
    "\n",
    "    if hasattr(best_est, \"predict_proba\"):\n",
    "        try:\n",
    "            y_proba = best_est.predict_proba(X_test)\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            roc_auc_macro_ovr = float(\n",
    "                roc_auc_score(y_test, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "            )\n",
    "        except Exception:\n",
    "            roc_auc_macro_ovr = float(\"nan\")\n",
    "        try:\n",
    "            auc_pr_macro = float(\n",
    "                average_precision_score(y_test, y_proba, average=\"macro\")\n",
    "            )\n",
    "        except Exception:\n",
    "            auc_pr_macro = float(\"nan\")\n",
    "\n",
    "    return dict(\n",
    "        mejores_params=search.best_params_,\n",
    "        tiempo=elapsed,\n",
    "        cv=dict(\n",
    "            f1=cv_f1,\n",
    "            bacc=cv_bacc,\n",
    "            mcc=cv_mcc,\n",
    "            kappa=cv_kappa,\n",
    "        ),\n",
    "        test=dict(\n",
    "            f1=test_f1,\n",
    "            bacc=test_bacc,\n",
    "            mcc=test_mcc,\n",
    "            kappa=test_kappa,\n",
    "            accuracy=test_accuracy,\n",
    "            f1_por_clase=f1_por_clase,\n",
    "            recall_minoritario=recall_minoritario,\n",
    "            gmean=test_gmean,\n",
    "            roc_auc_macro_ovr=roc_auc_macro_ovr,\n",
    "            auc_pr_macro=auc_pr_macro,\n",
    "            clases=clases,\n",
    "        ),\n",
    "    )\n",
    "# =========================\n",
    "# Orquestaci√≥n (lista plana de tareas)\n",
    "# --- Pol√≠ticas globales de exclusi√≥n (en min√∫sculas) ---\n",
    "EXCLUIR_DATASETS = {}  # agregar aqu√≠ otros: {\"shuttle\", \"ecoli\", ...}\n",
    "# =========================\n",
    "\n",
    "def construir_lista_plana_de_tareas(model_registry, dataset_combinations, orden_modelos,\n",
    "                                    excluir_datasets=EXCLUIR_DATASETS, verbose=True):\n",
    "    \"\"\"\n",
    "    Crea una lista plana de tareas (modelo, combinaci√≥n) y aplica pol√≠ticas de exclusi√≥n.\n",
    "    - excluir_datasets: conjunto de nombres de dataset (en min√∫sculas) a excluir por completo.\n",
    "    - Mantiene la pol√≠tica existente de omitir SVM en Shuttle aumentado si est√° activa.\n",
    "    \"\"\"\n",
    "    tareas = []\n",
    "    excluidos_por_dataset = 0\n",
    "    excluidos_por_politica_svm_shuttle = 0\n",
    "\n",
    "    for nombre_modelo in orden_modelos:\n",
    "        for combo in dataset_combinations:\n",
    "            ds = combo.dataset_logico.lower()\n",
    "\n",
    "            # 1) Excluir datasets completos (p. ej., shuttle)\n",
    "            if ds in (excluir_datasets or set()):\n",
    "                excluidos_por_dataset += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Pol√≠tica original: omitir SVM en Shuttle aumentado\n",
    "            if (OMITIR_SVM_EN_SHUTTLE_AUMENTADO and\n",
    "                nombre_modelo == \"SVM\" and\n",
    "                ds == \"shuttle\" and\n",
    "                combo.tipo_combination == \"aumentado\"):\n",
    "                excluidos_por_politica_svm_shuttle += 1\n",
    "                continue\n",
    "\n",
    "            tareas.append((nombre_modelo, combo))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üßÆ Tareas planificadas: {len(tareas)} \"\n",
    "              f\"(excluidos por dataset: {excluidos_por_dataset}, \"\n",
    "              f\"por pol√≠tica SVM-Shuttle‚Üë: {excluidos_por_politica_svm_shuttle})\")\n",
    "    return tareas\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exportaci√≥n a Excel\n",
    "# =========================\n",
    "def generar_archivo_excel_resultados(ruta_excel, df_resultados, df_resumen_base_vs_aumentado, df_rankings):\n",
    "    \"\"\"\n",
    "    Escribe un archivo Excel con tres hojas:\n",
    "      - 'resultados': filas con CV vs Test por (dataset, tipo, modelo, combinaci√≥n).\n",
    "      - 'resumen_base_vs_aumentado': promedios por dataset y modelo + delta (aum - base) en F1-Test y F1-CV.\n",
    "      - 'rankings': ranking por dataset y modelo, ordenado por F1-Test.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(ruta_excel, engine=\"xlsxwriter\") as writer:\n",
    "        df_resultados.to_excel(writer, sheet_name=\"resultados\", index=False)\n",
    "        df_resumen_base_vs_aumentado.to_excel(writer, sheet_name=\"resumen_base_vs_aumentado\", index=True)\n",
    "        df_rankings.to_excel(writer, sheet_name=\"rankings\", index=False)\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "\n",
    "def ejecutar_experimentos_y_generar_excel():\n",
    "    \"\"\"\n",
    "    Ejecuta todos los experimentos sobre datasets base y aumentados,\n",
    "    con b√∫squeda aleatoria de hiperpar√°metros y m√©tricas CV vs Test.\n",
    "    Muestra progreso detallado por consola y exporta resultados a Excel.\n",
    "    \"\"\"\n",
    "    # 1 -- Enumerar combinaciones base y aumentadas\n",
    "    print(\"üîé Enumerando combinaciones base y aumentadas...\")\n",
    "\n",
    "    # ORDEN CORRECTO DE PAR√ÅMETROS:\n",
    "    #   ruta_base          = RUTA_DATASETS_BASE\n",
    "    #   ruta_clasicos      = RUTA_DATASETS_CLASICOS\n",
    "    #   ruta_aumentados    = RUTA_DATASETS_AUMENTADOS\n",
    "\n",
    "    combinaciones = enumerar_combinaciones_base_y_aumentadas(\n",
    "        ruta_base=RUTA_DATASETS_BASE,\n",
    "        ruta_clasicos=RUTA_DATASETS_CLASICOS,\n",
    "        ruta_aumentados=RUTA_DATASETS_AUMENTADOS,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if not combinaciones:\n",
    "        print(\"‚ùå No se encontraron combinaciones de datasets.\")\n",
    "        return\n",
    "\n",
    "    datasets_con_base = {c.dataset_logico for c in combinaciones if c.tipo_combination == \"base\"}\n",
    "    if not datasets_con_base:\n",
    "        print(\"‚ùå No hay datasets base para comparar.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # 2 -- Construir plan de tareas\n",
    "    \"\"\"\n",
    "    Con las combinaciones descubiertas, construimos plan de tareas\n",
    "    \"\"\"\n",
    "    tareas = construir_lista_plana_de_tareas(\n",
    "        REGISTRO_MODELOS,\n",
    "        combinaciones,\n",
    "        ORDEN_MODELOS,\n",
    "        excluir_datasets={\"shuttle\"},   # datasets a excluir por completo\n",
    "        verbose=True\n",
    "    )\n",
    "    total_tareas = len(tareas)\n",
    "    print(f\"üì¶ Total de tareas planificadas: {total_tareas}\")\n",
    "\n",
    "    registros = []\n",
    "    inicio_total = time.perf_counter()\n",
    "\n",
    "    # 3 -- Ejecutar tareas\n",
    "    for idx, (nombre_modelo, combo) in enumerate(tareas, start=1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üèÅ [{idx}/{total_tareas}] Dataset: {combo.dataset_logico} | \"\n",
    "              f\"Tipo: {combo.tipo_combination} | Modelo: {nombre_modelo}\")\n",
    "        print(f\"üìÇ Train: {os.path.basename(combo.ruta_train_csv)}\")\n",
    "\n",
    "        try:\n",
    "            X_train, y_train = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_train_csv)\n",
    "            X_test, y_test = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_test_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error leyendo archivos CSV: {e}\")\n",
    "            continue\n",
    "\n",
    "        configuracion_busqueda = definir_configuracion_busqueda_para_dataset(\n",
    "            X_train, combo.dataset_logico, combo.tipo_combination\n",
    "        )\n",
    "        print(f\"‚öôÔ∏è  Configuraci√≥n de b√∫squeda: \"\n",
    "              f\"n_iter={configuracion_busqueda['n_iter']}, \"\n",
    "              f\"folds={configuracion_busqueda['cv'].n_splits}, \"\n",
    "              f\"n_jobs={configuracion_busqueda['n_jobs']}\")\n",
    "\n",
    "        # 3.1 -- Construir estimador y espacio de hiperpar√°metros\n",
    "        # por modelo seleccionado obtener el estimador y espacio\n",
    "        estimator, space = REGISTRO_MODELOS[nombre_modelo]()\n",
    "        print(f\"üöÄ Iniciando RandomizedSearchCV...\")\n",
    "\n",
    "        try:\n",
    "            resultados = ejecutar_rs_y_comparar_cv_con_test(\n",
    "                estimator, space, X_train, y_train, X_test, y_test,\n",
    "                configuracion_busqueda=configuracion_busqueda, verbose=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error durante la b√∫squeda: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚úÖ B√∫squeda completada en {resultados['tiempo']:.2f} s\")\n",
    "        print(f\"üìä F1(CV): {resultados['cv']['f1']:.4f} | \"\n",
    "              f\"F1(Test): {resultados['test']['f1']:.4f}\")\n",
    "\n",
    "        # formateo F1 por clase como string legible para Excel\n",
    "        f1_por_clase = resultados[\"test\"][\"f1_por_clase\"]\n",
    "        f1_por_clase_str = \";\".join([f\"{v:.4f}\" for v in f1_por_clase])\n",
    "\n",
    "        registros.append(asdict(RegistroRendimiento(\n",
    "            dataset_logico=combo.dataset_logico,\n",
    "            tipo_combination=combo.tipo_combination,\n",
    "            nombre_modelo_aprendizaje=nombre_modelo,\n",
    "            tecnica_aumento=combo.tecnica_aumento,\n",
    "            valor_densidad=combo.valor_densidad,\n",
    "            valor_riesgo=combo.valor_riesgo,\n",
    "            criterio_pureza=combo.criterio_pureza,\n",
    "            grado_limpieza=combo.grado_limpieza,\n",
    "            cantidad_train=int(X_train.shape[0]),\n",
    "            cantidad_test=int(X_test.shape[0]),\n",
    "            cantidad_caracteristicas=int(X_train.shape[1]),\n",
    "            cv_f1_macro=resultados[\"cv\"][\"f1\"],\n",
    "            cv_balanced_accuracy=resultados[\"cv\"][\"bacc\"],\n",
    "            cv_mcc=resultados[\"cv\"][\"mcc\"],\n",
    "            cv_cohen_kappa=resultados[\"cv\"][\"kappa\"],\n",
    "            test_f1_macro=resultados[\"test\"][\"f1\"],\n",
    "            test_balanced_accuracy=resultados[\"test\"][\"bacc\"],\n",
    "            test_mcc=resultados[\"test\"][\"mcc\"],\n",
    "            test_cohen_kappa=resultados[\"test\"][\"kappa\"],\n",
    "            test_accuracy=resultados[\"test\"][\"accuracy\"],\n",
    "            test_f1_por_clase=f1_por_clase_str,\n",
    "            test_recall_minoritario=resultados[\"test\"][\"recall_minoritario\"],\n",
    "            test_gmean=resultados[\"test\"][\"gmean\"],\n",
    "            test_roc_auc_macro_ovr=resultados[\"test\"][\"roc_auc_macro_ovr\"],\n",
    "            test_auc_pr_macro=resultados[\"test\"][\"auc_pr_macro\"],\n",
    "            mejores_hiperparametros=str(resultados[\"mejores_params\"]),\n",
    "            tiempo_busqueda_seg=float(resultados[\"tiempo\"]),\n",
    "        )))\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # ----------------- DataFrames finales -----------------\n",
    "    print(\"\\nüìä Compilando resultados globales...\")\n",
    "    df_resultados = pd.DataFrame(registros)\n",
    "\n",
    "    resumen = (df_resultados\n",
    "        .groupby([\"dataset_logico\", \"nombre_modelo_aprendizaje\", \"tipo_combination\"])\n",
    "        .agg(cv_f1_prom=(\"cv_f1_macro\", \"mean\"),\n",
    "             test_f1_prom=(\"test_f1_macro\", \"mean\"))\n",
    "        .unstack(\"tipo_combination\"))\n",
    "    for col in [(\"cv_f1_prom\", \"base\"), (\"cv_f1_prom\", \"aumentado\"), (\"test_f1_prom\", \"base\"), (\"test_f1_prom\", \"aumentado\")]:\n",
    "        if col not in resumen.columns:\n",
    "            resumen[col] = np.nan\n",
    "    resumen = resumen.assign(\n",
    "        delta_cv_f1 = resumen[(\"cv_f1_prom\", \"aumentado\")] - resumen[(\"cv_f1_prom\", \"base\")],\n",
    "        delta_test_f1 = resumen[(\"test_f1_prom\", \"aumentado\")] - resumen[(\"test_f1_prom\", \"base\")]\n",
    "    )\n",
    "\n",
    "    lista_rankings = []\n",
    "    for ds in sorted(df_resultados[\"dataset_logico\"].unique()):\n",
    "        sub_ds = df_resultados[df_resultados[\"dataset_logico\"] == ds]\n",
    "        for mdl in ORDEN_MODELOS:\n",
    "            sub_mdl = sub_ds[sub_ds[\"nombre_modelo_aprendizaje\"] == mdl].copy()\n",
    "            if sub_mdl.empty:\n",
    "                continue\n",
    "            sub_mdl.sort_values(\"test_f1_macro\", ascending=False, inplace=True)\n",
    "            sub_mdl[\"ranking\"] = range(1, len(sub_mdl) + 1)\n",
    "            lista_rankings.append(sub_mdl[[\n",
    "                \"dataset_logico\",\"nombre_modelo_aprendizaje\",\"ranking\",\n",
    "                \"valor_riesgo\",\"valor_densidad\",\"criterio_pureza\",\n",
    "                \"tipo_combination\",\"test_f1_macro\",\"cv_f1_macro\",\n",
    "                \"tecnica_aumento\",\"mejores_hiperparametros\"\n",
    "            ]])\n",
    "    df_rankings = pd.concat(lista_rankings, ignore_index=True) if lista_rankings else pd.DataFrame()\n",
    "\n",
    "    generar_archivo_excel_resultados(\n",
    "        ruta_excel=NOMBRE_ARCHIVO_EXCEL,\n",
    "        df_resultados=df_resultados,\n",
    "        df_resumen_base_vs_aumentado=resumen,\n",
    "        df_rankings=df_rankings\n",
    "    )\n",
    "\n",
    "    fin_total = time.perf_counter()\n",
    "    duracion = round(fin_total - inicio_total, 2)\n",
    "    print(f\"\\nüèÅ Ejecuci√≥n total completada en {duracion} s\")\n",
    "    print(f\"üìò Archivo Excel generado: {NOMBRE_ARCHIVO_EXCEL}\")\n",
    "\n",
    "\n",
    "ejecutar_experimentos_y_generar_excel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
