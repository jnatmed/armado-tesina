{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la t√©cnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import gc, os, time                                                      # Operaciones con el sistema de archivos\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa: F401  <- necesario\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Evitar sobre-suscripci√≥n de CPU (BLAS/OpenMP)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "SCORING_PRIMARY = 'f1_macro'  # √∫nico m√©trico para Halving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc87fa1",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n de modelos con validaci√≥n cruzada estratificada\n",
    "\n",
    "Para evaluar el rendimiento de los modelos de clasificaci√≥n sobre los datasets previamente balanceados, se utiliz√≥ validaci√≥n cruzada estratificada de 5 particiones (Stratified K-Fold con *k=5*). Este m√©todo garantiza que en cada fold de entrenamiento y validaci√≥n se preserve la proporci√≥n original de clases, lo cual es especialmente importante en tareas de clasificaci√≥n multiclase con datasets balanceados artificialmente.\n",
    "\n",
    "Durante el proceso, cada modelo es entrenado y evaluado cinco veces, cada vez usando un subconjunto distinto como conjunto de prueba y el resto como conjunto de entrenamiento. Las m√©tricas calculadas en cada iteraci√≥n (F1-score macro, balanced accuracy, MCC y kappa de Cohen) se promedian para obtener un valor representativo y del rendimiento general del modelo sobre ese dataset aumentado.\n",
    "\n",
    "Este enfoque evita sobreajuste y proporciona una evaluaci√≥n m√°s confiable que una simple divisi√≥n train/test, permitiendo comparar de forma justa distintas configuraciones de sobremuestreo y modelos de clasificaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5320c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 361\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    351\u001b[39m     search = make_halving(\n\u001b[32m    352\u001b[39m         estimator=pipeline_local,\n\u001b[32m    353\u001b[39m         params=params_local,\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m         min_resources=min_res_samples\n\u001b[32m    359\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m elapsed = \u001b[38;5;28mround\u001b[39m(time.perf_counter() - t0, \u001b[32m3\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# CV (mejor √≠ndice)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:253\u001b[39m, in \u001b[36mBaseSuccessiveHalving.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mself\u001b[39m._check_input_parameters(\n\u001b[32m    248\u001b[39m     X=X, y=y, split_params=routed_params.splitter.split\n\u001b[32m    249\u001b[39m )\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m._n_samples_orig = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m.best_score_ = \u001b[38;5;28mself\u001b[39m.cv_results_[\u001b[33m\"\u001b[39m\u001b[33mmean_test_score\u001b[39m\u001b[33m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m.best_index_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:357\u001b[39m, in \u001b[36mBaseSuccessiveHalving._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m    350\u001b[39m     cv = \u001b[38;5;28mself\u001b[39m._checked_cv_orig\n\u001b[32m    352\u001b[39m more_results = {\n\u001b[32m    353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33miter\u001b[39m\u001b[33m\"\u001b[39m: [itr] * n_candidates,\n\u001b[32m    354\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_resources\u001b[39m\u001b[33m\"\u001b[39m: [n_resources] * n_candidates,\n\u001b[32m    355\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m results = \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmore_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmore_results\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m n_candidates_to_keep = ceil(n_candidates / \u001b[38;5;28mself\u001b[39m.factor)\n\u001b[32m    362\u001b[39m candidate_params = _top_k(results, n_candidates_to_keep, itr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Config optimizaci√≥n\n",
    "# -----------------------------\n",
    "SCORING_PRIMARY = 'f1_macro'   # m√©trico √∫nico para Halving (refit = scoring)\n",
    "\n",
    "# -----------------------------\n",
    "# Modelos base + espacios (afinados)\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "SVM (Maquinas de soporte vectorial): Soporta bien datasets peque√±os/medianos; puede ser lento en grandes.\n",
    "LogisticRegression (Regresion logistica): R√°pido y eficiente; buen rendimiento en muchos casos.\n",
    "RandomForest (Bosques aleatorios): Robusto y vers√°til; maneja bien datos ruidosos y no line\n",
    "\"\"\"\n",
    "modelos = {\n",
    "    \"SVM\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            # max_iter : maximo numero de iteraciones que el algoritmo puede realizar, si se deja -1 no hay limite\n",
    "            # cache_size: tama√±o de la cach√© en MB para almacenar los c√°lculos del kernel\n",
    "            ('classifier', SVC(random_state=42, probability=False, max_iter=5000, cache_size=400))\n",
    "        ]),\n",
    "        \"param_distributions\": {\n",
    "            # rbf es \n",
    "            'classifier__kernel': ['linear', 'rbf'],\n",
    "            'classifier__C': loguniform(1e-3, 1e2),\n",
    "            'classifier__gamma': loguniform(1e-4, 1e0),  # ignorado con 'linear'\n",
    "            'classifier__shrinking': [True, False],\n",
    "            'classifier__class_weight': [None, 'balanced']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"LogisticRegression\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('classifier', LogisticRegression(max_iter=5000, random_state=42))\n",
    "        ]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__solver': ['liblinear', 'saga'],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__C': loguniform(1e-4, 1e2),\n",
    "            'classifier__fit_intercept': [True, False],\n",
    "            'classifier__class_weight': [None, 'balanced'],\n",
    "            'classifier__tol': loguniform(1e-5, 1e-3),\n",
    "            'classifier__dual': [False]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([('classifier', RandomForestClassifier(\n",
    "            random_state=42, n_jobs=1, bootstrap=True, oob_score=False\n",
    "        ))]),\n",
    "        # IMPORTANTE: NO incluir 'classifier__n_estimators' en este espacio cuando se use Halving con resource='classifier__n_estimators'\n",
    "        \"param_distributions\": {\n",
    "            'classifier__criterion': ['gini', 'entropy'],\n",
    "            'classifier__max_depth': [None, 10, 20, 40],\n",
    "            'classifier__min_samples_split': randint(2, 21),\n",
    "            'classifier__min_samples_leaf': randint(1, 11),\n",
    "            'classifier__max_features': ['sqrt', 'log2'],\n",
    "            'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "            'classifier__ccp_alpha': uniform(0.0, 0.01)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Orden: primero lo m√°s r√°pido/estable\n",
    "orden_modelos = [\"LogisticRegression\", \"RandomForest\", \"SVM\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Rutas\n",
    "# -----------------------------\n",
    "ruta_aug  = \"../datasets/datasets_aumentados/\"\n",
    "ruta_base = \"../datasets/datasets_aumentados/base/\"\n",
    "dir_out   = \"../resultados\"\n",
    "os.makedirs(dir_out, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Listado de pares train/test\n",
    "# -----------------------------\n",
    "def listar_pares(ruta, tipo):\n",
    "    pares = []\n",
    "    if not os.path.isdir(ruta):\n",
    "        return pares\n",
    "    for f in os.listdir(ruta):\n",
    "        if not (f.endswith(\"_train.csv\") and os.path.isfile(os.path.join(ruta, f))):\n",
    "            continue\n",
    "        path_train = os.path.join(ruta, f)\n",
    "        path_test  = os.path.join(ruta, f.replace(\"_train.csv\", \"_test.csv\"))\n",
    "        if not os.path.isfile(path_test):\n",
    "            print(f\"‚ö†Ô∏è Falta el test para: {f} -> esperado: {os.path.basename(path_test)}\")\n",
    "            continue\n",
    "        pares.append({\n",
    "            \"tipo\": tipo,\n",
    "            \"nombre_train\": f,\n",
    "            \"path_train\": path_train,\n",
    "            \"path_test\": path_test\n",
    "        })\n",
    "    return pares\n",
    "\n",
    "def make_cv_splits(cv_obj, X, y):\n",
    "    \"\"\"Materializa (train_idx, valid_idx) para reuso entre modelos.\"\"\"\n",
    "    return list(cv_obj.split(X, y))\n",
    "\n",
    "# -----------------------------\n",
    "# Resumible: claves ya calculadas\n",
    "# -----------------------------\n",
    "def cargar_claves_existentes():\n",
    "    done = {m: set() for m in modelos}\n",
    "    for m in modelos:\n",
    "        p = os.path.join(dir_out, f\"resultados_{m}.csv\")\n",
    "        if os.path.exists(p):\n",
    "            try:\n",
    "                df_prev = pd.read_csv(p)\n",
    "                for _, r in df_prev.iterrows():\n",
    "                    done[m].add((\n",
    "                        str(r.get('dataset')), str(r.get('tipo', '')),\n",
    "                        str(r.get('tecnica')), str(r.get('densidad')),\n",
    "                        str(r.get('riesgo')), str(r.get('pureza'))\n",
    "                    ))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return done\n",
    "\n",
    "# -----------------------------\n",
    "# Helper de carga (float32)\n",
    "# -----------------------------\n",
    "def cargar_xy(path_csv):\n",
    "    df = pd.read_csv(path_csv)\n",
    "    if \"target\" in df.columns:\n",
    "        X = df.drop(columns=[\"target\"]).to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df[\"target\"].to_numpy()\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df.iloc[:, -1].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "pares = listar_pares(ruta_aug, \"aumentado\") + listar_pares(ruta_base, \"base\")\n",
    "pares = sorted(pares, key=lambda x: (x[\"tipo\"], x[\"nombre_train\"]))\n",
    "total_pares = len(pares)\n",
    "\n",
    "# -----------------------------\n",
    "# M√©tricas CV (referencia; Halving usa solo SCORING_PRIMARY)\n",
    "# -----------------------------\n",
    "scoring = {\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'balanced_accuracy': 'balanced_accuracy',\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'cohen_kappa': make_scorer(cohen_kappa_score)\n",
    "}\n",
    "\n",
    "ya_hechos = cargar_claves_existentes()\n",
    "resultados_por_modelo = {nombre: [] for nombre in modelos}\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers Halving\n",
    "# -----------------------------\n",
    "def make_halving(estimator, params, cv, n_jobs, verbose, factor, min_resources):\n",
    "    \"\"\"\n",
    "    Halving para LR / SVM (resource='n_samples' por defecto).\n",
    "    min_resources: entero (p.ej., 20/30/50).\n",
    "    \"\"\"\n",
    "    return HalvingRandomSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=params,\n",
    "        cv=cv,\n",
    "        scoring=SCORING_PRIMARY,\n",
    "        refit=SCORING_PRIMARY,\n",
    "        n_jobs=n_jobs,\n",
    "        factor=factor,\n",
    "        min_resources=min_resources,\n",
    "        aggressive_elimination=True,\n",
    "        random_state=42,\n",
    "        verbose=verbose,\n",
    "        error_score=np.nan,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "def make_halving_rf(estimator, params, cv, n_jobs, verbose, factor, min_trees, max_trees):\n",
    "    \"\"\"\n",
    "    Halving para RandomForest variando 'classifier__n_estimators'.\n",
    "    - NO incluir 'classifier__n_estimators' en 'params'.\n",
    "    - Se DEBE fijar max_resources (si no, queda 'auto' y solo permite 'n_samples').\n",
    "    \"\"\"\n",
    "    return HalvingRandomSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=params,\n",
    "        cv=cv,\n",
    "        scoring=SCORING_PRIMARY,\n",
    "        refit=SCORING_PRIMARY,\n",
    "        n_jobs=n_jobs,\n",
    "        factor=factor,\n",
    "        resource='classifier__n_estimators',\n",
    "        min_resources=min_trees,\n",
    "        max_resources=max_trees,\n",
    "        aggressive_elimination=True,\n",
    "        random_state=42,\n",
    "        verbose=verbose,\n",
    "        error_score=np.nan,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "# =============================\n",
    "# Loop principal\n",
    "# =============================\n",
    "for idx_par, item in enumerate(pares, start=1):\n",
    "    archivo_train = item[\"nombre_train\"]\n",
    "    ruta_train = item[\"path_train\"]; ruta_test = item[\"path_test\"]\n",
    "    tipo = item[\"tipo\"]\n",
    "\n",
    "    # Parseo nombre\n",
    "    nombre = archivo_train.replace(\".csv\", \"\")\n",
    "    partes = nombre.split(\"_\")\n",
    "    if tipo == \"aumentado\":\n",
    "        if len(partes) < 6:\n",
    "            print(f\"‚ö†Ô∏è Nombre inv√°lido/incompleto (aumentado): {archivo_train}\")\n",
    "            continue\n",
    "        tecnica = partes[0]; nombre_dataset = partes[1]\n",
    "        densidad = partes[2][1:]; riesgo = partes[3][1:]; pureza = partes[4][1:]\n",
    "    else:\n",
    "        tecnica = \"base\"; nombre_dataset = nombre.replace(\"_train\", \"\")\n",
    "        densidad = riesgo = pureza = \"NA\"\n",
    "\n",
    "    print(f\"\\nüìÇ ({idx_par}/{total_pares}) Par: {archivo_train}  (tipo: {tipo})\")\n",
    "    print(f\"üîé T√©cnica: {tecnica} | Dataset: {nombre_dataset} | Densidad: {densidad} | Riesgo: {riesgo} | Pureza: {pureza}\")\n",
    "\n",
    "    # Carga de datos\n",
    "    try:\n",
    "        X_train, y_train = cargar_xy(ruta_train)\n",
    "        X_test,  y_test  = cargar_xy(ruta_test)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al leer train/test ({archivo_train}): {e}\")\n",
    "        continue\n",
    "\n",
    "    n_samples, n_features = X_train.shape\n",
    "    es_grande = (n_samples >= 10000) or (nombre_dataset.lower() == \"shuttle\")\n",
    "    es_shuttle_aum = (nombre_dataset.lower() == \"shuttle\") and (tipo == \"aumentado\")\n",
    "\n",
    "    # CV / paralelismo\n",
    "    cv_base   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_grande = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    cv_shuttle= StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    if es_shuttle_aum:\n",
    "        cv_actual = cv_shuttle\n",
    "    else:\n",
    "        cv_actual = (cv_grande if es_grande else cv_base)\n",
    "\n",
    "    # Materializar splits UNA sola vez por par\n",
    "    splits = make_cv_splits(cv_actual, X_train, y_train)\n",
    "\n",
    "    cpu = os.cpu_count() or 4\n",
    "    n_jobs_default = 1 if es_grande else max(1, min(4, cpu // 2))\n",
    "\n",
    "    # Presupuestos Halving (por par)\n",
    "    factor_halving = 3\n",
    "\n",
    "    # LR/SVM: resource='n_samples'\n",
    "    min_res_samples = max(30, int(0.10 * n_samples))\n",
    "    # no dejes que el m√≠nimo sea tan alto que impida ‚â•2 iteraciones\n",
    "    min_res_samples = min(min_res_samples, max(20, n_samples // (factor_halving**2)))\n",
    "\n",
    "    # RF: √°rboles min y max para ~3 iteraciones (min * factor^(3-1) = min * 9)\n",
    "    min_trees = 30 if es_grande else 50\n",
    "    target_iters = 3\n",
    "    max_trees = int(min_trees * (factor_halving ** (target_iters - 1)))  # 270 o 450 aprox\n",
    "\n",
    "    # -------------------------\n",
    "    # Por modelo (ordenado)\n",
    "    # -------------------------\n",
    "    for nombre_modelo in orden_modelos:\n",
    "        info = modelos[nombre_modelo]\n",
    "        key = (nombre_dataset, tipo, tecnica, str(densidad), str(riesgo), str(pureza))\n",
    "        if key in ya_hechos[nombre_modelo]:\n",
    "            print(f\"‚è≠Ô∏è  {nombre_modelo}: ya existe {key}; se omite.\")\n",
    "            continue\n",
    "\n",
    "        # REGLA ESPECIAL: Shuttle aumentado ‚Üí NO SVM\n",
    "        if es_shuttle_aum and nombre_modelo == \"SVM\":\n",
    "            print(\"‚è≠Ô∏è  SVM omitido para Shuttle aumentado (regla de rendimiento).\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚öôÔ∏è Validando modelo con HalvingRandomSearchCV: {nombre_modelo}\")\n",
    "\n",
    "        # Config local + espacio adaptado\n",
    "        if nombre_modelo == \"SVM\":\n",
    "            pipeline_local = Pipeline([\n",
    "                ('classifier', SVC(random_state=42, probability=False, max_iter=5000, cache_size=400))\n",
    "            ])\n",
    "            params_local = {\n",
    "                'classifier__kernel': ['linear'] if es_grande else ['linear', 'rbf'],\n",
    "                'classifier__C': (uniform(0.1, 10.0) if es_grande else loguniform(1e-3, 1e2)),\n",
    "                'classifier__gamma': (['scale'] if es_grande else loguniform(1e-4, 1e0)),\n",
    "                'classifier__shrinking': [True, False],\n",
    "                'classifier__class_weight': [None, 'balanced']\n",
    "            }\n",
    "\n",
    "        elif nombre_modelo == \"LogisticRegression\":\n",
    "            pipeline_local = info['pipeline']\n",
    "            if es_shuttle_aum:\n",
    "                params_local = {\n",
    "                    'classifier__solver': ['saga'],\n",
    "                    'classifier__penalty': ['l2'],\n",
    "                    'classifier__C': loguniform(1e-3, 1e1),\n",
    "                    'classifier__fit_intercept': [True],\n",
    "                    'classifier__class_weight': ['balanced'],\n",
    "                    'classifier__tol': loguniform(1e-4, 1e-2),\n",
    "                    'classifier__dual': [False]\n",
    "                }\n",
    "            else:\n",
    "                params_local = info['param_distributions']\n",
    "\n",
    "        else:  # RandomForest\n",
    "            pipeline_local = info['pipeline']  # n_jobs=1 en el clasificador interno\n",
    "            if es_shuttle_aum:\n",
    "                params_local = {\n",
    "                    # SIN n_estimators (lo maneja Halving via resource='classifier__n_estimators')\n",
    "                    'classifier__criterion': ['gini'],\n",
    "                    'classifier__max_depth': [None, 10, 20],\n",
    "                    'classifier__min_samples_split': randint(2, 11),\n",
    "                    'classifier__min_samples_leaf': randint(1, 6),\n",
    "                    'classifier__max_features': ['sqrt'],\n",
    "                    'classifier__class_weight': ['balanced_subsample'],\n",
    "                    'classifier__ccp_alpha': uniform(0.0, 0.005),\n",
    "                    'classifier__max_samples': uniform(0.5, 0.45)  # 0.5‚Äì0.95\n",
    "                }\n",
    "            else:\n",
    "                base = info['param_distributions'].copy()\n",
    "                base.pop('classifier__n_estimators', None)  # clave: no buscar sobre este hiper\n",
    "                if es_grande:\n",
    "                    base['classifier__criterion'] = ['gini']\n",
    "                    base['classifier__max_samples'] = uniform(0.4, 0.3)  # 0.4‚Äì0.7\n",
    "                else:\n",
    "                    base['classifier__max_samples'] = uniform(0.6, 0.35)  # 0.6‚Äì0.95\n",
    "                params_local = base\n",
    "\n",
    "        n_jobs_actual = n_jobs_default\n",
    "\n",
    "        # Construcci√≥n de la b√∫squeda (Halving)\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            if nombre_modelo == \"RandomForest\":\n",
    "                search = make_halving_rf(\n",
    "                    estimator=pipeline_local,\n",
    "                    params=params_local,          # sin n_estimators\n",
    "                    cv=splits,\n",
    "                    n_jobs=n_jobs_actual,\n",
    "                    verbose=1,\n",
    "                    factor=factor_halving,\n",
    "                    min_trees=min_trees,\n",
    "                    max_trees=max_trees          # <- fija max_resources expl√≠citamente\n",
    "                )\n",
    "            else:\n",
    "                search = make_halving(\n",
    "                    estimator=pipeline_local,\n",
    "                    params=params_local,\n",
    "                    cv=splits,\n",
    "                    n_jobs=n_jobs_actual,\n",
    "                    verbose=1,\n",
    "                    factor=factor_halving,\n",
    "                    min_resources=min_res_samples\n",
    "                )\n",
    "\n",
    "            search.fit(X_train, y_train)\n",
    "            elapsed = round(time.perf_counter() - t0, 3)\n",
    "\n",
    "            # CV (mejor √≠ndice)\n",
    "            bi = search.best_index_\n",
    "            cvres = search.cv_results_\n",
    "            cv_f1 = float(cvres.get('mean_test_f1_macro', cvres['mean_test_score'])[bi])\n",
    "            # Otras m√©tricas de CV no se calculan en Halving con single scoring\n",
    "            cv_bacc  = np.nan\n",
    "            cv_mcc   = np.nan\n",
    "            cv_kappa = np.nan\n",
    "\n",
    "            # Test\n",
    "            best_est = search.best_estimator_\n",
    "            y_pred   = best_est.predict(X_test)\n",
    "            test_f1   = float(f1_score(y_test, y_pred, average='macro'))\n",
    "            test_bacc = float(balanced_accuracy_score(y_test, y_pred))\n",
    "            test_mcc  = float(matthews_corrcoef(y_test, y_pred))\n",
    "            test_kappa= float(cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "            # Auditor√≠a de Halving\n",
    "            rondas = getattr(search, 'n_resources_', None)\n",
    "            cands  = getattr(search, 'n_candidates_', None)\n",
    "            rondas_str = str(list(rondas)) if rondas is not None else \"\"\n",
    "            cands_str  = str(list(cands))  if cands  is not None else \"\"\n",
    "\n",
    "            # Registrar\n",
    "            resultados_por_modelo[nombre_modelo].append({\n",
    "                'dataset': nombre_dataset, 'tipo': tipo, 'tecnica': tecnica,\n",
    "                'densidad': densidad, 'riesgo': riesgo, 'pureza': pureza,\n",
    "                'n_train': int(n_samples), 'n_test': int(X_test.shape[0]),\n",
    "                'n_features': int(n_features), 'es_grande': bool(es_grande),\n",
    "                'cv_splits': len(splits),\n",
    "                'modelo': nombre_modelo, 'mejor_configuracion': str(search.best_params_),\n",
    "                'cv_f1_macro': cv_f1, 'cv_balanced_accuracy': cv_bacc,\n",
    "                'cv_mcc': cv_mcc, 'cv_cohen_kappa': cv_kappa,\n",
    "                'test_f1_macro': test_f1, 'test_balanced_accuracy': test_bacc,\n",
    "                'test_mcc': test_mcc, 'test_cohen_kappa': test_kappa,\n",
    "                'search_time_sec': elapsed, 'n_jobs_search': n_jobs_actual,\n",
    "                'halving_n_resources': rondas_str,\n",
    "                'halving_n_candidates': cands_str\n",
    "            })\n",
    "\n",
    "            # Volcado incremental y marca como hecho\n",
    "            out_path = os.path.join(dir_out, f\"resultados_{nombre_modelo}.csv\")\n",
    "            pd.DataFrame(resultados_por_modelo[nombre_modelo]).to_csv(out_path, index=False)\n",
    "            ya_hechos[nombre_modelo].add(key)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al validar {nombre_modelo} en {archivo_train}: {e}\")\n",
    "\n",
    "        # Limpieza de memoria\n",
    "        gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# Persistencia final por modelo\n",
    "# -----------------------------\n",
    "for nombre_modelo, lista in resultados_por_modelo.items():\n",
    "    out_path = os.path.join(dir_out, f\"resultados_{nombre_modelo}.csv\")\n",
    "    pd.DataFrame(lista).to_csv(out_path, index=False)\n",
    "    print(f\"üìÅ Resultados guardados: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e43a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Enumerando combinaciones base y aumentadas...\n",
      "üìÇ Explorando carpeta base: ../datasets/datasets_aumentados/base/\n",
      "  ‚ö™ Omitido (no es *_train.csv): glass_test.csv\n",
      "  ‚úÖ Base encontrada: glass\n",
      "  ‚ö™ Omitido (no es *_train.csv): heart_test.csv\n",
      "  ‚úÖ Base encontrada: heart\n",
      "  ‚ö™ Omitido (no es *_train.csv): shuttle_test.csv\n",
      "  ‚úÖ Base encontrada: shuttle\n",
      "  ‚ö™ Omitido (no es *_train.csv): wdbc_test.csv\n",
      "  ‚úÖ Base encontrada: wdbc\n",
      "üìÇ Explorando carpeta aumentados: ../datasets/datasets_aumentados/\n",
      "  ‚ö™ Omitido (no es *_train.csv): base\n",
      "  ‚ö™ Omitido (no es *_train.csv): logs\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R25_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R25_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R25_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R25_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R50_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R50_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R50_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R50_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R75_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R75_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D25_R75_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D25_R75_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R25_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R25_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R25_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R25_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R50_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R50_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R50_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R50_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R75_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R75_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D50_R75_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D50_R75_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R25_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R25_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R25_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R25_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R50_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R50_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R50_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R50_entropia_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R75_Pproporcion_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R75_Pproporcion_train.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): pcsmote_glass_D75_R75_entropia_test.csv\n",
      "  ‚úÖ Aumentado encontrado: pcsmote_glass_D75_R75_entropia_train.csv\n",
      "üìä Total combinaciones descubiertas: 22\n",
      "üßÆ Tareas planificadas: 63 (excluidos por dataset: 3, por pol√≠tica SVM-Shuttle‚Üë: 0)\n",
      "üì¶ Total de tareas planificadas: 63\n",
      "\n",
      "================================================================================\n",
      "üèÅ [1/63] Dataset: glass | Tipo: base | Modelo: LogisticRegression\n",
      "üìÇ Train: glass_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 39.85 s\n",
      "üìä F1(CV): 0.4923 | F1(Test): 0.4057\n",
      "\n",
      "================================================================================\n",
      "üèÅ [2/63] Dataset: heart | Tipo: base | Modelo: LogisticRegression\n",
      "üìÇ Train: heart_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 53.53 s\n",
      "üìä F1(CV): 0.3410 | F1(Test): 0.3059\n",
      "\n",
      "================================================================================\n",
      "üèÅ [3/63] Dataset: wdbc | Tipo: base | Modelo: LogisticRegression\n",
      "üìÇ Train: wdbc_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 88.19 s\n",
      "üìä F1(CV): 0.9193 | F1(Test): 0.9017\n",
      "\n",
      "================================================================================\n",
      "üèÅ [4/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 28.21 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [5/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 33.58 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [6/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 27.23 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [7/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 34.60 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [8/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 29.01 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [9/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D25_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 35.26 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [10/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 28.14 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [11/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 33.67 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [12/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 27.67 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [13/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 34.28 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [14/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 29.45 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [15/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D50_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 33.82 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [16/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 28.34 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [17/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 35.92 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [18/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 28.51 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [19/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 35.24 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [20/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 28.18 s\n",
      "üìä F1(CV): 0.7586 | F1(Test): 0.6765\n",
      "\n",
      "================================================================================\n",
      "üèÅ [21/63] Dataset: glass | Tipo: aumentado | Modelo: LogisticRegression\n",
      "üìÇ Train: pcsmote_glass_D75_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 34.48 s\n",
      "üìä F1(CV): 0.7397 | F1(Test): 0.5646\n",
      "\n",
      "================================================================================\n",
      "üèÅ [22/63] Dataset: glass | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: glass_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 114.37 s\n",
      "üìä F1(CV): 0.6660 | F1(Test): 0.7962\n",
      "\n",
      "================================================================================\n",
      "üèÅ [23/63] Dataset: heart | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: heart_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 114.64 s\n",
      "üìä F1(CV): 0.4005 | F1(Test): 0.2403\n",
      "\n",
      "================================================================================\n",
      "üèÅ [24/63] Dataset: wdbc | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: wdbc_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 135.88 s\n",
      "üìä F1(CV): 0.9403 | F1(Test): 0.9618\n",
      "\n",
      "================================================================================\n",
      "üèÅ [25/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 119.51 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [26/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 128.37 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [27/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 120.04 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [28/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 125.73 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [29/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 115.03 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [30/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D25_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 121.41 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [31/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 116.95 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [32/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 121.62 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [33/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 116.50 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [34/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 120.66 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [35/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 115.63 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [36/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D50_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 122.05 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [37/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 114.73 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [38/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 123.86 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [39/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 114.73 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [40/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 120.94 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [41/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 116.75 s\n",
      "üìä F1(CV): 0.8135 | F1(Test): 0.7043\n",
      "\n",
      "================================================================================\n",
      "üèÅ [42/63] Dataset: glass | Tipo: aumentado | Modelo: RandomForest\n",
      "üìÇ Train: pcsmote_glass_D75_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 120.69 s\n",
      "üìä F1(CV): 0.8459 | F1(Test): 0.7460\n",
      "\n",
      "================================================================================\n",
      "üèÅ [43/63] Dataset: glass | Tipo: base | Modelo: SVM\n",
      "üìÇ Train: glass_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 1.78 s\n",
      "üìä F1(CV): 0.5637 | F1(Test): 0.6317\n",
      "\n",
      "================================================================================\n",
      "üèÅ [44/63] Dataset: heart | Tipo: base | Modelo: SVM\n",
      "üìÇ Train: heart_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 3.61 s\n",
      "üìä F1(CV): 0.3517 | F1(Test): 0.3309\n",
      "\n",
      "================================================================================\n",
      "üèÅ [45/63] Dataset: wdbc | Tipo: base | Modelo: SVM\n",
      "üìÇ Train: wdbc_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 3.58 s\n",
      "üìä F1(CV): 0.9361 | F1(Test): 0.9620\n",
      "\n",
      "================================================================================\n",
      "üèÅ [46/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.12 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [47/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.65 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [48/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.16 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [49/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.30 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [50/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.55 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [51/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D25_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.24 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [52/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.36 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [53/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.22 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [54/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.05 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [55/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.60 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [56/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.03 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [57/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D50_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.53 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [58/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R25_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.00 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [59/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R25_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.69 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [60/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R50_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 1.99 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [61/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R50_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.28 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "================================================================================\n",
      "üèÅ [62/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R75_Pproporcion_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "‚úÖ B√∫squeda completada en 2.77 s\n",
      "üìä F1(CV): 0.7400 | F1(Test): 0.6321\n",
      "\n",
      "================================================================================\n",
      "üèÅ [63/63] Dataset: glass | Tipo: aumentado | Modelo: SVM\n",
      "üìÇ Train: pcsmote_glass_D75_R75_entropia_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=40, folds=5, n_jobs=2\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ B√∫squeda completada en 2.29 s\n",
      "üìä F1(CV): 0.7449 | F1(Test): 0.6960\n",
      "\n",
      "üìä Compilando resultados globales...\n",
      "\n",
      "üèÅ Ejecuci√≥n total completada en 3324.71 s\n",
      "üìò Archivo Excel generado: ../resultados\\resultados_RS_cv_vs_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os, gc, time\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    f1_score, balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "# =========================\n",
    "# Configuraci√≥n general\n",
    "# =========================\n",
    "RUTA_DATASETS_BASE = \"../datasets/datasets_aumentados/base/\"\n",
    "RUTA_DATASETS_AUMENTADOS = \"../datasets/datasets_aumentados/\"\n",
    "DIRECTORIO_SALIDA = \"../resultados\"\n",
    "os.makedirs(DIRECTORIO_SALIDA, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SCORING_REFIT = \"f1_macro\"\n",
    "SCORING_MULTIPLE = {\n",
    "    \"f1_macro\": \"f1_macro\",\n",
    "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "    \"mcc\": make_scorer(matthews_corrcoef),\n",
    "    \"cohen_kappa\": make_scorer(cohen_kappa_score),\n",
    "}\n",
    "N_ITER_BUSQUEDA_POR_DEFECTO = 40\n",
    "OMITIR_SVM_EN_SHUTTLE_AUMENTADO = True  # pol√≠tica opcional\n",
    "NOMBRE_ARCHIVO_EXCEL = os.path.join(DIRECTORIO_SALIDA, \"resultados_RS_cv_vs_test.xlsx\")\n",
    "\n",
    "# =========================\n",
    "# Estructuras de datos\n",
    "# =========================\n",
    "@dataclass\n",
    "class DatasetCombination:\n",
    "    \"\"\"Representa una combinaci√≥n base o aumentada (metadatos y paths).\"\"\"\n",
    "    dataset_logico: str          # p. ej. \"Shuttle\"\n",
    "    tipo_combination: str        # \"base\" | \"aumentado\"\n",
    "    ruta_train_csv: str\n",
    "    ruta_test_csv: str\n",
    "    tecnica_aumento: str = \"base\"\n",
    "    valor_densidad: str = \"--\"\n",
    "    valor_riesgo: str = \"--\"\n",
    "    criterio_pureza: str = \"--\"\n",
    "\n",
    "@dataclass\n",
    "class RegistroRendimiento:\n",
    "    \"\"\"Fila de resultados con m√©tricas CV vs Test y auditor√≠a de b√∫squeda.\"\"\"\n",
    "    dataset_logico: str\n",
    "    tipo_combination: str\n",
    "    nombre_modelo_aprendizaje: str\n",
    "    tecnica_aumento: str\n",
    "    valor_densidad: str\n",
    "    valor_riesgo: str\n",
    "    criterio_pureza: str\n",
    "    cantidad_train: int\n",
    "    cantidad_test: int\n",
    "    cantidad_caracteristicas: int\n",
    "    # CV (sobre el mejor candidato de RS)\n",
    "    cv_f1_macro: float\n",
    "    cv_balanced_accuracy: float\n",
    "    cv_mcc: float\n",
    "    cv_cohen_kappa: float\n",
    "    # Test\n",
    "    test_f1_macro: float\n",
    "    test_balanced_accuracy: float\n",
    "    test_mcc: float\n",
    "    test_cohen_kappa: float\n",
    "    # Auditor√≠a\n",
    "    mejores_hiperparametros: str\n",
    "    tiempo_busqueda_seg: float\n",
    "\n",
    "# =========================\n",
    "# Utilidades de datos\n",
    "# =========================\n",
    "def cargar_matriz_caracteristicas_y_etiquetas_desde_csv(ruta_csv):\n",
    "    \"\"\"Lee un CSV y devuelve (X, y). Si existe 'target', la usa; si no, usa la √∫ltima columna como y.\"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    if \"target\" in df.columns:\n",
    "        X = df.drop(columns=[\"target\"]).to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df[\"target\"].to_numpy()\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df.iloc[:, -1].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "def enumerar_combinaciones_base_y_aumentadas(ruta_base, ruta_aumentados, verbose=True):\n",
    "    \"\"\"\n",
    "    Descubre *_train.csv y su *_test.csv correspondiente.\n",
    "    Base: usa nombre l√≥gico tal cual.\n",
    "    Aumentados: espera tecnica_dataset_Dx_Ry_Pz_train.csv para extraer metadatos.\n",
    "    Si verbose=True, imprime diagn√≥sticos de archivos encontrados/omitidos.\n",
    "    \"\"\"\n",
    "    combinaciones = []\n",
    "\n",
    "    # ===================== BASES =====================\n",
    "    if os.path.isdir(ruta_base):\n",
    "        if verbose:\n",
    "            print(f\"üìÇ Explorando carpeta base: {ruta_base}\")\n",
    "        for nombre in sorted(os.listdir(ruta_base)):\n",
    "            if not nombre.endswith(\"_train.csv\"):\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ö™ Omitido (no es *_train.csv): {nombre}\")\n",
    "                continue\n",
    "\n",
    "            ruta_train = os.path.join(ruta_base, nombre)\n",
    "            ruta_test = os.path.join(ruta_base, nombre.replace(\"_train.csv\", \"_test.csv\"))\n",
    "\n",
    "            if not os.path.isfile(ruta_test):\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ö†Ô∏è  Falta test para {nombre}, se omite.\")\n",
    "                continue\n",
    "\n",
    "            dataset_logico = nombre.replace(\"_train.csv\", \"\")\n",
    "            combinaciones.append(DatasetCombination(\n",
    "                dataset_logico=dataset_logico,\n",
    "                tipo_combination=\"base\",\n",
    "                ruta_train_csv=ruta_train,\n",
    "                ruta_test_csv=ruta_test\n",
    "            ))\n",
    "            if verbose:\n",
    "                print(f\"  ‚úÖ Base encontrada: {dataset_logico}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå No existe carpeta base: {ruta_base}\")\n",
    "\n",
    "    # ===================== AUMENTADOS =====================\n",
    "    if os.path.isdir(ruta_aumentados):\n",
    "        if verbose:\n",
    "            print(f\"üìÇ Explorando carpeta aumentados: {ruta_aumentados}\")\n",
    "        for nombre in sorted(os.listdir(ruta_aumentados)):\n",
    "            if not nombre.endswith(\"_train.csv\"):\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ö™ Omitido (no es *_train.csv): {nombre}\")\n",
    "                continue\n",
    "\n",
    "            ruta_train = os.path.join(ruta_aumentados, nombre)\n",
    "            ruta_test = os.path.join(ruta_aumentados, nombre.replace(\"_train.csv\", \"_test.csv\"))\n",
    "\n",
    "            if not os.path.isfile(ruta_test):\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ö†Ô∏è  Falta test para {nombre}, se omite.\")\n",
    "                continue\n",
    "\n",
    "            partes = nombre.replace(\"_train.csv\", \"\").split(\"_\")\n",
    "            if len(partes) >= 5:\n",
    "                tecnica, dataset_logico, dens, ries, pur = partes[:5]\n",
    "                combinaciones.append(DatasetCombination(\n",
    "                    dataset_logico=dataset_logico,\n",
    "                    tipo_combination=\"aumentado\",\n",
    "                    ruta_train_csv=ruta_train,\n",
    "                    ruta_test_csv=ruta_test,\n",
    "                    tecnica_aumento=tecnica,\n",
    "                    valor_densidad=dens[1:],   # quita 'D'\n",
    "                    valor_riesgo=ries[1:],     # quita 'R'\n",
    "                    criterio_pureza=pur[1:]    # quita 'P'\n",
    "                ))\n",
    "                if verbose:\n",
    "                    print(f\"  ‚úÖ Aumentado encontrado: {nombre}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"  ‚ö†Ô∏è  No cumple patr√≥n esperado (tecnica_dataset_Dx_Ry_Pz): {nombre}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå No existe carpeta aumentados: {ruta_aumentados}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üìä Total combinaciones descubiertas: {len(combinaciones)}\")\n",
    "    return combinaciones\n",
    "\n",
    "def definir_configuracion_busqueda_para_dataset(X_train, nombre_dataset_logico, tipo_combination):\n",
    "    \"\"\"\n",
    "    Define la configuraci√≥n de b√∫squeda: esquema de CV (n_splits), n_iter (RandomizedSearchCV) y n_jobs.\n",
    "    Mantiene: Shuttle aumentado ‚Üí 2 folds; Shuttle o n‚â•10k ‚Üí 3 folds; resto ‚Üí 5 folds.\n",
    "    \"\"\"\n",
    "    n_muestras = X_train.shape[0]\n",
    "    es_shuttle = (nombre_dataset_logico.lower() == \"shuttle\")\n",
    "    if es_shuttle and tipo_combination == \"aumentado\":\n",
    "        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "    elif es_shuttle or n_muestras >= 10000:\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    cpu = os.cpu_count() or 4\n",
    "    n_jobs = 1 if (es_shuttle or n_muestras >= 10000) else max(1, min(4, cpu // 2))\n",
    "    n_iter = 25 if (es_shuttle or n_muestras >= 10000) else N_ITER_BUSQUEDA_POR_DEFECTO\n",
    "    return dict(cv=cv, n_jobs=n_jobs, n_iter=n_iter)\n",
    "\n",
    "# =========================\n",
    "# Registro de modelos\n",
    "# =========================\n",
    "def construir_estimador_y_espacio_svm():\n",
    "    \"\"\"Devuelve (pipeline SVM, espacio de hiperpar√°metros) con kernels linear/rbf.\"\"\"\n",
    "    est = Pipeline([('classifier', SVC(random_state=RANDOM_STATE, probability=False, max_iter=5000, cache_size=400))])\n",
    "    space = {\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__C': loguniform(1e-3, 1e2),\n",
    "        'classifier__gamma': loguniform(1e-4, 1e0),  # ignorado cuando kernel='linear'\n",
    "        'classifier__shrinking': [True, False],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "def construir_estimador_y_espacio_regresion_logistica_saga():\n",
    "    \"\"\"Devuelve (pipeline LR con solver saga) evitando combinaciones inv√°lidas; penalizaci√≥n L1/L2.\"\"\"\n",
    "    est = Pipeline([('classifier', LogisticRegression(max_iter=5000, random_state=RANDOM_STATE, solver='saga'))])\n",
    "    space = {\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__C': loguniform(1e-4, 1e2),\n",
    "        'classifier__fit_intercept': [True, False],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "        'classifier__tol': loguniform(1e-5, 1e-3),\n",
    "        'classifier__l1_ratio': [None],  # evitamos elasticnet para simplificar\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "def construir_estimador_y_espacio_random_forest():\n",
    "    \"\"\"Devuelve (pipeline RF) con espacio adecuado para RS; el RF interno usa n_jobs=1 para no sobreparalelizar.\"\"\"\n",
    "    est = Pipeline([('classifier', RandomForestClassifier(\n",
    "        random_state=RANDOM_STATE, n_jobs=1, bootstrap=True, oob_score=False\n",
    "    ))])\n",
    "    space = {\n",
    "        'classifier__n_estimators': randint(150, 500),\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__max_depth': [None, 10, 20, 40],\n",
    "        'classifier__min_samples_split': randint(2, 21),\n",
    "        'classifier__min_samples_leaf': randint(1, 11),\n",
    "        'classifier__max_features': ['sqrt', 'log2'],\n",
    "        'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "        'classifier__ccp_alpha': uniform(0.0, 0.01),\n",
    "        'classifier__max_samples': uniform(0.6, 0.35)  # 0.6‚Äì0.95 acelera en grandes\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "REGISTRO_MODELOS = {\n",
    "    \"LogisticRegression\": construir_estimador_y_espacio_regresion_logistica_saga,\n",
    "    \"RandomForest\": construir_estimador_y_espacio_random_forest,\n",
    "    \"SVM\": construir_estimador_y_espacio_svm\n",
    "}\n",
    "ORDEN_MODELOS = [\"LogisticRegression\", \"RandomForest\", \"SVM\"]  # r√°pido ‚Üí lento\n",
    "\n",
    "# =========================\n",
    "# Entrenamiento y evaluaci√≥n\n",
    "# =========================\n",
    "def ejecutar_rs_y_comparar_cv_con_test(estimator, space, X_train, y_train, X_test, y_test, configuracion_busqueda, verbose=0):\n",
    "    \"\"\"\n",
    "    Ejecuta RandomizedSearchCV con scoring m√∫ltiple (refit=f1_macro).\n",
    "    Devuelve m√©tricas promedio de CV del mejor candidato y m√©tricas en Test, m√°s auditor√≠a.\n",
    "    \"\"\"\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=space,\n",
    "        n_iter=configuracion_busqueda[\"n_iter\"],\n",
    "        scoring=SCORING_MULTIPLE,\n",
    "        refit=SCORING_REFIT,\n",
    "        cv=configuracion_busqueda[\"cv\"],\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=configuracion_busqueda[\"n_jobs\"],\n",
    "        verbose=verbose,\n",
    "        error_score=np.nan,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    search.fit(X_train, y_train)\n",
    "    elapsed = round(time.perf_counter() - t0, 3)\n",
    "\n",
    "    # M√©tricas de CV del mejor √≠ndice\n",
    "    best_idx = search.best_index_\n",
    "    cv_results = search.cv_results_\n",
    "    cv_f1 = float(cv_results['mean_test_f1_macro'][best_idx])\n",
    "    cv_bacc = float(cv_results['mean_test_balanced_accuracy'][best_idx])\n",
    "    cv_mcc = float(cv_results['mean_test_mcc'][best_idx])\n",
    "    cv_kappa = float(cv_results['mean_test_cohen_kappa'][best_idx])\n",
    "\n",
    "    # M√©tricas Test\n",
    "    best_estimator = search.best_estimator_\n",
    "    y_pred_test = best_estimator.predict(X_test)\n",
    "    test_f1 = float(f1_score(y_test, y_pred_test, average='macro'))\n",
    "    test_bacc = float(balanced_accuracy_score(y_test, y_pred_test))\n",
    "    test_mcc = float(matthews_corrcoef(y_test, y_pred_test))\n",
    "    test_kappa = float(cohen_kappa_score(y_test, y_pred_test))\n",
    "\n",
    "    return dict(\n",
    "        mejores_params=search.best_params_,\n",
    "        tiempo=elapsed,\n",
    "        cv=dict(f1=cv_f1, bacc=cv_bacc, mcc=cv_mcc, kappa=cv_kappa),\n",
    "        test=dict(f1=test_f1, bacc=test_bacc, mcc=test_mcc, kappa=test_kappa)\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Orquestaci√≥n (lista plana de tareas)\n",
    "# --- Pol√≠ticas globales de exclusi√≥n (en min√∫sculas) ---\n",
    "EXCLUIR_DATASETS = {\"shuttle\"}  # agregar aqu√≠ otros: {\"shuttle\", \"ecoli\", ...}\n",
    "# =========================\n",
    "\n",
    "def construir_lista_plana_de_tareas(model_registry, dataset_combinations, orden_modelos,\n",
    "                                    excluir_datasets=EXCLUIR_DATASETS, verbose=True):\n",
    "    \"\"\"\n",
    "    Crea una lista plana de tareas (modelo, combinaci√≥n) y aplica pol√≠ticas de exclusi√≥n.\n",
    "    - excluir_datasets: conjunto de nombres de dataset (en min√∫sculas) a excluir por completo.\n",
    "    - Mantiene la pol√≠tica existente de omitir SVM en Shuttle aumentado si est√° activa.\n",
    "    \"\"\"\n",
    "    tareas = []\n",
    "    excluidos_por_dataset = 0\n",
    "    excluidos_por_politica_svm_shuttle = 0\n",
    "\n",
    "    for nombre_modelo in orden_modelos:\n",
    "        for combo in dataset_combinations:\n",
    "            ds = combo.dataset_logico.lower()\n",
    "\n",
    "            # 1) Excluir datasets completos (p. ej., shuttle)\n",
    "            if ds in (excluir_datasets or set()):\n",
    "                excluidos_por_dataset += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Pol√≠tica original: omitir SVM en Shuttle aumentado\n",
    "            if (OMITIR_SVM_EN_SHUTTLE_AUMENTADO and\n",
    "                nombre_modelo == \"SVM\" and\n",
    "                ds == \"shuttle\" and\n",
    "                combo.tipo_combination == \"aumentado\"):\n",
    "                excluidos_por_politica_svm_shuttle += 1\n",
    "                continue\n",
    "\n",
    "            tareas.append((nombre_modelo, combo))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üßÆ Tareas planificadas: {len(tareas)} \"\n",
    "              f\"(excluidos por dataset: {excluidos_por_dataset}, \"\n",
    "              f\"por pol√≠tica SVM-Shuttle‚Üë: {excluidos_por_politica_svm_shuttle})\")\n",
    "    return tareas\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Exportaci√≥n a Excel\n",
    "# =========================\n",
    "def generar_archivo_excel_resultados(ruta_excel, df_resultados, df_resumen_base_vs_aumentado, df_rankings):\n",
    "    \"\"\"\n",
    "    Escribe un archivo Excel con tres hojas:\n",
    "      - 'resultados': filas con CV vs Test por (dataset, tipo, modelo, combinaci√≥n).\n",
    "      - 'resumen_base_vs_aumentado': promedios por dataset y modelo + delta (aum - base) en F1-Test y F1-CV.\n",
    "      - 'rankings': ranking por dataset y modelo, ordenado por F1-Test.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(ruta_excel, engine=\"xlsxwriter\") as writer:\n",
    "        df_resultados.to_excel(writer, sheet_name=\"resultados\", index=False)\n",
    "        df_resumen_base_vs_aumentado.to_excel(writer, sheet_name=\"resumen_base_vs_aumentado\", index=True)\n",
    "        df_rankings.to_excel(writer, sheet_name=\"rankings\", index=False)\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "\n",
    "def ejecutar_experimentos_y_generar_excel():\n",
    "    \"\"\"\n",
    "    Ejecuta todos los experimentos sobre datasets base y aumentados,\n",
    "    con b√∫squeda aleatoria de hiperpar√°metros y m√©tricas CV vs Test.\n",
    "    Muestra progreso detallado por consola y exporta resultados a Excel.\n",
    "    \"\"\"\n",
    "    print(\"üîé Enumerando combinaciones base y aumentadas...\")\n",
    "    combinaciones = enumerar_combinaciones_base_y_aumentadas(\n",
    "        RUTA_DATASETS_BASE, RUTA_DATASETS_AUMENTADOS\n",
    "    )\n",
    "\n",
    "    if not combinaciones:\n",
    "        print(\"‚ùå No se encontraron combinaciones de datasets.\")\n",
    "        return\n",
    "\n",
    "    datasets_con_base = {c.dataset_logico for c in combinaciones if c.tipo_combination == \"base\"}\n",
    "    if not datasets_con_base:\n",
    "        print(\"‚ùå No hay datasets base para comparar.\")\n",
    "        return\n",
    "\n",
    "    tareas = construir_lista_plana_de_tareas(\n",
    "        REGISTRO_MODELOS,\n",
    "        combinaciones,\n",
    "        ORDEN_MODELOS,\n",
    "        excluir_datasets={\"shuttle\"},   # datasets a excluir por completo\n",
    "        verbose=True\n",
    "    )\n",
    "    total_tareas = len(tareas)\n",
    "    print(f\"üì¶ Total de tareas planificadas: {total_tareas}\")\n",
    "\n",
    "    registros = []\n",
    "    inicio_total = time.perf_counter()\n",
    "\n",
    "    for idx, (nombre_modelo, combo) in enumerate(tareas, start=1):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üèÅ [{idx}/{total_tareas}] Dataset: {combo.dataset_logico} | \"\n",
    "              f\"Tipo: {combo.tipo_combination} | Modelo: {nombre_modelo}\")\n",
    "        print(f\"üìÇ Train: {os.path.basename(combo.ruta_train_csv)}\")\n",
    "\n",
    "        try:\n",
    "            X_train, y_train = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_train_csv)\n",
    "            X_test, y_test = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_test_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error leyendo archivos CSV: {e}\")\n",
    "            continue\n",
    "\n",
    "        configuracion_busqueda = definir_configuracion_busqueda_para_dataset(\n",
    "            X_train, combo.dataset_logico, combo.tipo_combination\n",
    "        )\n",
    "        print(f\"‚öôÔ∏è  Configuraci√≥n de b√∫squeda: \"\n",
    "              f\"n_iter={configuracion_busqueda['n_iter']}, \"\n",
    "              f\"folds={configuracion_busqueda['cv'].n_splits}, \"\n",
    "              f\"n_jobs={configuracion_busqueda['n_jobs']}\")\n",
    "\n",
    "        estimator, space = REGISTRO_MODELOS[nombre_modelo]()\n",
    "        print(f\"üöÄ Iniciando RandomizedSearchCV...\")\n",
    "\n",
    "        try:\n",
    "            resultados = ejecutar_rs_y_comparar_cv_con_test(\n",
    "                estimator, space, X_train, y_train, X_test, y_test,\n",
    "                configuracion_busqueda=configuracion_busqueda, verbose=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error durante la b√∫squeda: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚úÖ B√∫squeda completada en {resultados['tiempo']:.2f} s\")\n",
    "        print(f\"üìä F1(CV): {resultados['cv']['f1']:.4f} | \"\n",
    "              f\"F1(Test): {resultados['test']['f1']:.4f}\")\n",
    "\n",
    "        registros.append(asdict(RegistroRendimiento(\n",
    "            dataset_logico=combo.dataset_logico,\n",
    "            tipo_combination=combo.tipo_combination,\n",
    "            nombre_modelo_aprendizaje=nombre_modelo,\n",
    "            tecnica_aumento=combo.tecnica_aumento,\n",
    "            valor_densidad=combo.valor_densidad,\n",
    "            valor_riesgo=combo.valor_riesgo,\n",
    "            criterio_pureza=combo.criterio_pureza,\n",
    "            cantidad_train=int(X_train.shape[0]),\n",
    "            cantidad_test=int(X_test.shape[0]),\n",
    "            cantidad_caracteristicas=int(X_train.shape[1]),\n",
    "            cv_f1_macro=resultados[\"cv\"][\"f1\"],\n",
    "            cv_balanced_accuracy=resultados[\"cv\"][\"bacc\"],\n",
    "            cv_mcc=resultados[\"cv\"][\"mcc\"],\n",
    "            cv_cohen_kappa=resultados[\"cv\"][\"kappa\"],\n",
    "            test_f1_macro=resultados[\"test\"][\"f1\"],\n",
    "            test_balanced_accuracy=resultados[\"test\"][\"bacc\"],\n",
    "            test_mcc=resultados[\"test\"][\"mcc\"],\n",
    "            test_cohen_kappa=resultados[\"test\"][\"kappa\"],\n",
    "            mejores_hiperparametros=str(resultados[\"mejores_params\"]),\n",
    "            tiempo_busqueda_seg=float(resultados[\"tiempo\"]),\n",
    "        )))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    # ----------------- DataFrames finales -----------------\n",
    "    print(\"\\nüìä Compilando resultados globales...\")\n",
    "    df_resultados = pd.DataFrame(registros)\n",
    "\n",
    "    resumen = (df_resultados\n",
    "        .groupby([\"dataset_logico\", \"nombre_modelo_aprendizaje\", \"tipo_combination\"])\n",
    "        .agg(cv_f1_prom=(\"cv_f1_macro\", \"mean\"),\n",
    "             test_f1_prom=(\"test_f1_macro\", \"mean\"))\n",
    "        .unstack(\"tipo_combination\"))\n",
    "    for col in [(\"cv_f1_prom\", \"base\"), (\"cv_f1_prom\", \"aumentado\"), (\"test_f1_prom\", \"base\"), (\"test_f1_prom\", \"aumentado\")]:\n",
    "        if col not in resumen.columns:\n",
    "            resumen[col] = np.nan\n",
    "    resumen = resumen.assign(\n",
    "        delta_cv_f1 = resumen[(\"cv_f1_prom\", \"aumentado\")] - resumen[(\"cv_f1_prom\", \"base\")],\n",
    "        delta_test_f1 = resumen[(\"test_f1_prom\", \"aumentado\")] - resumen[(\"test_f1_prom\", \"base\")]\n",
    "    )\n",
    "\n",
    "    lista_rankings = []\n",
    "    for ds in sorted(df_resultados[\"dataset_logico\"].unique()):\n",
    "        sub_ds = df_resultados[df_resultados[\"dataset_logico\"] == ds]\n",
    "        for mdl in ORDEN_MODELOS:\n",
    "            sub_mdl = sub_ds[sub_ds[\"nombre_modelo_aprendizaje\"] == mdl].copy()\n",
    "            if sub_mdl.empty:\n",
    "                continue\n",
    "            sub_mdl.sort_values(\"test_f1_macro\", ascending=False, inplace=True)\n",
    "            sub_mdl[\"ranking\"] = range(1, len(sub_mdl) + 1)\n",
    "            lista_rankings.append(sub_mdl[[\n",
    "                \"dataset_logico\",\"nombre_modelo_aprendizaje\",\"ranking\",\n",
    "                \"valor_riesgo\",\"valor_densidad\",\"criterio_pureza\",\n",
    "                \"tipo_combination\",\"test_f1_macro\",\"cv_f1_macro\",\n",
    "                \"tecnica_aumento\",\"mejores_hiperparametros\"\n",
    "            ]])\n",
    "    df_rankings = pd.concat(lista_rankings, ignore_index=True) if lista_rankings else pd.DataFrame()\n",
    "\n",
    "    generar_archivo_excel_resultados(\n",
    "        ruta_excel=NOMBRE_ARCHIVO_EXCEL,\n",
    "        df_resultados=df_resultados,\n",
    "        df_resumen_base_vs_aumentado=resumen,\n",
    "        df_rankings=df_rankings\n",
    "    )\n",
    "\n",
    "    fin_total = time.perf_counter()\n",
    "    duracion = round(fin_total - inicio_total, 2)\n",
    "    print(f\"\\nüèÅ Ejecuci√≥n total completada en {duracion} s\")\n",
    "    print(f\"üìò Archivo Excel generado: {NOMBRE_ARCHIVO_EXCEL}\")\n",
    "\n",
    "\n",
    "ejecutar_experimentos_y_generar_excel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
