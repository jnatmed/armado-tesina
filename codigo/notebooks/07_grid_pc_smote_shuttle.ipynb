{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluación de PC-SMOTE con Grid Search en el dataset Shuttle (Generación de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de búsqueda de módulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar módulos o paquetes cuando hacés un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importación de módulos y librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Módulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Función para cargar datasets según configuración\n",
    "from config_datasets import config_datasets                    # Diccionario de configuración de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluación de sobremuestreo con partición hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la técnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementación principal de PCSMOTE\n",
    "\n",
    "# --- Librerías estándar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generación de combinaciones de parámetros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "\n",
    "# --- Librerías científicas ---\n",
    "import numpy as np                                              # Operaciones numéricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulación y análisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para búsqueda de hiperparámetros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificación de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creación de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: división y validación ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # División de datos en train/test\n",
    "    StratifiedKFold,                                            # Validación cruzada estratificada\n",
    "    RandomizedSearchCV                                          # Búsqueda aleatoria de hiperparámetros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducción de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # Análisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: métricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # Métrica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisión balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresión logística\n",
    "from sklearn.svm import SVC                                      # Máquinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generación del caso base\n",
    "\n",
    "Este código realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio específico para almacenar la versión original del dataset sin ningún tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma función `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) están en formato de texto u objeto, se convierten a valores numéricos con `LabelEncoder`.\n",
    "   - Se realiza una división estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporción 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- función: generar caso base (train/test sin sobremuestreo) ---\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin PCSMOTE) para un dataset: guarda train y test en ruta_base.\n",
    "    Usa la misma lógica de carga que el resto del pipeline (cargar_dataset + LabelEncoder opcional).\n",
    "\n",
    "    Retorna:\n",
    "        (path_train, path_test)\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test  # ya generado\n",
    "\n",
    "    # 1) Cargar dataset base con tu helper habitual\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar etiquetas numéricas si vienen como strings/objects\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado (mismo seed para reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Guardar CSVs\n",
    "    pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_train, index=False)\n",
    "    pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_test, index=False)\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(nombre_dataset, config, percentil_densidad, percentil_riesgo, criterio_pureza, test_size=0.2):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "\n",
    "    try:\n",
    "        # 1) Cargar dataset original\n",
    "        X, y, clases = cargar_dataset(\n",
    "            path=config[\"path\"],\n",
    "            clase_minoria=config.get(\"clase_minoria\"),\n",
    "            col_features=config.get(\"col_features\"),\n",
    "            col_target=config.get(\"col_target\"),\n",
    "            sep=config.get(\"sep\", \",\"),\n",
    "            header=config.get(\"header\", None),\n",
    "            binarizar=False,\n",
    "            tipo=config.get(\"tipo\", \"tabular\")\n",
    "        )\n",
    "\n",
    "        # 2) Codificar etiquetas si son strings\n",
    "        if y.dtype == object or isinstance(y[0], str):\n",
    "            y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # 3) Si es un dataset de imágenes, convertir a vector plano\n",
    "        if config.get(\"tipo\") == \"imagen\":\n",
    "            X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "        # 4) Dividir en train/test (antes de sobremuestrear)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # 5) Escalar ambos usando estadísticas del train\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train) # hacer antes\n",
    "        X_test = scaler.transform(X_test) # hacer antes\n",
    "\n",
    "        # 6) Aplicar PCSMOTE sobre el set de entrenamiento escalado\n",
    "        print(f\"🧬 Aplicando PCSMOTE | Densidad: {percentil_densidad} | Riesgo: {percentil_riesgo} | Pureza: {criterio_pureza}\")\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_dist=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            modo_espacial='3d'\n",
    "        )\n",
    "        sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_train_res, y_train_res = sampler.fit_resample_multiclass(X_train, y_train)\n",
    "        else:\n",
    "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 7) Guardar datasets: train aumentado y test escalado\n",
    "        print(f\"💾 Guardando datasets aumentados...\")\n",
    "\n",
    "        ruta_salida = f\"../datasets/datasets_aumentados/\"\n",
    "        os.makedirs(ruta_salida, exist_ok=True)\n",
    "\n",
    "        nombre_base = f\"pcsmote_{nombre_dataset}_D{percentil_densidad}_R{percentil_riesgo}_P{criterio_pureza}\"\n",
    "        path_train = os.path.join(ruta_salida, f\"{nombre_base}_train.csv\")\n",
    "        path_test = os.path.join(ruta_salida, f\"{nombre_base}_test.csv\")\n",
    "\n",
    "        # Guardar train aumentado\n",
    "        df_train = pd.DataFrame(X_train_res)\n",
    "        df_train[\"target\"] = y_train_res\n",
    "        df_train.to_csv(path_train, index=False)\n",
    "\n",
    "        # Guardar test escalado (sin sobremuestrear)\n",
    "        df_test = pd.DataFrame(X_test)\n",
    "        df_test[\"target\"] = y_test\n",
    "        df_test.to_csv(path_test, index=False)\n",
    "\n",
    "        print(f\"✅ Datasets guardados:\\n- Train aumentado: {path_train}\\n- Test escalado: {path_test}\")\n",
    "        return path_train, path_test, sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### 🧬 Aumento de Datasets mediante Técnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versión balanceada de cada dataset original mediante la aplicación de técnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la técnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generación de muestras sintéticas en función de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones específicas de parámetros según la técnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuración empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ⚠️ Esta fase no incluye entrenamiento ni validación de modelos. Su único propósito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partición `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluación imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Dataset: shuttle\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/shuttle_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/shuttle_test.csv\n",
      "\n",
      "📁 Dataset: wdbc\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/wdbc_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/wdbc_test.csv\n",
      "\n",
      "📁 Dataset: glass\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/glass_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/glass_test.csv\n",
      "\n",
      "📁 Dataset: heart\n",
      "⚠️ Advertencia: columnas no numéricas detectadas: [11, 12]\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/heart_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/heart_test.csv\n"
     ]
    }
   ],
   "source": [
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "combinaciones = list(product(percentiles_densidad, percentiles_riesgo, criterios_pureza))\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📁 Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # --- CASO BASE ---\n",
    "    base_train, base_test = generar_caso_base(nombre_dataset, config)\n",
    "    print(f\"🟦 Caso base generado:\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "    # --- GRID DE PCSMOTE ---\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        print(f\"#{idx:02d} ➕ Aumentando con D={pdens} | R={priesgo} | P={criterio}\")\n",
    "\n",
    "        path_train, path_test, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            config=config,\n",
    "            percentil_densidad=pdens,\n",
    "            percentil_riesgo=priesgo,\n",
    "            criterio_pureza=criterio\n",
    "        )\n",
    "\n",
    "        if path_train and sampler:\n",
    "            print(f\"✅ Guardado exitoso:\\n - Train: {path_train}\\n - Test: {path_test}\")\n",
    "            log_path = f\"../datasets/datasets_aumentados/logs/log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_P{criterio}.csv\"\n",
    "            sampler.exportar_log_csv(log_path)\n",
    "            print(f\"📄 Log exportado: {log_path}\")\n",
    "        else:\n",
    "            print(\"❌ Falló la generación.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc87fa1",
   "metadata": {},
   "source": [
    "### Evaluación de modelos con validación cruzada estratificada\n",
    "\n",
    "Para evaluar el rendimiento de los modelos de clasificación sobre los datasets previamente balanceados, se utilizó validación cruzada estratificada de 5 particiones (Stratified K-Fold con *k=5*). Este método garantiza que en cada fold de entrenamiento y validación se preserve la proporción original de clases, lo cual es especialmente importante en tareas de clasificación multiclase con datasets balanceados artificialmente.\n",
    "\n",
    "Durante el proceso, cada modelo es entrenado y evaluado cinco veces, cada vez usando un subconjunto distinto como conjunto de prueba y el resto como conjunto de entrenamiento. Las métricas calculadas en cada iteración (F1-score macro, balanced accuracy, MCC y kappa de Cohen) se promedian para obtener un valor representativo y del rendimiento general del modelo sobre ese dataset aumentado.\n",
    "\n",
    "Este enfoque evita sobreajuste y proporciona una evaluación más confiable que una simple división train/test, permitiendo comparar de forma justa distintas configuraciones de sobremuestreo y modelos de clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5320c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 (1/76) Par: pcsmote_glass_D25_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (2/76) Par: pcsmote_glass_D25_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (3/76) Par: pcsmote_glass_D25_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (4/76) Par: pcsmote_glass_D25_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (5/76) Par: pcsmote_glass_D25_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (6/76) Par: pcsmote_glass_D25_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (7/76) Par: pcsmote_glass_D50_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (8/76) Par: pcsmote_glass_D50_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (9/76) Par: pcsmote_glass_D50_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (10/76) Par: pcsmote_glass_D50_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (11/76) Par: pcsmote_glass_D50_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (12/76) Par: pcsmote_glass_D50_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (13/76) Par: pcsmote_glass_D75_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (14/76) Par: pcsmote_glass_D75_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (15/76) Par: pcsmote_glass_D75_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (16/76) Par: pcsmote_glass_D75_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (17/76) Par: pcsmote_glass_D75_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (18/76) Par: pcsmote_glass_D75_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (19/76) Par: pcsmote_heart_D25_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (20/76) Par: pcsmote_heart_D25_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (21/76) Par: pcsmote_heart_D25_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (22/76) Par: pcsmote_heart_D25_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (23/76) Par: pcsmote_heart_D25_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (24/76) Par: pcsmote_heart_D25_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (25/76) Par: pcsmote_heart_D50_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (26/76) Par: pcsmote_heart_D50_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (27/76) Par: pcsmote_heart_D50_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (28/76) Par: pcsmote_heart_D50_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (29/76) Par: pcsmote_heart_D50_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (30/76) Par: pcsmote_heart_D50_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (31/76) Par: pcsmote_heart_D75_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (32/76) Par: pcsmote_heart_D75_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (33/76) Par: pcsmote_heart_D75_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (34/76) Par: pcsmote_heart_D75_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (35/76) Par: pcsmote_heart_D75_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (36/76) Par: pcsmote_heart_D75_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (37/76) Par: pcsmote_shuttle_D25_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  20.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.2s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.4s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.5s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.6s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  20.6s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.1s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  19.0s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (38/76) Par: pcsmote_shuttle_D25_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.2s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.2s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  12.6s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  15.0s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (39/76) Par: pcsmote_shuttle_D25_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.9s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.8s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  19.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  17.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.5s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (40/76) Par: pcsmote_shuttle_D25_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   9.8s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   9.9s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.2s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.9s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (41/76) Par: pcsmote_shuttle_D25_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.9s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.0s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.9s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.0s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.4s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.7s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  13.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  20.5s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.6s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (42/76) Par: pcsmote_shuttle_D25_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.0s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.6s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.5s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.9s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  10.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  15.5s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  15.5s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (43/76) Par: pcsmote_shuttle_D50_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.4s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.7s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.2s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.5s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.5s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.9s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.5s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  20.5s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  19.0s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (44/76) Par: pcsmote_shuttle_D50_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.2s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.6s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  12.6s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.9s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (45/76) Par: pcsmote_shuttle_D50_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.7s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  10.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  10.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.8s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  11.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  19.2s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.5s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (46/76) Par: pcsmote_shuttle_D50_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   9.7s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   9.8s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   9.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   8.5s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.2s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  13.8s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (47/76) Par: pcsmote_shuttle_D50_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  13.9s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.0s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  14.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=  11.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  11.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.5s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  12.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.7s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.8s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  12.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  19.4s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  17.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  18.6s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (48/76) Par: pcsmote_shuttle_D50_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.3s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.4s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.5s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.5s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.1s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.7s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.4s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  15.0s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (49/76) Par: pcsmote_shuttle_D75_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.2s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.9s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.8s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.6s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  17.1s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.4s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.9s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (50/76) Par: pcsmote_shuttle_D75_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   7.0s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   7.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   6.6s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.2s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   6.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.1s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   5.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   6.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   5.5s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   5.8s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   6.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   5.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   9.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  10.1s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   9.8s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (51/76) Par: pcsmote_shuttle_D75_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.2s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  10.5s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   7.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.3s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   8.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.1s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.4s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   9.4s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  16.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.2s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.6s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (52/76) Par: pcsmote_shuttle_D75_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   7.3s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   7.4s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   6.9s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.5s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.2s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   5.7s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   6.1s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   5.3s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   5.9s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   6.2s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   5.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   9.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  10.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   9.9s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (53/76) Par: pcsmote_shuttle_D75_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=  11.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.7s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   9.1s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   8.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.4s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   9.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=  10.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  10.5s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  10.5s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=  10.4s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  16.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  15.1s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  14.8s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (54/76) Par: pcsmote_shuttle_D75_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   8.1s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   8.5s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   8.2s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   5.8s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   6.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   6.2s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   6.4s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   6.6s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   6.6s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   6.7s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   7.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   6.7s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  10.8s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  11.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=  10.8s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (55/76) Par: pcsmote_wdbc_D25_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (56/76) Par: pcsmote_wdbc_D25_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (57/76) Par: pcsmote_wdbc_D25_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (58/76) Par: pcsmote_wdbc_D25_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (59/76) Par: pcsmote_wdbc_D25_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (60/76) Par: pcsmote_wdbc_D25_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (61/76) Par: pcsmote_wdbc_D50_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (62/76) Par: pcsmote_wdbc_D50_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (63/76) Par: pcsmote_wdbc_D50_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (64/76) Par: pcsmote_wdbc_D50_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (65/76) Par: pcsmote_wdbc_D50_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (66/76) Par: pcsmote_wdbc_D50_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (67/76) Par: pcsmote_wdbc_D75_R25_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (68/76) Par: pcsmote_wdbc_D75_R25_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (69/76) Par: pcsmote_wdbc_D75_R50_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (70/76) Par: pcsmote_wdbc_D75_R50_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (71/76) Par: pcsmote_wdbc_D75_R75_Pentropia_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (72/76) Par: pcsmote_wdbc_D75_R75_Pproporcion_train.csv  (tipo: aumentado)\n",
      "🔎 Técnica: pcsmote | Dataset: wdbc | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (73/76) Par: glass_train.csv  (tipo: base)\n",
      "🔎 Técnica: base | Dataset: glass | Densidad: NA | Riesgo: NA | Pureza: NA\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (74/76) Par: heart_train.csv  (tipo: base)\n",
      "🔎 Técnica: base | Dataset: heart | Densidad: NA | Riesgo: NA | Pureza: NA\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (75/76) Par: shuttle_train.csv  (tipo: base)\n",
      "🔎 Técnica: base | Dataset: shuttle | Densidad: NA | Riesgo: NA | Pureza: NA\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   1.2s\n",
      "[CV] END classifier__C=3.845401188473625, classifier__kernel=linear; total time=   0.9s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=9.60714306409916, classifier__kernel=linear; total time=   0.9s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=7.41993941811405, classifier__kernel=linear; total time=   0.9s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=6.086584841970366, classifier__kernel=linear; total time=   0.9s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   1.0s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   1.5s\n",
      "[CV] END classifier__C=1.6601864044243653, classifier__kernel=linear; total time=   0.9s\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "📂 (76/76) Par: wdbc_train.csv  (tipo: base)\n",
      "🔎 Técnica: base | Dataset: wdbc | Densidad: NA | Riesgo: NA | Pureza: NA\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "📁 Resultados guardados: ../resultados/resultados_SVM.csv\n",
      "📁 Resultados guardados: ../resultados/resultados_LogisticRegression.csv\n",
      "📁 Resultados guardados: ../resultados/resultados_RandomForest.csv\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# --- Definición de modelos e hiperparámetros (base) ---\n",
    "modelos = {\n",
    "    \"SVM\": {\n",
    "        \"pipeline\": Pipeline([('classifier', SVC(random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__C': uniform(0.1, 10),\n",
    "            'classifier__kernel': ['linear', 'rbf'],\n",
    "            'classifier__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"pipeline\": Pipeline([('classifier', LogisticRegression(max_iter=1000, random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__C': uniform(0.1, 10),\n",
    "            'classifier__penalty': ['l2'],\n",
    "            'classifier__solver': ['lbfgs']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([('classifier', RandomForestClassifier(random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Rutas ---\n",
    "ruta_aug  = \"../datasets/datasets_aumentados/\"\n",
    "ruta_base = \"../datasets/datasets_aumentados/base/\"\n",
    "\n",
    "# --- Armar pares (train/test) tanto en aumentados como en base ---\n",
    "def listar_pares(ruta, tipo):\n",
    "    pares = []\n",
    "    if not os.path.isdir(ruta):\n",
    "        return pares\n",
    "    for f in os.listdir(ruta):\n",
    "        if not (f.endswith(\"_train.csv\") and os.path.isfile(os.path.join(ruta, f))):\n",
    "            continue\n",
    "        path_train = os.path.join(ruta, f)\n",
    "        path_test  = os.path.join(ruta, f.replace(\"_train.csv\", \"_test.csv\"))\n",
    "        if not os.path.isfile(path_test):\n",
    "            print(f\"⚠️ Falta el test para: {f} -> esperado: {os.path.basename(path_test)}\")\n",
    "            continue\n",
    "        pares.append({\n",
    "            \"tipo\": tipo,\n",
    "            \"nombre_train\": f,\n",
    "            \"path_train\": path_train,\n",
    "            \"path_test\": path_test\n",
    "        })\n",
    "    return pares\n",
    "\n",
    "pares = listar_pares(ruta_aug, \"aumentado\") + listar_pares(ruta_base, \"base\")\n",
    "\n",
    "# Orden estable para reproducibilidad (por tipo y nombre de train)\n",
    "pares = sorted(pares, key=lambda x: (x[\"tipo\"], x[\"nombre_train\"]))\n",
    "\n",
    "# --- Métricas personalizadas para CV ---\n",
    "scoring = {\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'balanced_accuracy': 'balanced_accuracy',\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'cohen_kappa': make_scorer(cohen_kappa_score)\n",
    "}\n",
    "\n",
    "# --- Acumuladores de resultados (un CSV por modelo con todo junto) ---\n",
    "resultados_por_modelo = {nombre: [] for nombre in modelos}\n",
    "\n",
    "# --- Contar total de pares ---\n",
    "total_pares = len(pares)\n",
    "contador_par = 0\n",
    "\n",
    "# --- Helper: carga CSV con fallback de target ---\n",
    "def cargar_xy(path_csv):\n",
    "    df = pd.read_csv(path_csv)\n",
    "    if \"target\" in df.columns:\n",
    "        X = df.drop(columns=[\"target\"]).values\n",
    "        y = df[\"target\"].values\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "    return X, y\n",
    "\n",
    "# --- Evaluación por par (train/test) ---\n",
    "for item in pares:\n",
    "    contador_par += 1\n",
    "    archivo_train = item[\"nombre_train\"]\n",
    "    ruta_train = item[\"path_train\"]\n",
    "    ruta_test  = item[\"path_test\"]\n",
    "    tipo = item[\"tipo\"]\n",
    "\n",
    "    # Parseo de metadatos desde el nombre *_train.csv\n",
    "    nombre = archivo_train.replace(\".csv\", \"\")\n",
    "    partes = nombre.split(\"_\")\n",
    "    if tipo == \"aumentado\":\n",
    "        # Formato esperado: pcsmote_<dataset>_D<densidad>_R<riesgo>_P<pureza>_train\n",
    "        if len(partes) < 6:\n",
    "            print(f\"⚠️ Nombre inválido/incompleto (aumentado): {archivo_train}\")\n",
    "            continue\n",
    "        tecnica = partes[0]\n",
    "        nombre_dataset = partes[1]\n",
    "        densidad = partes[2][1:]  # quita 'D'\n",
    "        riesgo   = partes[3][1:]  # quita 'R'\n",
    "        pureza   = partes[4][1:]  # quita 'P'\n",
    "    else:\n",
    "        # Base: <dataset>_train\n",
    "        tecnica = \"base\"\n",
    "        nombre_dataset = nombre.replace(\"_train\", \"\")\n",
    "        densidad = \"NA\"\n",
    "        riesgo   = \"NA\"\n",
    "        pureza   = \"NA\"\n",
    "\n",
    "    # Progreso\n",
    "    print(f\"\\n📂 ({contador_par}/{total_pares}) Par: {archivo_train}  (tipo: {tipo})\")\n",
    "    print(f\"🔎 Técnica: {tecnica} | Dataset: {nombre_dataset} | Densidad: {densidad} | Riesgo: {riesgo} | Pureza: {pureza}\")\n",
    "\n",
    "    # Carga train/test\n",
    "    try:\n",
    "        X_train, y_train = cargar_xy(ruta_train)\n",
    "        X_test,  y_test  = cargar_xy(ruta_test)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al leer train/test ({archivo_train}): {e}\")\n",
    "        continue\n",
    "\n",
    "    # Heurística: datasets grandes (ej. shuttle) => simplificar SVM\n",
    "    n_samples = X_train.shape[0]\n",
    "    es_grande = (n_samples >= 30000) or (nombre_dataset.lower() == \"shuttle\")\n",
    "\n",
    "    # CV base y grande\n",
    "    cv_base = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_grande = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    # Búsqueda aleatoria + CV por modelo\n",
    "    for nombre_modelo, info in modelos.items():\n",
    "        print(f\"⚙️ Validando modelo con Pipeline y RandomizedSearchCV: {nombre_modelo}\")\n",
    "\n",
    "        # Config local por modelo (sin mutar el dict base)\n",
    "        pipeline_local = None\n",
    "        params_local = None\n",
    "        cv_actual = cv_base\n",
    "        n_iter_actual = 10\n",
    "        n_jobs_actual = -1\n",
    "\n",
    "        if nombre_modelo == \"SVM\":\n",
    "            pipeline_local = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', SVC(random_state=42))\n",
    "            ])\n",
    "            if es_grande:\n",
    "                params_local = {\n",
    "                    'classifier__kernel': ['linear'], # agregar mas kernel (gaussiano, radiales, cubicos, etc)\n",
    "                    'classifier__C': uniform(0.1, 10)\n",
    "                }\n",
    "                pipeline_local.named_steps['classifier'].set_params(max_iter=5000, cache_size=1000)\n",
    "                cv_actual = cv_grande\n",
    "                n_iter_actual = 5\n",
    "                n_jobs_actual = 1\n",
    "            else:\n",
    "                params_local = info['param_distributions']\n",
    "\n",
    "        elif nombre_modelo == \"LogisticRegression\":\n",
    "            pipeline_local = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "            ])\n",
    "            params_local = info['param_distributions']\n",
    "\n",
    "        else:  # RandomForest\n",
    "            pipeline_local = info['pipeline']\n",
    "            params_local = info['param_distributions']\n",
    "            # agregar mas parametros para ajustar. Buscar hipercalibration\n",
    "            \n",
    "            # aumentar info de la pcsmote en los archivos de log\n",
    "\n",
    "        try:\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=pipeline_local,\n",
    "                param_distributions=params_local,\n",
    "                n_iter=n_iter_actual,\n",
    "                cv=cv_actual,\n",
    "                scoring=scoring,\n",
    "                refit='f1_macro',      # refitea el mejor en TODO el train\n",
    "                random_state=42,\n",
    "                n_jobs=n_jobs_actual,\n",
    "                pre_dispatch=\"2*n_jobs\",\n",
    "                verbose=2,\n",
    "                error_score='raise'\n",
    "            )\n",
    "            search.fit(X_train, y_train)\n",
    "\n",
    "            # Métricas CV en el mejor índice\n",
    "            idx = search.best_index_\n",
    "            cv_f1   = search.cv_results_['mean_test_f1_macro'][idx]\n",
    "            cv_bacc = search.cv_results_['mean_test_balanced_accuracy'][idx]\n",
    "            cv_mcc  = search.cv_results_['mean_test_mcc'][idx]\n",
    "            cv_kappa= search.cv_results_['mean_test_cohen_kappa'][idx]\n",
    "\n",
    "            # Evaluación out-of-sample en TEST con el mejor estimador ya refiteado\n",
    "            best_est = search.best_estimator_\n",
    "            y_pred = best_est.predict(X_test)\n",
    "\n",
    "            test_f1   = f1_score(y_test, y_pred, average='macro')\n",
    "            test_bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "            test_mcc  = matthews_corrcoef(y_test, y_pred)\n",
    "            test_kappa= cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "            resultados_por_modelo[nombre_modelo].append({\n",
    "                # metadata\n",
    "                'dataset': nombre_dataset,\n",
    "                'tecnica': tecnica,\n",
    "                'densidad': densidad,\n",
    "                'riesgo': riesgo,\n",
    "                'pureza': pureza,\n",
    "                'modelo': nombre_modelo,\n",
    "                'mejor_configuracion': str(search.best_params_),\n",
    "                # métricas CV (train CV)\n",
    "                'cv_f1_macro': cv_f1,\n",
    "                'cv_balanced_accuracy': cv_bacc,\n",
    "                'cv_mcc': cv_mcc,\n",
    "                'cv_cohen_kappa': cv_kappa,\n",
    "                # métricas TEST (out-of-sample)\n",
    "                'test_f1_macro': test_f1,\n",
    "                'test_balanced_accuracy': test_bacc,\n",
    "                'test_mcc': test_mcc,\n",
    "                'test_cohen_kappa': test_kappa,\n",
    "            })\n",
    "\n",
    "            # Guardado parcial (sobrescribe) por modelo\n",
    "            os.makedirs(\"../resultados\", exist_ok=True)\n",
    "            output_path = f\"../resultados/resultados_{nombre_modelo}.csv\"\n",
    "            pd.DataFrame(resultados_por_modelo[nombre_modelo]).to_csv(output_path, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al validar {nombre_modelo} en {archivo_train}: {e}\")\n",
    "\n",
    "# --- Persistencia final (por si quedó algo sin volcar) ---\n",
    "os.makedirs(\"../resultados\", exist_ok=True)\n",
    "for nombre_modelo, lista_resultados in resultados_por_modelo.items():\n",
    "    df_final = pd.DataFrame(lista_resultados)\n",
    "    output_path = f\"../resultados/resultados_{nombre_modelo}.csv\"\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"📁 Resultados guardados: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb30cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_otra_tecnica_grid(nombre_dataset, config, tecnica, modelo_clasificador, nombre_modelo, nombre_tec):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    if y.dtype == object or isinstance(y[0], str):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    if config.get(\"tipo\") == \"imagen\":\n",
    "        X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "    clases_minor = config.get(\"clases_minor\", [])\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metricas_fold = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"🔁 Fold {fold_idx + 1}/5\")\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        pca = PCA(n_components=min(X_train.shape[1], 100))\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        try:\n",
    "            if tecnica:\n",
    "                X_res, y_res = tecnica.fit_resample(X_train_pca, y_train)\n",
    "            else:\n",
    "                X_res, y_res = X_train_pca, y_train\n",
    "\n",
    "            modelo_escalado = get_modelo_escalado_si_es_necesario(modelo_clasificador, nombre_modelo)\n",
    "            modelo_escalado.fit(X_res, y_res)\n",
    "            y_pred = modelo_escalado.predict(X_test_pca)\n",
    "\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "            f1_minor = None\n",
    "            if clases_minor:\n",
    "                mask = np.isin(y_test, clases_minor)\n",
    "                if np.any(mask):\n",
    "                    f1_minor = f1_score(y_test[mask], y_pred[mask], average='macro')\n",
    "\n",
    "            metricas_fold.append({\n",
    "                'f1_score_macro': f1_macro,\n",
    "                'f1_score_minor': f1_minor,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'mcc': mcc,\n",
    "                'cohen_kappa': kappa\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en fold {fold_idx + 1}: {e}\")\n",
    "            metricas_fold.append({\n",
    "                'f1_score_macro': None,\n",
    "                'f1_score_minor': None,\n",
    "                'balanced_accuracy': None,\n",
    "                'mcc': None,\n",
    "                'cohen_kappa': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    df_metricas = pd.DataFrame(metricas_fold)\n",
    "    df_mean = df_metricas.dropna().mean(numeric_only=True).to_dict()\n",
    "    df_mean.update({\n",
    "        'dataset': nombre_dataset,\n",
    "        'modelo': nombre_modelo,\n",
    "        'tecnica': nombre_tec\n",
    "    })\n",
    "\n",
    "    df_final = pd.DataFrame([df_mean])\n",
    "    df_final.to_csv(f\"../resultados/{nombre_tec}_grid_{nombre_dataset}_{nombre_modelo}.csv\", index=False)\n",
    "    print(f\"📁 Resultados guardados en: ../resultados/{nombre_tec}_grid_{nombre_dataset}_{nombre_modelo}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluar_pcsmote_grid_search(nombre_dataset, config, percentiles_densidad, percentiles_riesgo, criterios_pureza, modelo_clasificador, nombre_modelo):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "\n",
    "    # 1) Cargar dataset según los parámetros recibidos (ruta, columnas, tipo, etc.)\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Codificar etiquetas si son strings u objetos\n",
    "    if y.dtype == object or isinstance(y[0], str):  \n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Si es un dataset de imágenes, aplastar (reshape) cada imagen a vector 1D\n",
    "    # Esto convierte (N, H, W, C) en (N, H*W*C) para trabajar como tabular\n",
    "    if config.get(\"tipo\") == \"imagen\":\n",
    "        X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "    # 4) Obtener clases minoritarias si están definidas\n",
    "    clases_minor = config.get(\"clases_minor\", [])\n",
    "    resultados = []\n",
    "\n",
    "    # 5) Dividir en train/test con estratificación, antes del sobremuestreo\n",
    "    X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 6) Generar todas las combinaciones posibles entre los parámetros definidos\n",
    "    combinaciones = list(product(percentiles_densidad, percentiles_riesgo, criterios_pureza))\n",
    "\n",
    "    # 7) Iterar sobre cada combinación y aplicar sobremuestreo + entrenamiento + evaluación\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        inicio = datetime.now()\n",
    "        print(f\"#{idx:02d} 🧪 {nombre_modelo} | {nombre_dataset} | Densidad: {pdens} | Riesgo: {priesgo} | Pureza: {criterio}\")\n",
    "        print(f\"⏱️  Tiempo de inicio: {inicio.strftime('%H:%M:%S')} hs\")        \n",
    "\n",
    "        try:\n",
    "            # 7.a) Inicializar sampler PCSMOTE con los parámetros actuales\n",
    "            sampler = PCSMOTE(\n",
    "                random_state=42,\n",
    "                percentil_densidad=pdens,\n",
    "                percentil_dist=priesgo,\n",
    "                criterio_pureza=criterio,\n",
    "                modo_espacial='3d'  # Puede usarse '2d' si se desea modificar\n",
    "            )\n",
    "\n",
    "            # 7.b) Aplicar sobremuestreo sobre el set de entrenamiento (solo si método está definido)\n",
    "            if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "                X_res, y_res = sampler.fit_resample_multiclass(X_train_eval, y_train_eval)\n",
    "            else:\n",
    "                X_res, y_res = sampler.fit_resample(X_train_eval, y_train_eval)\n",
    "\n",
    "            # 7.c) Entrenar modelo sobre los datos aumentados\n",
    "            modelo_clasificador.fit(X_res, y_res)\n",
    "\n",
    "            # 7.d) Predecir sobre el set de evaluación (test)\n",
    "            y_pred = modelo_clasificador.predict(X_test_eval)\n",
    "\n",
    "            # 7.e) Calcular métricas de rendimiento\n",
    "            f1_macro = f1_score(y_test_eval, y_pred, average='macro')\n",
    "            balanced_acc = balanced_accuracy_score(y_test_eval, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test_eval, y_pred)\n",
    "            kappa = cohen_kappa_score(y_test_eval, y_pred)\n",
    "\n",
    "            # 7.f) Si hay clases minoritarias definidas, calcular también f1_score sobre ellas\n",
    "            f1_minor = None\n",
    "            if clases_minor:\n",
    "                mask = np.isin(y_test_eval, clases_minor)\n",
    "                if np.any(mask):\n",
    "                    f1_minor = f1_score(y_test_eval[mask], y_pred[mask], average='macro')\n",
    "                    print(f\"📊 f1_minor sobre {np.sum(mask)} muestras minoritarias.\")\n",
    "                    print(\"✔️ Verdaderas:\", y_test_eval[mask])\n",
    "                    print(\"❌ Predichas:\", y_pred[mask])\n",
    "\n",
    "            # 7.g) Mostrar resumen de rendimiento\n",
    "            print(f\"✅ Config OK | F1_macro: {f1_macro:.4f}\" + (f\", F1_minor: {f1_minor:.4f}\" if f1_minor else \"\"))\n",
    "\n",
    "            # 7.h) Guardar resultados de esta corrida\n",
    "            resultados.append({\n",
    "                'dataset': nombre_dataset,\n",
    "                'modelo': nombre_modelo,\n",
    "                'densidad': pdens,\n",
    "                'riesgo': priesgo,\n",
    "                'pureza': criterio,\n",
    "                'f1_score_macro': f1_macro,\n",
    "                'f1_score_minor': f1_minor,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'mcc': mcc,\n",
    "                'cohen_kappa': kappa\n",
    "            })\n",
    "            \n",
    "            # 7.i) Mostrar tiempo de ejecución\n",
    "            fin = datetime.now()\n",
    "            transcurrido = fin - inicio\n",
    "            print(f\"✅ Tiempo final: {fin.strftime('%H:%M:%S')} hs\")\n",
    "            print(f\"🕒 Total transcurrido: {str(transcurrido).rjust(8, '0')} hs\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # 8) En caso de error, registrar el fallo con detalles\n",
    "            print(f\"⚠️ Error con config D={pdens} R={priesgo} P={criterio}: {e}\")\n",
    "            resultados.append({\n",
    "                'dataset': nombre_dataset,\n",
    "                'modelo': nombre_modelo,\n",
    "                'densidad': pdens,\n",
    "                'riesgo': priesgo,\n",
    "                'pureza': criterio,\n",
    "                'f1_score_macro': None,\n",
    "                'f1_score_minor': None,\n",
    "                'balanced_accuracy': None,\n",
    "                'mcc': None,\n",
    "                'cohen_kappa': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # 9) Convertir resultados a DataFrame y exportar a CSV\n",
    "    df = pd.DataFrame(resultados)\n",
    "    df.to_csv(f\"../resultados/pcsmote_grid_{nombre_dataset}_{nombre_modelo}.csv\", index=False)\n",
    "    print(f\"📁 Resultados guardados en: ../resultados/pcsmote_grid_{nombre_dataset}_{nombre_modelo}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from config_datasets import config_datasets\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "tecnicas_sobremuestreo = {\n",
    "    # \"base\": None,\n",
    "    \"pcsmote\": PCSMOTE(\n",
    "        random_state=42,\n",
    "        percentil_densidad=50,  # serán reemplazados dinámicamente en el grid\n",
    "        percentil_dist=50,\n",
    "        criterio_pureza='entropia',\n",
    "        modo_espacial='3d',\n",
    "        verbose=False\n",
    "    ),\n",
    "    # \"smote\": SMOTE(random_state=42),\n",
    "    # \"adasyn\": ADASYN(random_state=42),\n",
    "    # \"borderline\": BorderlineSMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "modelos = {\n",
    "    # \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    # \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "    # \"XGBoost\": XGBClassifier(...)  // esta fallando por la etiquetas\n",
    "}\n",
    "\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    if nombre_dataset == \"shuttle\":\n",
    "        for nombre_modelo, modelo in modelos.items():\n",
    "            for nombre_tec, tecnica in tecnicas_sobremuestreo.items():\n",
    "                print(f\"\\n=== Ejecutando grid para {nombre_dataset} | modelo: {nombre_modelo} | técnica: {nombre_tec} ===\")\n",
    "\n",
    "                if nombre_tec == \"pcsmote\":\n",
    "\n",
    "\n",
    "                    evaluar_pcsmote_grid_search(\n",
    "                        nombre_dataset, config,\n",
    "                        percentiles_densidad, percentiles_riesgo,\n",
    "                        criterios_pureza, modelo, nombre_modelo\n",
    "                    )\n",
    "                # else:\n",
    "                #     evaluar_otra_tecnica_grid(\n",
    "                #         nombre_dataset, config, tecnica,\n",
    "                #         modelo, nombre_modelo, nombre_tec\n",
    "                #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ca6ff",
   "metadata": {},
   "source": [
    "## Busqueda del mejor resultado para cada tecnica de sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "datasets = ['shuttle']\n",
    "tecnicas = ['base', 'smote', 'adasyn', 'borderline', 'pcsmote']\n",
    "modelos = ['RandomForest', 'LogisticRegression', 'SVM']\n",
    "\n",
    "resumen = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for tecnica in tecnicas:\n",
    "        archivos = glob.glob(f\"../resultados/{tecnica}_grid_{dataset}_*.csv\")\n",
    "        if not archivos:\n",
    "            print(f\"⚠️ No hay resultados para {dataset} con técnica {tecnica}\")\n",
    "            continue\n",
    "\n",
    "        mejores_filas = []\n",
    "\n",
    "        for archivo in archivos:\n",
    "            try:\n",
    "                df = pd.read_csv(archivo)\n",
    "                df_valid = df.dropna(subset=[\"f1_score\"])\n",
    "                if df_valid.empty:\n",
    "                    continue\n",
    "\n",
    "                fila = df_valid.sort_values(by=\"f1_score\", ascending=False).iloc[0].copy()\n",
    "                fila['dataset'] = dataset\n",
    "                fila['tecnica'] = tecnica\n",
    "\n",
    "                nombre_archivo = os.path.basename(archivo)\n",
    "                modelo_detectado = next((m for m in modelos if m in nombre_archivo), \"desconocido\")\n",
    "                fila['modelo'] = modelo_detectado\n",
    "\n",
    "                mejores_filas.append(fila)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error procesando {archivo}: {e}\")\n",
    "\n",
    "\n",
    "        # Guardar el mejor resultado por técnica\n",
    "        if mejores_filas:\n",
    "            mejor = pd.DataFrame(mejores_filas).sort_values(by=\"f1_score\", ascending=False).iloc[0]\n",
    "            resumen.append(mejor)\n",
    "\n",
    "# Crear DataFrame final\n",
    "resumen_df = pd.DataFrame(resumen)\n",
    "\n",
    "# Reordenar columnas según disponibilidad\n",
    "cols = ['dataset', 'tecnica', 'modelo', 'f1_score']\n",
    "for col in ['balanced_accuracy', 'mcc', 'cohen_kappa', 'densidad', 'riesgo', 'pureza','error']:\n",
    "    if col in resumen_df.columns:\n",
    "        cols.append(col)\n",
    "\n",
    "resumen_df = resumen_df[cols]\n",
    "resumen_df.to_csv(\"../resultados/resumen_mejores_por_tecnica.csv\", index=False)\n",
    "print(\"✅ Resumen guardado en ../resultados/resumen_mejores_por_tecnica.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e351ce",
   "metadata": {},
   "source": [
    "## Generacion de graficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17140685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar el resumen\n",
    "df = pd.read_csv(\"../resultados/resumen_mejores_por_tecnica.csv\")\n",
    "\n",
    "# Ajustar orden de técnicas para mejor visualización\n",
    "orden_tecnicas = ['base', 'smote', 'adasyn', 'borderline', 'pcsmote']\n",
    "df['tecnica'] = pd.Categorical(df['tecnica'], categories=orden_tecnicas, ordered=True)\n",
    "\n",
    "# Plot por dataset\n",
    "for dataset in df['dataset'].unique():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df_sub = df[df['dataset'] == dataset].sort_values(\"tecnica\")\n",
    "\n",
    "    sns.barplot(\n",
    "        data=df_sub,\n",
    "        x=\"tecnica\",\n",
    "        y=\"f1_score\",\n",
    "        hue=\"modelo\",  # opcional: para ver qué modelo dio ese resultado\n",
    "        dodge=False,\n",
    "        palette=\"viridis\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"F1-score por técnica de sobremuestreo - {dataset}\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel(\"Técnica\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
