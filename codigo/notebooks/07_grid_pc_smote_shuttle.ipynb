{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluación de PC-SMOTE con Grid Search en el dataset Shuttle (Generación de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de búsqueda de módulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar módulos o paquetes cuando hacés un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importación de módulos y librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Módulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Función para cargar datasets según configuración\n",
    "from config_datasets import config_datasets                    # Diccionario de configuración de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluación de sobremuestreo con partición hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la técnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementación principal de PCSMOTE\n",
    "\n",
    "# --- Librerías estándar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generación de combinaciones de parámetros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "\n",
    "# --- Librerías científicas ---\n",
    "import numpy as np                                              # Operaciones numéricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulación y análisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para búsqueda de hiperparámetros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificación de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creación de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: división y validación ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # División de datos en train/test\n",
    "    StratifiedKFold,                                            # Validación cruzada estratificada\n",
    "    RandomizedSearchCV                                          # Búsqueda aleatoria de hiperparámetros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducción de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # Análisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: métricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # Métrica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisión balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresión logística\n",
    "from sklearn.svm import SVC                                      # Máquinas de Vectores de Soporte (SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generación del caso base\n",
    "\n",
    "Este código realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio específico para almacenar la versión original del dataset sin ningún tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma función `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) están en formato de texto u objeto, se convierten a valores numéricos con `LabelEncoder`.\n",
    "   - Se realiza una división estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporción 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- función: generar caso base (train/test sin sobremuestreo) ---\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin PCSMOTE) para un dataset: guarda train y test en ruta_base.\n",
    "    Usa la misma lógica de carga que el resto del pipeline (cargar_dataset + LabelEncoder opcional).\n",
    "\n",
    "    Retorna:\n",
    "        (path_train, path_test)\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test  # ya generado\n",
    "\n",
    "    # 1) Cargar dataset base con tu helper habitual\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar etiquetas numéricas si vienen como strings/objects\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado (mismo seed para reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Guardar CSVs\n",
    "    pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_train, index=False)\n",
    "    pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_test, index=False)\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(nombre_dataset, config, percentil_densidad, percentil_riesgo, criterio_pureza, test_size=0.2):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "\n",
    "    try:\n",
    "        # 1) Cargar dataset original\n",
    "        X, y, clases = cargar_dataset(\n",
    "            path=config[\"path\"],\n",
    "            clase_minoria=config.get(\"clase_minoria\"),\n",
    "            col_features=config.get(\"col_features\"),\n",
    "            col_target=config.get(\"col_target\"),\n",
    "            sep=config.get(\"sep\", \",\"),\n",
    "            header=config.get(\"header\", None),\n",
    "            binarizar=False,\n",
    "            tipo=config.get(\"tipo\", \"tabular\")\n",
    "        )\n",
    "\n",
    "        # 2) Codificar etiquetas si son strings\n",
    "        if y.dtype == object or isinstance(y[0], str):\n",
    "            y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        # 3) Si es un dataset de imágenes, convertir a vector plano\n",
    "        if config.get(\"tipo\") == \"imagen\":\n",
    "            X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "        # 4) Dividir en train/test (antes de sobremuestrear)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # 5) Escalar ambos usando estadísticas del train\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 6) Aplicar PCSMOTE sobre el set de entrenamiento escalado\n",
    "        print(f\"🧬 Aplicando PCSMOTE | Densidad: {percentil_densidad} | Riesgo: {percentil_riesgo} | Pureza: {criterio_pureza}\")\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_dist=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            modo_espacial='3d'\n",
    "        )\n",
    "        sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_train_res, y_train_res = sampler.fit_resample_multiclass(X_train, y_train)\n",
    "        else:\n",
    "            X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        # 7) Guardar datasets: train aumentado y test escalado\n",
    "        print(f\"💾 Guardando datasets aumentados...\")\n",
    "\n",
    "        ruta_salida = f\"../datasets/datasets_aumentados/\"\n",
    "        os.makedirs(ruta_salida, exist_ok=True)\n",
    "\n",
    "        nombre_base = f\"pcsmote_{nombre_dataset}_D{percentil_densidad}_R{percentil_riesgo}_P{criterio_pureza}\"\n",
    "        path_train = os.path.join(ruta_salida, f\"{nombre_base}_train.csv\")\n",
    "        path_test = os.path.join(ruta_salida, f\"{nombre_base}_test.csv\")\n",
    "\n",
    "        # Guardar train aumentado\n",
    "        df_train = pd.DataFrame(X_train_res)\n",
    "        df_train[\"target\"] = y_train_res\n",
    "        df_train.to_csv(path_train, index=False)\n",
    "\n",
    "        # Guardar test escalado (sin sobremuestrear)\n",
    "        df_test = pd.DataFrame(X_test)\n",
    "        df_test[\"target\"] = y_test\n",
    "        df_test.to_csv(path_test, index=False)\n",
    "\n",
    "        print(f\"✅ Datasets guardados:\\n- Train aumentado: {path_train}\\n- Test escalado: {path_test}\")\n",
    "        return path_train, path_test, sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### 🧬 Aumento de Datasets mediante Técnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versión balanceada de cada dataset original mediante la aplicación de técnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la técnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generación de muestras sintéticas en función de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones específicas de parámetros según la técnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuración empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ⚠️ Esta fase no incluye entrenamiento ni validación de modelos. Su único propósito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partición `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluación imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Dataset: shuttle\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/shuttle_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/shuttle_test.csv\n",
      "\n",
      "📁 Dataset: wdbc\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/wdbc_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/wdbc_test.csv\n",
      "\n",
      "📁 Dataset: glass\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/glass_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/glass_test.csv\n",
      "\n",
      "📁 Dataset: heart\n",
      "⚠️ Advertencia: columnas no numéricas detectadas: [11, 12]\n",
      "🟦 Caso base generado:\n",
      " - Train: ../datasets/datasets_aumentados/base/heart_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/heart_test.csv\n"
     ]
    }
   ],
   "source": [
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "combinaciones = list(product(percentiles_densidad, percentiles_riesgo, criterios_pureza))\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📁 Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # --- CASO BASE ---\n",
    "    base_train, base_test = generar_caso_base(nombre_dataset, config)\n",
    "    print(f\"🟦 Caso base generado:\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "    # --- GRID DE PCSMOTE ---\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        print(f\"#{idx:02d} ➕ Aumentando con D={pdens} | R={priesgo} | P={criterio}\")\n",
    "\n",
    "        path_train, path_test, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            config=config,\n",
    "            percentil_densidad=pdens,\n",
    "            percentil_riesgo=priesgo,\n",
    "            criterio_pureza=criterio\n",
    "        )\n",
    "\n",
    "        if path_train and sampler:\n",
    "            print(f\"✅ Guardado exitoso:\\n - Train: {path_train}\\n - Test: {path_test}\")\n",
    "            log_path = f\"../datasets/datasets_aumentados/logs/log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_P{criterio}.csv\"\n",
    "            sampler.exportar_log_csv(log_path)\n",
    "            print(f\"📄 Log exportado: {log_path}\")\n",
    "        else:\n",
    "            print(\"❌ Falló la generación.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc87fa1",
   "metadata": {},
   "source": [
    "### Evaluación de modelos con validación cruzada estratificada\n",
    "\n",
    "Para evaluar el rendimiento de los modelos de clasificación sobre los datasets previamente balanceados, se utilizó validación cruzada estratificada de 5 particiones (Stratified K-Fold con *k=5*). Este método garantiza que en cada fold de entrenamiento y validación se preserve la proporción original de clases, lo cual es especialmente importante en tareas de clasificación multiclase con datasets balanceados artificialmente.\n",
    "\n",
    "Durante el proceso, cada modelo es entrenado y evaluado cinco veces, cada vez usando un subconjunto distinto como conjunto de prueba y el resto como conjunto de entrenamiento. Las métricas calculadas en cada iteración (F1-score macro, balanced accuracy, MCC y kappa de Cohen) se promedian para obtener un valor representativo y del rendimiento general del modelo sobre ese dataset aumentado.\n",
    "\n",
    "Este enfoque evita sobreajuste y proporciona una evaluación más confiable que una simple división train/test, permitiendo comparar de forma justa distintas configuraciones de sobremuestreo y modelos de clasificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5320c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D25_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D50_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_glass_D75_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: glass | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D25_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 25 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D50_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 50 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R25_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 25 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R50_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 50 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R50_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 50 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R75_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 75 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_heart_D75_R75_Pproporcion_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: heart | Densidad: 75 | Riesgo: 75 | Pureza: proporcion\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: LogisticRegression\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: RandomForest\n",
      "\n",
      "📂 Dataset: pcsmote_shuttle_D25_R25_Pentropia_train.csv, tipo: aumentado\n",
      "🔎 Técnica: pcsmote | Dataset: shuttle | Densidad: 25 | Riesgo: 25 | Pureza: entropia\n",
      "⚙️ Validando modelo con Pipeline y RandomizedSearchCV: SVM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 125\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    115\u001b[39m     search = RandomizedSearchCV(\n\u001b[32m    116\u001b[39m         estimator=info[\u001b[33m'\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    117\u001b[39m         param_distributions=info[\u001b[33m'\u001b[39m\u001b[33mparam_distributions\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m         n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m    124\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     resultados_por_modelo[nombre_modelo].append({\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m: nombre_dataset,\n\u001b[32m    129\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtecnica\u001b[39m\u001b[33m'\u001b[39m: tecnica,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmejor_configuracion\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(search.best_params_)\n\u001b[32m    139\u001b[39m     })\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Requiere:\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# --- Definición de modelos e hiperparámetros ---\n",
    "modelos = {\n",
    "    \"SVM\": {\n",
    "        \"pipeline\": Pipeline([('classifier', SVC(random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__C': uniform(0.1, 10),\n",
    "            'classifier__kernel': ['linear', 'rbf'],\n",
    "            'classifier__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"pipeline\": Pipeline([('classifier', LogisticRegression(max_iter=1000, random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__C': uniform(0.1, 10),\n",
    "            'classifier__penalty': ['l2'],\n",
    "            'classifier__solver': ['lbfgs']\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([('classifier', RandomForestClassifier(random_state=42))]),\n",
    "        \"param_distributions\": {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Rutas ---\n",
    "ruta_aug  = \"../datasets/datasets_aumentados/\"\n",
    "ruta_base = \"../datasets/datasets_aumentados/base/\"\n",
    "\n",
    "# --- Listado de archivos: aumentados + base ---\n",
    "archivos = []\n",
    "\n",
    "# Aumentados (evita tomar la carpeta base)\n",
    "if os.path.isdir(ruta_aug):\n",
    "    for f in os.listdir(ruta_aug):\n",
    "        p = os.path.join(ruta_aug, f)\n",
    "        if os.path.isfile(p) and f.endswith(\"_train.csv\"):\n",
    "            archivos.append({\"path\": p, \"tipo\": \"aumentado\", \"nombre\": f})\n",
    "\n",
    "# Base\n",
    "if os.path.isdir(ruta_base):\n",
    "    for f in os.listdir(ruta_base):\n",
    "        p = os.path.join(ruta_base, f)\n",
    "        if os.path.isfile(p) and f.endswith(\"_train.csv\"):\n",
    "            archivos.append({\"path\": p, \"tipo\": \"base\", \"nombre\": f})\n",
    "\n",
    "# Orden estable para reproducibilidad\n",
    "archivos = sorted(archivos, key=lambda x: (x[\"tipo\"], x[\"nombre\"]))\n",
    "\n",
    "# --- Métricas personalizadas ---\n",
    "scoring = {\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'balanced_accuracy': 'balanced_accuracy',\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'cohen_kappa': make_scorer(cohen_kappa_score)\n",
    "}\n",
    "\n",
    "# --- Acumuladores de resultados (un CSV por modelo con todo junto) ---\n",
    "resultados_por_modelo = {nombre: [] for nombre in modelos}\n",
    "\n",
    "# --- Evaluación ---\n",
    "for item in archivos:\n",
    "    ruta_archivo = item[\"path\"]\n",
    "    archivo = item[\"nombre\"]\n",
    "    tipo = item[\"tipo\"]\n",
    "\n",
    "    # Parseo de metadatos desde el nombre\n",
    "    if tipo == \"aumentado\":\n",
    "        # Formato esperado: pcsmote_<dataset>_D<densidad>_R<riesgo>_P<pureza>_train.csv\n",
    "        partes = archivo.replace(\".csv\", \"\").split(\"_\")\n",
    "        if len(partes) < 5:\n",
    "            print(f\"⚠️ Nombre de archivo inválido o incompleto (aumentado): {archivo}\")\n",
    "            continue\n",
    "        tecnica = partes[0]\n",
    "        nombre_dataset = partes[1]\n",
    "        densidad = partes[2][1:]  # quita 'D'\n",
    "        riesgo   = partes[3][1:]  # quita 'R'\n",
    "        pureza   = partes[4][1:]  # quita 'P'\n",
    "    else:\n",
    "        # Base: <dataset>_train.csv\n",
    "        tecnica = \"base\"\n",
    "        nombre_dataset = archivo.replace(\"_train.csv\", \"\")\n",
    "        densidad = \"NA\"\n",
    "        riesgo   = \"NA\"\n",
    "        pureza   = \"NA\"\n",
    "\n",
    "    print(f\"\\n📂 Dataset: {archivo}, tipo: {tipo}\")\n",
    "    print(f\"🔎 Técnica: {tecnica} | Dataset: {nombre_dataset} | Densidad: {densidad} | Riesgo: {riesgo} | Pureza: {pureza}\")\n",
    "\n",
    "    # Carga y split X/y\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_archivo)\n",
    "        if \"target\" in df.columns:\n",
    "            X = df.drop(columns=[\"target\"]).values\n",
    "            y = df[\"target\"].values\n",
    "        else:\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y = df.iloc[:, -1].values\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al leer {archivo}: {e}\")\n",
    "        continue\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Búsqueda aleatoria + CV por modelo\n",
    "    for nombre_modelo, info in modelos.items():\n",
    "        print(f\"⚙️ Validando modelo con Pipeline y RandomizedSearchCV: {nombre_modelo}\")\n",
    "        try:\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=info['pipeline'],\n",
    "                param_distributions=info['param_distributions'],\n",
    "                n_iter=10,\n",
    "                cv=cv,\n",
    "                scoring=scoring,\n",
    "                refit='f1_macro',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            search.fit(X, y)\n",
    "\n",
    "            resultados_por_modelo[nombre_modelo].append({\n",
    "                'dataset': nombre_dataset,\n",
    "                'tecnica': tecnica,\n",
    "                'densidad': densidad,\n",
    "                'riesgo': riesgo,\n",
    "                'pureza': pureza,\n",
    "                'modelo': nombre_modelo,\n",
    "                'f1_score_macro': search.cv_results_['mean_test_f1_macro'][search.best_index_],\n",
    "                'balanced_accuracy': search.cv_results_['mean_test_balanced_accuracy'][search.best_index_],\n",
    "                'mcc': search.cv_results_['mean_test_mcc'][search.best_index_],\n",
    "                'cohen_kappa': search.cv_results_['mean_test_cohen_kappa'][search.best_index_],\n",
    "                'mejor_configuracion': str(search.best_params_)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al validar {nombre_modelo} en {archivo}: {e}\")\n",
    "\n",
    "# --- Persistencia de resultados (un archivo por modelo con base + aumentados) ---\n",
    "os.makedirs(\"../resultados\", exist_ok=True)\n",
    "for nombre_modelo, lista_resultados in resultados_por_modelo.items():\n",
    "    df_final = pd.DataFrame(lista_resultados)\n",
    "    output_path = f\"../resultados/resultados_{nombre_modelo}.csv\"\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"📁 Resultados guardados: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb30cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_otra_tecnica_grid(nombre_dataset, config, tecnica, modelo_clasificador, nombre_modelo, nombre_tec):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    if y.dtype == object or isinstance(y[0], str):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    if config.get(\"tipo\") == \"imagen\":\n",
    "        X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "    clases_minor = config.get(\"clases_minor\", [])\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metricas_fold = []\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"🔁 Fold {fold_idx + 1}/5\")\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        pca = PCA(n_components=min(X_train.shape[1], 100))\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "        try:\n",
    "            if tecnica:\n",
    "                X_res, y_res = tecnica.fit_resample(X_train_pca, y_train)\n",
    "            else:\n",
    "                X_res, y_res = X_train_pca, y_train\n",
    "\n",
    "            modelo_escalado = get_modelo_escalado_si_es_necesario(modelo_clasificador, nombre_modelo)\n",
    "            modelo_escalado.fit(X_res, y_res)\n",
    "            y_pred = modelo_escalado.predict(X_test_pca)\n",
    "\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "            f1_minor = None\n",
    "            if clases_minor:\n",
    "                mask = np.isin(y_test, clases_minor)\n",
    "                if np.any(mask):\n",
    "                    f1_minor = f1_score(y_test[mask], y_pred[mask], average='macro')\n",
    "\n",
    "            metricas_fold.append({\n",
    "                'f1_score_macro': f1_macro,\n",
    "                'f1_score_minor': f1_minor,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'mcc': mcc,\n",
    "                'cohen_kappa': kappa\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en fold {fold_idx + 1}: {e}\")\n",
    "            metricas_fold.append({\n",
    "                'f1_score_macro': None,\n",
    "                'f1_score_minor': None,\n",
    "                'balanced_accuracy': None,\n",
    "                'mcc': None,\n",
    "                'cohen_kappa': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    df_metricas = pd.DataFrame(metricas_fold)\n",
    "    df_mean = df_metricas.dropna().mean(numeric_only=True).to_dict()\n",
    "    df_mean.update({\n",
    "        'dataset': nombre_dataset,\n",
    "        'modelo': nombre_modelo,\n",
    "        'tecnica': nombre_tec\n",
    "    })\n",
    "\n",
    "    df_final = pd.DataFrame([df_mean])\n",
    "    df_final.to_csv(f\"../resultados/{nombre_tec}_grid_{nombre_dataset}_{nombre_modelo}.csv\", index=False)\n",
    "    print(f\"📁 Resultados guardados en: ../resultados/{nombre_tec}_grid_{nombre_dataset}_{nombre_modelo}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluar_pcsmote_grid_search(nombre_dataset, config, percentiles_densidad, percentiles_riesgo, criterios_pureza, modelo_clasificador, nombre_modelo):\n",
    "    print(f\"📂 Cargando dataset: {nombre_dataset}\")\n",
    "\n",
    "    # 1) Cargar dataset según los parámetros recibidos (ruta, columnas, tipo, etc.)\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Codificar etiquetas si son strings u objetos\n",
    "    if y.dtype == object or isinstance(y[0], str):  \n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Si es un dataset de imágenes, aplastar (reshape) cada imagen a vector 1D\n",
    "    # Esto convierte (N, H, W, C) en (N, H*W*C) para trabajar como tabular\n",
    "    if config.get(\"tipo\") == \"imagen\":\n",
    "        X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "    # 4) Obtener clases minoritarias si están definidas\n",
    "    clases_minor = config.get(\"clases_minor\", [])\n",
    "    resultados = []\n",
    "\n",
    "    # 5) Dividir en train/test con estratificación, antes del sobremuestreo\n",
    "    X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 6) Generar todas las combinaciones posibles entre los parámetros definidos\n",
    "    combinaciones = list(product(percentiles_densidad, percentiles_riesgo, criterios_pureza))\n",
    "\n",
    "    # 7) Iterar sobre cada combinación y aplicar sobremuestreo + entrenamiento + evaluación\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        inicio = datetime.now()\n",
    "        print(f\"#{idx:02d} 🧪 {nombre_modelo} | {nombre_dataset} | Densidad: {pdens} | Riesgo: {priesgo} | Pureza: {criterio}\")\n",
    "        print(f\"⏱️  Tiempo de inicio: {inicio.strftime('%H:%M:%S')} hs\")        \n",
    "\n",
    "        try:\n",
    "            # 7.a) Inicializar sampler PCSMOTE con los parámetros actuales\n",
    "            sampler = PCSMOTE(\n",
    "                random_state=42,\n",
    "                percentil_densidad=pdens,\n",
    "                percentil_dist=priesgo,\n",
    "                criterio_pureza=criterio,\n",
    "                modo_espacial='3d'  # Puede usarse '2d' si se desea modificar\n",
    "            )\n",
    "\n",
    "            # 7.b) Aplicar sobremuestreo sobre el set de entrenamiento (solo si método está definido)\n",
    "            if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "                X_res, y_res = sampler.fit_resample_multiclass(X_train_eval, y_train_eval)\n",
    "            else:\n",
    "                X_res, y_res = sampler.fit_resample(X_train_eval, y_train_eval)\n",
    "\n",
    "            # 7.c) Entrenar modelo sobre los datos aumentados\n",
    "            modelo_clasificador.fit(X_res, y_res)\n",
    "\n",
    "            # 7.d) Predecir sobre el set de evaluación (test)\n",
    "            y_pred = modelo_clasificador.predict(X_test_eval)\n",
    "\n",
    "            # 7.e) Calcular métricas de rendimiento\n",
    "            f1_macro = f1_score(y_test_eval, y_pred, average='macro')\n",
    "            balanced_acc = balanced_accuracy_score(y_test_eval, y_pred)\n",
    "            mcc = matthews_corrcoef(y_test_eval, y_pred)\n",
    "            kappa = cohen_kappa_score(y_test_eval, y_pred)\n",
    "\n",
    "            # 7.f) Si hay clases minoritarias definidas, calcular también f1_score sobre ellas\n",
    "            f1_minor = None\n",
    "            if clases_minor:\n",
    "                mask = np.isin(y_test_eval, clases_minor)\n",
    "                if np.any(mask):\n",
    "                    f1_minor = f1_score(y_test_eval[mask], y_pred[mask], average='macro')\n",
    "                    print(f\"📊 f1_minor sobre {np.sum(mask)} muestras minoritarias.\")\n",
    "                    print(\"✔️ Verdaderas:\", y_test_eval[mask])\n",
    "                    print(\"❌ Predichas:\", y_pred[mask])\n",
    "\n",
    "            # 7.g) Mostrar resumen de rendimiento\n",
    "            print(f\"✅ Config OK | F1_macro: {f1_macro:.4f}\" + (f\", F1_minor: {f1_minor:.4f}\" if f1_minor else \"\"))\n",
    "\n",
    "            # 7.h) Guardar resultados de esta corrida\n",
    "            resultados.append({\n",
    "                'dataset': nombre_dataset,\n",
    "                'modelo': nombre_modelo,\n",
    "                'densidad': pdens,\n",
    "                'riesgo': priesgo,\n",
    "                'pureza': criterio,\n",
    "                'f1_score_macro': f1_macro,\n",
    "                'f1_score_minor': f1_minor,\n",
    "                'balanced_accuracy': balanced_acc,\n",
    "                'mcc': mcc,\n",
    "                'cohen_kappa': kappa\n",
    "            })\n",
    "            \n",
    "            # 7.i) Mostrar tiempo de ejecución\n",
    "            fin = datetime.now()\n",
    "            transcurrido = fin - inicio\n",
    "            print(f\"✅ Tiempo final: {fin.strftime('%H:%M:%S')} hs\")\n",
    "            print(f\"🕒 Total transcurrido: {str(transcurrido).rjust(8, '0')} hs\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # 8) En caso de error, registrar el fallo con detalles\n",
    "            print(f\"⚠️ Error con config D={pdens} R={priesgo} P={criterio}: {e}\")\n",
    "            resultados.append({\n",
    "                'dataset': nombre_dataset,\n",
    "                'modelo': nombre_modelo,\n",
    "                'densidad': pdens,\n",
    "                'riesgo': priesgo,\n",
    "                'pureza': criterio,\n",
    "                'f1_score_macro': None,\n",
    "                'f1_score_minor': None,\n",
    "                'balanced_accuracy': None,\n",
    "                'mcc': None,\n",
    "                'cohen_kappa': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # 9) Convertir resultados a DataFrame y exportar a CSV\n",
    "    df = pd.DataFrame(resultados)\n",
    "    df.to_csv(f\"../resultados/pcsmote_grid_{nombre_dataset}_{nombre_modelo}.csv\", index=False)\n",
    "    print(f\"📁 Resultados guardados en: ../resultados/pcsmote_grid_{nombre_dataset}_{nombre_modelo}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from config_datasets import config_datasets\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "tecnicas_sobremuestreo = {\n",
    "    # \"base\": None,\n",
    "    \"pcsmote\": PCSMOTE(\n",
    "        random_state=42,\n",
    "        percentil_densidad=50,  # serán reemplazados dinámicamente en el grid\n",
    "        percentil_dist=50,\n",
    "        criterio_pureza='entropia',\n",
    "        modo_espacial='3d',\n",
    "        verbose=False\n",
    "    ),\n",
    "    # \"smote\": SMOTE(random_state=42),\n",
    "    # \"adasyn\": ADASYN(random_state=42),\n",
    "    # \"borderline\": BorderlineSMOTE(random_state=42)\n",
    "}\n",
    "\n",
    "modelos = {\n",
    "    # \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    # \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "    # \"XGBoost\": XGBClassifier(...)  // esta fallando por la etiquetas\n",
    "}\n",
    "\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    if nombre_dataset == \"shuttle\":\n",
    "        for nombre_modelo, modelo in modelos.items():\n",
    "            for nombre_tec, tecnica in tecnicas_sobremuestreo.items():\n",
    "                print(f\"\\n=== Ejecutando grid para {nombre_dataset} | modelo: {nombre_modelo} | técnica: {nombre_tec} ===\")\n",
    "\n",
    "                if nombre_tec == \"pcsmote\":\n",
    "\n",
    "\n",
    "                    evaluar_pcsmote_grid_search(\n",
    "                        nombre_dataset, config,\n",
    "                        percentiles_densidad, percentiles_riesgo,\n",
    "                        criterios_pureza, modelo, nombre_modelo\n",
    "                    )\n",
    "                # else:\n",
    "                #     evaluar_otra_tecnica_grid(\n",
    "                #         nombre_dataset, config, tecnica,\n",
    "                #         modelo, nombre_modelo, nombre_tec\n",
    "                #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ca6ff",
   "metadata": {},
   "source": [
    "## Busqueda del mejor resultado para cada tecnica de sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "datasets = ['shuttle']\n",
    "tecnicas = ['base', 'smote', 'adasyn', 'borderline', 'pcsmote']\n",
    "modelos = ['RandomForest', 'LogisticRegression', 'SVM']\n",
    "\n",
    "resumen = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for tecnica in tecnicas:\n",
    "        archivos = glob.glob(f\"../resultados/{tecnica}_grid_{dataset}_*.csv\")\n",
    "        if not archivos:\n",
    "            print(f\"⚠️ No hay resultados para {dataset} con técnica {tecnica}\")\n",
    "            continue\n",
    "\n",
    "        mejores_filas = []\n",
    "\n",
    "        for archivo in archivos:\n",
    "            try:\n",
    "                df = pd.read_csv(archivo)\n",
    "                df_valid = df.dropna(subset=[\"f1_score\"])\n",
    "                if df_valid.empty:\n",
    "                    continue\n",
    "\n",
    "                fila = df_valid.sort_values(by=\"f1_score\", ascending=False).iloc[0].copy()\n",
    "                fila['dataset'] = dataset\n",
    "                fila['tecnica'] = tecnica\n",
    "\n",
    "                nombre_archivo = os.path.basename(archivo)\n",
    "                modelo_detectado = next((m for m in modelos if m in nombre_archivo), \"desconocido\")\n",
    "                fila['modelo'] = modelo_detectado\n",
    "\n",
    "                mejores_filas.append(fila)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error procesando {archivo}: {e}\")\n",
    "\n",
    "\n",
    "        # Guardar el mejor resultado por técnica\n",
    "        if mejores_filas:\n",
    "            mejor = pd.DataFrame(mejores_filas).sort_values(by=\"f1_score\", ascending=False).iloc[0]\n",
    "            resumen.append(mejor)\n",
    "\n",
    "# Crear DataFrame final\n",
    "resumen_df = pd.DataFrame(resumen)\n",
    "\n",
    "# Reordenar columnas según disponibilidad\n",
    "cols = ['dataset', 'tecnica', 'modelo', 'f1_score']\n",
    "for col in ['balanced_accuracy', 'mcc', 'cohen_kappa', 'densidad', 'riesgo', 'pureza','error']:\n",
    "    if col in resumen_df.columns:\n",
    "        cols.append(col)\n",
    "\n",
    "resumen_df = resumen_df[cols]\n",
    "resumen_df.to_csv(\"../resultados/resumen_mejores_por_tecnica.csv\", index=False)\n",
    "print(\"✅ Resumen guardado en ../resultados/resumen_mejores_por_tecnica.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e351ce",
   "metadata": {},
   "source": [
    "## Generacion de graficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17140685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar el resumen\n",
    "df = pd.read_csv(\"../resultados/resumen_mejores_por_tecnica.csv\")\n",
    "\n",
    "# Ajustar orden de técnicas para mejor visualización\n",
    "orden_tecnicas = ['base', 'smote', 'adasyn', 'borderline', 'pcsmote']\n",
    "df['tecnica'] = pd.Categorical(df['tecnica'], categories=orden_tecnicas, ordered=True)\n",
    "\n",
    "# Plot por dataset\n",
    "for dataset in df['dataset'].unique():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df_sub = df[df['dataset'] == dataset].sort_values(\"tecnica\")\n",
    "\n",
    "    sns.barplot(\n",
    "        data=df_sub,\n",
    "        x=\"tecnica\",\n",
    "        y=\"f1_score\",\n",
    "        hue=\"modelo\",  # opcional: para ver qué modelo dio ese resultado\n",
    "        dodge=False,\n",
    "        palette=\"viridis\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"F1-score por técnica de sobremuestreo - {dataset}\")\n",
    "    plt.ylabel(\"F1-score\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlabel(\"Técnica\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
