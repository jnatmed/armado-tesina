{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la t√©cnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from conexion import DatabaseConnection  # tu clase\n",
    "db = DatabaseConnection()\n",
    "db.connect()\n",
    "from GestorExperimentosDB import GestorExperimentosDB\n",
    "\n",
    "gestor_db = GestorExperimentosDB(db) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- funci√≥n: generar caso base (train/test sin sobremuestreo) ---\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin PCSMOTE) para un dataset: guarda train y test en ruta_base.\n",
    "    Usa la misma l√≥gica de carga que el resto del pipeline (cargar_dataset + LabelEncoder opcional).\n",
    "\n",
    "    Retorna:\n",
    "        (path_train, path_test)\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test  # ya generado\n",
    "\n",
    "    # 1) Cargar dataset base con tu helper habitual\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar etiquetas num√©ricas si vienen como strings/objects\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado (mismo seed para reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Guardar CSVs\n",
    "    pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_train, index=False)\n",
    "    pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_test, index=False)\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7ff02",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de Par√°metros para PC-SMOTE\n",
    "\n",
    "En esta secci√≥n, definimos los par√°metros clave para la t√©cnica PC-SMOTE:\n",
    "\n",
    "- **percentiles_densidad**: Lista de percentiles (25, 50, 75) para controlar la densidad de vecindarios en el espacio de caracter√≠sticas.\n",
    "- **percentiles_riesgo**: Lista de percentiles (25, 50, 75) para determinar el umbral de riesgo para la generaci√≥n de muestras sint√©ticas.\n",
    "- **criterios_pureza**: Diferentes criterios para evaluar la pureza de los cl√∫steres durante el sobremuestreo.\n",
    "\n",
    "Estos par√°metros se utilizar√°n en una b√∫squeda en cuadr√≠cula (grid search) para encontrar la mejor combinaci√≥n que optimice el rendimiento del modelo en el dataset Shuttle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo   = [25, 50, 75]\n",
    "criterios_pureza     = [\"entropia\", \"proporcion\"]\n",
    "\n",
    "# Modelo \"placeholder\" cuando no entren√°s nada ac√° (solo gener√°s datasets)\n",
    "# Si despu√©s entren√°s LR/SVM/RF en otra notebook, ah√≠ usar√°s su modelo_id real.\n",
    "from time import time\n",
    "\n",
    "# un \"modelo\" placeholder para esta notebook donde solo gener√°s datasets\n",
    "modelo_id = gestor_db.get_or_create_modelo_id(\"PCSMOTE_only\")\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    for pdens in percentiles_densidad:\n",
    "        for priesgo in percentiles_riesgo:\n",
    "            for criterio in criterios_pureza:\n",
    "                print(f\"‚û°Ô∏è  D={pdens} | R={priesgo} | P={criterio}\")\n",
    "\n",
    "                t0 = time()\n",
    "                experimento_id, experimento_nombre, sampler = gestor_db.aumentar_dataset_pcsmote_y_guardar(\n",
    "                    nombre_dataset=nombre_dataset,\n",
    "                    config=config,\n",
    "                    percentil_densidad=pdens,\n",
    "                    percentil_riesgo=priesgo,\n",
    "                    criterio_pureza=criterio\n",
    "                )\n",
    "                elapsed = round(time() - t0, 3)\n",
    "\n",
    "                # Actualiza el mensaje de √©xito\n",
    "                if experimento_id and sampler:\n",
    "                    print(f\"‚úî Guardado en DB. experimento_id={experimento_id} (elapsed={elapsed}s)\")\n",
    "\n",
    "                if not (experimento_id and sampler):\n",
    "                    print(\"‚ùå Fall√≥ la generaci√≥n.\")\n",
    "                    continue\n",
    "\n",
    "                # 1) dataset_id (si no quer√©s leer tama√±os ac√°, dej√° NULLs)\n",
    "                n_train = None\n",
    "                n_test  = None\n",
    "                n_feat  = None\n",
    "                dataset_id = gestor_db.get_or_create_dataset_id(nombre_dataset, n_train, n_test, n_feat, es_grande=0)\n",
    "\n",
    "                # 2) config_id desde el sampler\n",
    "                cfg = PCSMOTE.to_config_dict(sampler)\n",
    "                config_id = gestor_db.get_or_create_config_id(**cfg)\n",
    "\n",
    "                # 3) experimento + logs (usa sampler.guardar_en_db que ya agregaste)\n",
    "                mejor_config = {\n",
    "                    \"pcsmote\": {\n",
    "                        \"k\": sampler.k,\n",
    "                        \"random_state\": sampler._loggable_random_state(),\n",
    "                        \"percentil_densidad\": sampler.percentil_densidad,\n",
    "                        \"percentil_riesgo\": sampler.percentil_dist,\n",
    "                        \"criterio_pureza\": sampler.criterio_pureza,\n",
    "                        \"percentil_entropia\": sampler.percentil_entropia,\n",
    "                        \"modo_espacial\": sampler.modo_espacial,\n",
    "                        \"factor_equilibrio\": sampler.factor_equilibrio\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                experimento_id = sampler.guardar_en_db(\n",
    "                    db,\n",
    "                    dataset_id=dataset_id,\n",
    "                    config_id=config_id,\n",
    "                    modelo_id=modelo_id,\n",
    "                    cv_splits=None, n_iter=None, n_jobs_search=None,\n",
    "                    search_time_sec=elapsed,\n",
    "                    mejor_configuracion=mejor_config,\n",
    "                    source_file=None,\n",
    "                    metricas=None,\n",
    "                    guardar_logs=True,          # ‚úÖ ahora guarda en `log_pcsmote`\n",
    "                    tabla_logs=\"log_pcsmote\"\n",
    "                )\n",
    "\n",
    "                print(f\"‚úî Guardado en DB. experimento_id={experimento_id} (elapsed={elapsed}s)\")\n",
    "\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
