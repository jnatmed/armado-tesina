{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluación de PC-SMOTE con Grid Search en el dataset Shuttle (Generación de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de búsqueda de módulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar módulos o paquetes cuando hacés un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importación de módulos y librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Módulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Función para cargar datasets según configuración\n",
    "from config_datasets import config_datasets                    # Diccionario de configuración de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluación de sobremuestreo con partición hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la técnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementación principal de PCSMOTE\n",
    "\n",
    "# --- Librerías estándar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generación de combinaciones de parámetros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "\n",
    "# --- Librerías científicas ---\n",
    "import numpy as np                                              # Operaciones numéricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulación y análisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para búsqueda de hiperparámetros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificación de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creación de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: división y validación ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # División de datos en train/test\n",
    "    StratifiedKFold,                                            # Validación cruzada estratificada\n",
    "    RandomizedSearchCV                                          # Búsqueda aleatoria de hiperparámetros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducción de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # Análisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: métricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # Métrica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisión balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresión logística\n",
    "from sklearn.svm import SVC                                      # Máquinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "from conexion import DatabaseConnection  # tu clase\n",
    "db = DatabaseConnection()\n",
    "db.connect()\n",
    "from GestorExperimentosDB import GestorExperimentosDB\n",
    "\n",
    "gestor_db = GestorExperimentosDB(db) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generación del caso base\n",
    "\n",
    "Este código realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio específico para almacenar la versión original del dataset sin ningún tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma función `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) están en formato de texto u objeto, se convierten a valores numéricos con `LabelEncoder`.\n",
    "   - Se realiza una división estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporción 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- función: generar caso base (train/test sin sobremuestreo) ---\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin PCSMOTE) para un dataset: guarda train y test en ruta_base.\n",
    "    Usa la misma lógica de carga que el resto del pipeline (cargar_dataset + LabelEncoder opcional).\n",
    "\n",
    "    Retorna:\n",
    "        (path_train, path_test)\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test  # ya generado\n",
    "\n",
    "    # 1) Cargar dataset base con tu helper habitual\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar etiquetas numéricas si vienen como strings/objects\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado (mismo seed para reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Guardar CSVs\n",
    "    pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_train, index=False)\n",
    "    pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_test, index=False)\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7ff02",
   "metadata": {},
   "source": [
    "## Configuración de Parámetros para PC-SMOTE\n",
    "\n",
    "En esta sección, definimos los parámetros clave para la técnica PC-SMOTE:\n",
    "\n",
    "- **percentiles_densidad**: Lista de percentiles (25, 50, 75) para controlar la densidad de vecindarios en el espacio de características.\n",
    "- **percentiles_riesgo**: Lista de percentiles (25, 50, 75) para determinar el umbral de riesgo para la generación de muestras sintéticas.\n",
    "- **criterios_pureza**: Diferentes criterios para evaluar la pureza de los clústeres durante el sobremuestreo.\n",
    "\n",
    "Estos parámetros se utilizarán en una búsqueda en cuadrícula (grid search) para encontrar la mejor combinación que optimice el rendimiento del modelo en el dataset Shuttle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### 🧬 Aumento de Datasets mediante Técnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versión balanceada de cada dataset original mediante la aplicación de técnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la técnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generación de muestras sintéticas en función de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones específicas de parámetros según la técnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuración empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ⚠️ Esta fase no incluye entrenamiento ni validación de modelos. Su único propósito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partición `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluación imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo   = [25, 50, 75]\n",
    "criterios_pureza     = [\"entropia\", \"proporcion\"]\n",
    "\n",
    "# Modelo \"placeholder\" cuando no entrenás nada acá (solo generás datasets)\n",
    "# Si después entrenás LR/SVM/RF en otra notebook, ahí usarás su modelo_id real.\n",
    "from time import time\n",
    "\n",
    "# un \"modelo\" placeholder para esta notebook donde solo generás datasets\n",
    "modelo_id = gestor_db.get_or_create_modelo_id(\"PCSMOTE_only\")\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset == \"eurosat\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📁 Dataset: {nombre_dataset}\")\n",
    "\n",
    "    for pdens in percentiles_densidad:\n",
    "        for priesgo in percentiles_riesgo:\n",
    "            for criterio in criterios_pureza:\n",
    "                print(f\"➡️  D={pdens} | R={priesgo} | P={criterio}\")\n",
    "\n",
    "                t0 = time()\n",
    "                experimento_id, experimento_nombre, sampler = gestor_db.aumentar_dataset_pcsmote_y_guardar(\n",
    "                    nombre_dataset=nombre_dataset,\n",
    "                    config=config,\n",
    "                    percentil_densidad=pdens,\n",
    "                    percentil_riesgo=priesgo,\n",
    "                    criterio_pureza=criterio\n",
    "                )\n",
    "                elapsed = round(time() - t0, 3)\n",
    "\n",
    "                # Actualiza el mensaje de éxito\n",
    "                if experimento_id and sampler:\n",
    "                    print(f\"✔ Guardado en DB. experimento_id={experimento_id} (elapsed={elapsed}s)\")\n",
    "\n",
    "                if not (experimento_id and sampler):\n",
    "                    print(\"❌ Falló la generación.\")\n",
    "                    continue\n",
    "\n",
    "                # 1) dataset_id (si no querés leer tamaños acá, dejá NULLs)\n",
    "                n_train = None\n",
    "                n_test  = None\n",
    "                n_feat  = None\n",
    "                dataset_id = gestor_db.get_or_create_dataset_id(nombre_dataset, n_train, n_test, n_feat, es_grande=0)\n",
    "\n",
    "                # 2) config_id desde el sampler\n",
    "                cfg = PCSMOTE.to_config_dict(sampler)\n",
    "                config_id = gestor_db.get_or_create_config_id(**cfg)\n",
    "\n",
    "                # 3) experimento + logs (usa sampler.guardar_en_db que ya agregaste)\n",
    "                mejor_config = {\n",
    "                    \"pcsmote\": {\n",
    "                        \"k\": sampler.k,\n",
    "                        \"random_state\": sampler._loggable_random_state(),\n",
    "                        \"percentil_densidad\": sampler.percentil_densidad,\n",
    "                        \"percentil_riesgo\": sampler.percentil_dist,\n",
    "                        \"criterio_pureza\": sampler.criterio_pureza,\n",
    "                        \"percentil_entropia\": sampler.percentil_entropia,\n",
    "                        \"modo_espacial\": sampler.modo_espacial,\n",
    "                        \"factor_equilibrio\": sampler.factor_equilibrio\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                experimento_id = sampler.guardar_en_db(\n",
    "                    db,\n",
    "                    dataset_id=dataset_id,\n",
    "                    config_id=config_id,\n",
    "                    modelo_id=modelo_id,\n",
    "                    cv_splits=None, n_iter=None, n_jobs_search=None,\n",
    "                    search_time_sec=elapsed,\n",
    "                    mejor_configuracion=mejor_config,\n",
    "                    source_file=None,\n",
    "                    metricas=None,\n",
    "                    guardar_logs=True,          # ✅ ahora guarda en `log_pcsmote`\n",
    "                    tabla_logs=\"log_pcsmote\"\n",
    "                )\n",
    "\n",
    "                print(f\"✔ Guardado en DB. experimento_id={experimento_id} (elapsed={elapsed}s)\")\n",
    "\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
