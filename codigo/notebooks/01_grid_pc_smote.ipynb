{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la t√©cnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "from graficador2d import Graficador2D                       # Clase para graficar resultados en 2D\n",
    "from isolation_cleaner import IsolationCleaner                 # Clase para limpieza de outliers con Isolation Forest\n",
    "from Utils import Utils                                       # Clase utilitaria con funciones auxiliares\n",
    "from limpiador import LimpiadorOutliers                                 # Clase para limpieza de datos\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "from pathlib import Path\n",
    "\n",
    "import traceback\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "RUTA_CLASICOS: str = \"../datasets/datasets_aumentados/resampler_clasicos/\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin sobremuestreo) aplicando RobustScaler\n",
    "    y guardando train/test consistentes con el pipeline PCSMOTE.\n",
    "\n",
    "    CORREGIDO:\n",
    "      - El escalado se ajusta SOLO con X_train.\n",
    "      - Luego se aplica el mismo scaler a X_test (sin fuga de informaci√≥n).\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test\n",
    "\n",
    "    # 1) Cargar dataset original\n",
    "    names = config.get(\"esquema\") if config.get(\"header\", None) is None else None\n",
    "\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config.get(\"path\"),\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\"),\n",
    "        impute=\"median\",\n",
    "        names=names\n",
    "    )\n",
    "\n",
    "    # Asegurar DataFrame para tener nombres de columnas coherentes\n",
    "    col_features = config.get(\"col_features\")\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X, columns=col_features)\n",
    "\n",
    "    # 2) Codificar etiquetas si es necesario\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado sobre X crudo (sin escalar todav√≠a)\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Ajustar RobustScaler SOLO con X_train y transformar ambos\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "    X_test_scaled  = scaler.transform(X_test_raw)\n",
    "\n",
    "    # 5) Guardar CSVs consistentes con los aumentados\n",
    "    df_train = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_train_scaled, columns=col_features),\n",
    "            pd.Series(y_train, name=config.get(\"col_target\", \"target\")),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_test = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_test_scaled, columns=col_features),\n",
    "            pd.Series(y_test, name=config.get(\"col_target\", \"target\")),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_train.to_csv(path_train, index=False)\n",
    "    df_test.to_csv(path_test, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Caso base generado con escalado robusto para {nombre_dataset}\")\n",
    "    print(f\"   Train: {path_train}\")\n",
    "    print(f\"   Test : {path_test}\")\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(\n",
    "    nombre_dataset,\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    percentil_densidad,\n",
    "    percentil_riesgo,\n",
    "    criterio_pureza,\n",
    "    porcentaje_limpieza,\n",
    "    col_target=\"target\",\n",
    "):\n",
    "    try:\n",
    "        # 1) Copias defensivas\n",
    "        X = np.asarray(X_train_base)\n",
    "        y = np.asarray(y_train_base).copy()\n",
    "        n_total_original = len(y)\n",
    "\n",
    "        # 2) Limpieza con IsolationCleaner\n",
    "        if porcentaje_limpieza <= 0:\n",
    "            X_limpio, y_limpio = X, y\n",
    "            info_limpieza = {\n",
    "                \"percentil_umbral\": 0.0,\n",
    "                \"removed_total\": 0,\n",
    "                \"total\": n_total_original,\n",
    "            }\n",
    "        else:\n",
    "            X_limpio, y_limpio, info_raw = IsolationCleaner.limpiarOutliers(\n",
    "                X=X,\n",
    "                y=y,\n",
    "                percentil_umbral=porcentaje_limpieza,\n",
    "                devolver_info=True,\n",
    "                verbose=False,\n",
    "            )\n",
    "            info_limpieza = {\n",
    "                \"percentil_umbral\": info_raw[\"percentil_umbral\"],\n",
    "                \"removed_total\": info_raw[\"removed_total\"],\n",
    "                \"total\": n_total_original,\n",
    "            }\n",
    "\n",
    "        print(\n",
    "            f\"[{nombre_dataset}] I{porcentaje_limpieza}: \"\n",
    "            f\"removidos={info_limpieza['removed_total']} / {info_limpieza['total']} \"\n",
    "            f\"(p={info_limpieza['percentil_umbral']}%)\"\n",
    "        )\n",
    "\n",
    "        # 3) PCSMOTE sobre el TRAIN limpio (ya escalado)\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_entropia=75 if criterio_pureza == \"entropia\" else None,\n",
    "            percentil_riesgo=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            factor_equilibrio=1,\n",
    "            metric=\"euclidean\",\n",
    "        )\n",
    "        sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_res, y_res = sampler.fit_resample_multiclass(X_limpio, y_limpio)\n",
    "        else:\n",
    "            X_res, y_res = sampler.fit_resample(X_limpio, y_limpio)\n",
    "\n",
    "        # 4) Guardar TRAIN aumentado\n",
    "        out_dir = Path(\"../datasets/datasets_aumentados/\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fname_train = (\n",
    "            f\"pcsmote_{nombre_dataset}_\"\n",
    "            f\"D{percentil_densidad}_R{percentil_riesgo}_\"\n",
    "            f\"{Utils.tag_p(criterio_pureza)}_I{porcentaje_limpieza}_train.csv\"\n",
    "        )\n",
    "        fname_train = Utils.safe_token(fname_train)\n",
    "        p_train = out_dir / fname_train\n",
    "\n",
    "        df_train = pd.DataFrame(X_res)\n",
    "        df_train[col_target] = y_res\n",
    "\n",
    "        df_train.to_csv(p_train, index=False)\n",
    "\n",
    "        return str(p_train), sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"‚ùå Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_aumentaciones_clasicas_y_guardar(\n",
    "    nombre_dataset: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    col_target: str,\n",
    "    ruta_clasicos: str = \"../datasets/datasets_aumentados/resampler_clasicos/\",\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera datasets aumentados con t√©cnicas cl√°sicas a partir de X_train, y_train:\n",
    "      - SMOTE\n",
    "      - BorderlineSMOTE\n",
    "      - ADASYN\n",
    "\n",
    "    Guarda los resultados en CSV en ruta_clasicos/ con nombres:\n",
    "      smote_{dataset}_train.csv\n",
    "      borderlinesmote_{dataset}_train.csv\n",
    "      adasyn_{dataset}_train.csv\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_clasicos, exist_ok=True)\n",
    "\n",
    "    columnas = list(X_train.columns)\n",
    "\n",
    "    print(f\"üîß Aumentaci√≥n cl√°sica (en memoria) para: {nombre_dataset}\")\n",
    "    print(f\"   X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "    resamplers = [\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"borderlinesmote\", BorderlineSMOTE(random_state=RANDOM_STATE, kind=\"borderline-1\")),\n",
    "        (\"adasyn\", ADASYN(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    "\n",
    "    for nombre_resampler, resampler in resamplers:\n",
    "        nombre_archivo = f\"{nombre_resampler}_{nombre_dataset}_train.csv\"\n",
    "        path_salida = os.path.join(ruta_clasicos, nombre_archivo)\n",
    "\n",
    "        if not overwrite and os.path.exists(path_salida):\n",
    "            print(f\"   ‚ö™ Omitido ({nombre_resampler}), ya existe: {nombre_archivo}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   üîÅ Aplicando {nombre_resampler} ...\")\n",
    "\n",
    "        try:\n",
    "            # fit_resample trabaja con ndarrays\n",
    "            X_res, y_res = resampler.fit_resample(X_train.values, y_train.values)\n",
    "        except ValueError as e:\n",
    "            # Caso t√≠pico: ADASYN (u otro) decide no generar muestras\n",
    "            print(f\"   ‚ö†Ô∏è {nombre_resampler} no gener√≥ muestras sint√©ticas: {e}\")\n",
    "            print(f\"      ‚Üí Se omite guardar {nombre_resampler}_{nombre_dataset}_train.csv para este dataset.\")\n",
    "            continue\n",
    "\n",
    "        df_res = pd.DataFrame(X_res, columns=columnas)\n",
    "        df_res[col_target] = y_res  # y_res ya es array, no .values\n",
    "\n",
    "        df_res.to_csv(path_salida, index=False)\n",
    "        print(f\"   ‚úÖ Guardado: {path_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GRID PCSMOTE ---\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "percentil_isolation_cleaner = [0,1] #[0, 1, 5]\n",
    "\n",
    "combinaciones = list(product(\n",
    "    percentiles_densidad,\n",
    "    percentiles_riesgo,\n",
    "    criterios_pureza,\n",
    "    percentil_isolation_cleaner\n",
    "))\n",
    "\n",
    "# --- Rutas base ---\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "RUTA_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "os.makedirs(RUTA_CLASICOS, exist_ok=True)\n",
    "\n",
    "datasets_a_ignorar = {\n",
    "    \"eurosat\",\n",
    "    # \"shuttle\",\n",
    "    # \"glass\",\n",
    "    # \"heart\",\n",
    "    # \"wdbc\",\n",
    "    # \"ecoli\",\n",
    "    \"iris\",\n",
    "    # \"us_crime\"\n",
    "}\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset in datasets_a_ignorar:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # --- CASO BASE (√∫nica lectura del dataset crudo) ---\n",
    "    base_train, base_test = generar_caso_base(nombre_dataset, config)\n",
    "    print(f\"üü¶ Caso base generado:\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "    # Cargar en memoria el TRAIN base ya escalado\n",
    "    col_target = config.get(\"col_target\", \"target\")\n",
    "    df_base_train = pd.read_csv(base_train)\n",
    "\n",
    "    if col_target not in df_base_train.columns:\n",
    "        raise ValueError(f\"La columna target '{col_target}' no est√° en {base_train}\")\n",
    "\n",
    "    # Versi√≥n DataFrame/Series para los cl√°sicos\n",
    "    X_train_df = df_base_train.drop(columns=[col_target])\n",
    "    y_train_series = df_base_train[col_target]\n",
    "\n",
    "    # Versi√≥n numpy para PCSMOTE (como ya lo usabas)\n",
    "    X_train_base = X_train_df.values\n",
    "    y_train_base = y_train_series.values\n",
    "\n",
    "    # --- AUMENTACI√ìN CL√ÅSICA (SMOTE, BorderlineSMOTE, ADASYN) ---\n",
    "    generar_aumentaciones_clasicas_y_guardar(\n",
    "        nombre_dataset=nombre_dataset,\n",
    "        X_train=X_train_df,\n",
    "        y_train=y_train_series,\n",
    "        col_target=col_target,\n",
    "        ruta_clasicos=RUTA_CLASICOS,\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "    # --- GRID DE PCSMOTE + ISOLATION CLEANER SOBRE ESTE TRAIN BASE ---\n",
    "    for idx, (pdens, priesgo, criterio, plimpieza) in enumerate(combinaciones, start=1):\n",
    "        print(\n",
    "            f\"#{idx:02d} ‚ûï Aumentando con \"\n",
    "            f\"D={pdens} | R={priesgo} | P={criterio} | I={plimpieza}\"\n",
    "        )\n",
    "\n",
    "        path_train, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            X_train_base=X_train_base,\n",
    "            y_train_base=y_train_base,\n",
    "            percentil_densidad=pdens,\n",
    "            percentil_riesgo=priesgo,\n",
    "            criterio_pureza=criterio,\n",
    "            porcentaje_limpieza=plimpieza,\n",
    "            col_target=col_target,\n",
    "        )\n",
    "\n",
    "        if path_train and sampler:\n",
    "            print(f\"‚úÖ Guardado exitoso:\\n - Train: {path_train}\")\n",
    "\n",
    "            # --- rutas de logs (sanitizadas) ---\n",
    "            base_logs = Path(\"../datasets/datasets_aumentados/logs/pcsmote\")\n",
    "            p_tag = Utils.tag_p(criterio)  # \"Pentropia\"/\"Pproporcion\"\n",
    "\n",
    "            fname_clase = Utils.safe_token(\n",
    "                f\"log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_{p_tag}_I{plimpieza}.csv\"\n",
    "            )\n",
    "            fname_muestra = Utils.safe_token(\n",
    "                f\"log_pcsmote_x_muestra_{nombre_dataset}_D{pdens}_R{priesgo}_{p_tag}_I{plimpieza}.xlsx\"\n",
    "            )\n",
    "\n",
    "            log_path_x_clase = str(base_logs / \"por_clase\" / fname_clase)\n",
    "            log_path_x_muestras = str(base_logs / \"por_muestras\" / fname_muestra)\n",
    "\n",
    "            # Asegurar subdirectorios de logs\n",
    "            os.makedirs(base_logs / \"por_clase\", exist_ok=True)\n",
    "            os.makedirs(base_logs / \"por_muestras\", exist_ok=True)\n",
    "\n",
    "            # --- export logs (CSV/Excel) ---\n",
    "            sampler.exportar_log_csv(log_path_x_clase)\n",
    "            sampler.exportar_log_muestras_excel(log_path_x_muestras)\n",
    "\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_clase}\")\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_muestras}\")\n",
    "        else:\n",
    "            print(\"‚ùå Fall√≥ la generaci√≥n.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
