{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "from graficador2d import Graficador2D                       # Clase para graficar resultados en 2D\n",
    "from isolation_cleaner import IsolationCleaner                 # Clase para limpieza de outliers con Isolation Forest\n",
    "from Utils import Utils                                       # Clase utilitaria con funciones auxiliares\n",
    "from limpiador import LimpiadorOutliers                                 # Clase para limpieza de datos\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "from pathlib import Path\n",
    "\n",
    "import traceback\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "RUTA_CLASICOS: str = \"../datasets/datasets_aumentados/resampler_clasicos/\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False,\n",
    "    porcentaje_limpieza: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin sobremuestreo) aplicando:\n",
    "      1) Carga del dataset seg√∫n config_datasets y cargar_dataset()\n",
    "      2) Split train/test\n",
    "      3) RobustScaler (fit SOLO en X_train, transform en X_train/X_test)\n",
    "      4) (Opcional) Limpieza de outliers con IsolationCleaner (IsolationForest por percentil)\n",
    "         SOLO sobre X_train_scaled.\n",
    "\n",
    "    IMPORTANTE:\n",
    "    - Aqu√≠ `porcentaje_limpieza` se usa para LIMPIAR realmente el conjunto\n",
    "      de entrenamiento con IsolationCleaner (si > 0). Internamente se pasa como\n",
    "      `percentil_umbral`.\n",
    "    - El valor de `porcentaje_limpieza` se refleja en el nombre del archivo de TRAIN\n",
    "      como sufijo `_I{porcentaje_limpieza}` para dejar traza.\n",
    "    - El TEST nunca se limpia con IsolationForest, por eso su nombre no lleva sufijo `I*`.\n",
    "\n",
    "    El resultado son dos CSV:\n",
    "      - {ruta_base}/{nombre_dataset}_I{porcentaje_limpieza}_train.csv\n",
    "        (train escalado y, si corresponde, limpiado con IF por percentil)\n",
    "      - {ruta_base}/{nombre_dataset}_test.csv\n",
    "        (test escalado, sin limpieza IF)\n",
    "\n",
    "    Estos archivos se usan como base para:\n",
    "      - el caso base (sin sobremuestreo),\n",
    "      - las t√©cnicas cl√°sicas (SMOTE, BorderlineSMOTE, ADASYN),\n",
    "      - y PC-SMOTE.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(\n",
    "        ruta_base,\n",
    "        f\"{nombre_dataset}_I{porcentaje_limpieza}_train.csv\"\n",
    "    )\n",
    "    path_test = os.path.join(\n",
    "        ruta_base,\n",
    "        f\"{nombre_dataset}_test.csv\"\n",
    "    )\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        print(f\"‚ö†Ô∏è Caso base ya existe para {nombre_dataset}. Usando archivos existentes.\")\n",
    "        return path_train, path_test\n",
    "\n",
    "    # 1) Cargar dataset crudo seg√∫n config_datasets usando cargar_dataset()\n",
    "    df_features, y, clases = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\"),\n",
    "        header=config.get(\"header\"),\n",
    "        binarizar=config.get(\"binarizar\", False),\n",
    "        tipo=config.get(\"tipo\", \"tabular\"),\n",
    "        impute=config.get(\"impute\", \"median\"),\n",
    "        na_values=config.get(\"na_values\", ('?', 'NA', 'None')),\n",
    "        dataset_name=config.get(\"dataset_name\", nombre_dataset),\n",
    "        names=config.get(\"esquema\"),\n",
    "    )\n",
    "\n",
    "    col_target   = config[\"col_target\"]\n",
    "    col_features = config[\"col_features\"]\n",
    "\n",
    "    # X son directamente las features que devuelve cargar_dataset\n",
    "    X = df_features  # ya tiene col_features como columnas\n",
    "    # y es el vector que devolvi√≥ cargar_dataset (np.ndarray)\n",
    "    # train_test_split acepta ndarray, no hace falta convertirlo, salvo para el concat final\n",
    "\n",
    "    # 2) Train / test split (estratificado)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    print(f\"[{nombre_dataset}] Split: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # 3) Escalado robusto (fit en train, transform en train y test)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # 4) (Opcional) Limpieza de outliers con IsolationCleaner SOLO sobre TRAIN\n",
    "    if porcentaje_limpieza is not None and porcentaje_limpieza > 0:\n",
    "        print(f\"[{nombre_dataset}] Aplicando IsolationCleaner (percentil={porcentaje_limpieza}%) sobre TRAIN\")\n",
    "\n",
    "        X_train_scaled, y_train, info_limpieza = IsolationCleaner.limpiarOutliers(\n",
    "            X=X_train_scaled,\n",
    "            y=y_train,  # ya es np.ndarray; tu IsolationCleaner trabaja con np.asarray internamente\n",
    "            percentil_umbral=float(porcentaje_limpieza),\n",
    "            contamination=\"auto\",\n",
    "            n_estimators=200,\n",
    "            max_samples=\"auto\",\n",
    "            random_state=random_state,\n",
    "            bootstrap=False,\n",
    "            normalizar_scores=False,\n",
    "            devolver_info=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        removed_total = info_limpieza.get(\"removed_total\", 0)\n",
    "        total_inicial = removed_total + len(y_train)\n",
    "        print(\n",
    "            f\"[{nombre_dataset}] Limpieza IF (percentil): \"\n",
    "            f\"removidos={removed_total} / total_inicial‚âà{total_inicial}\"\n",
    "        )\n",
    "\n",
    "    # 5) Reconstruir DataFrames con columnas\n",
    "    df_train = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_train_scaled, columns=col_features),\n",
    "            pd.Series(y_train, name=col_target),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_test = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_test_scaled, columns=col_features),\n",
    "            pd.Series(\n",
    "                y_test,\n",
    "                name=col_target,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # 6) Guardar CSV base\n",
    "    df_train.to_csv(path_train, index=False)\n",
    "    df_test.to_csv(path_test, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Caso base generado para {nombre_dataset}\")\n",
    "    print(f\"   Train: {path_train}  (rows={df_train.shape[0]})\")\n",
    "    print(f\"   Test : {path_test}   (rows={df_test.shape[0]})\")\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(\n",
    "    nombre_dataset,\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    percentil_densidad,\n",
    "    percentil_riesgo,\n",
    "    criterio_pureza,\n",
    "    col_target=\"target\",\n",
    "    percentil_isolation_etiqueta:float = 0.0\n",
    "):\n",
    "    try:\n",
    "        # 1) PCSMOTE sobre el TRAIN limpio (ya escalado)\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_entropia=50 if criterio_pureza == \"entropia\" else None,\n",
    "            percentil_riesgo=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            factor_equilibrio=1,\n",
    "            metric=\"euclidean\",\n",
    "        )\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_res, y_res = sampler.fit_resample_multiclass(X_train_base, y_train_base)\n",
    "        else:\n",
    "            X_res, y_res = sampler.fit_resample(X_train_base, y_train_base)\n",
    "\n",
    "        # 2) Guardar TRAIN aumentado\n",
    "        out_dir = Path(\"../datasets/datasets_aumentados/\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fname_train = (\n",
    "            f\"pcsmote_{nombre_dataset}_\"\n",
    "            f\"D{percentil_densidad}_R{percentil_riesgo}_\"\n",
    "            f\"{Utils.tag_p(criterio_pureza)}_I{percentil_isolation_etiqueta}_train.csv\"\n",
    "        )\n",
    "        fname_train = Utils.safe_token(fname_train)\n",
    "        p_train = out_dir / fname_train\n",
    "\n",
    "        df_train = pd.DataFrame(X_res)\n",
    "        df_train[col_target] = y_res\n",
    "\n",
    "        df_train.to_csv(p_train, index=False)\n",
    "\n",
    "        return str(p_train), sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"‚ùå Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d5fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_aumentaciones_clasicas_y_guardar(\n",
    "    nombre_dataset: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    col_target: str,\n",
    "    ruta_clasicos: str = \"../datasets/datasets_aumentados/resampler_clasicos/\",\n",
    "    overwrite: bool = False,\n",
    "    percentil_isolation_etiqueta:float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera datasets aumentados con t√©cnicas cl√°sicas a partir de X_train, y_train:\n",
    "      - SMOTE\n",
    "      - BorderlineSMOTE\n",
    "      - ADASYN\n",
    "\n",
    "    Guarda los resultados en CSV en ruta_clasicos/ con nombres:\n",
    "      smote_{dataset}_train.csv\n",
    "      borderlinesmote_{dataset}_train.csv\n",
    "      adasyn_{dataset}_train.csv\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_clasicos, exist_ok=True)\n",
    "\n",
    "    columnas = list(X_train.columns)\n",
    "\n",
    "    print(f\"üîß Aumentaci√≥n cl√°sica (en memoria) para: {nombre_dataset}\")\n",
    "    print(f\"   X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "    resamplers = [\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"borderlinesmote\", BorderlineSMOTE(random_state=RANDOM_STATE, kind=\"borderline-1\")),\n",
    "        (\"adasyn\", ADASYN(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    "\n",
    "    for nombre_resampler, resampler in resamplers:\n",
    "        nombre_archivo = f\"{nombre_resampler}_{nombre_dataset}_I{percentil_isolation_etiqueta}_train.csv\"\n",
    "        path_salida = os.path.join(ruta_clasicos, nombre_archivo)\n",
    "\n",
    "        if not overwrite and os.path.exists(path_salida):\n",
    "            print(f\"   ‚ö™ Omitido ({nombre_resampler}), ya existe: {nombre_archivo}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   üîÅ Aplicando {nombre_resampler} ...\")\n",
    "\n",
    "        try:\n",
    "            # fit_resample trabaja con ndarrays\n",
    "            X_res, y_res = resampler.fit_resample(X_train.values, y_train.values)\n",
    "        except ValueError as e:\n",
    "            # Caso t√≠pico: ADASYN (u otro) decide no generar muestras\n",
    "            print(f\"   ‚ö†Ô∏è {nombre_resampler} no gener√≥ muestras sint√©ticas: {e}\")\n",
    "            print(f\"      ‚Üí Se omite guardar {nombre_resampler}_{nombre_dataset}_train.csv para este dataset.\")\n",
    "            continue\n",
    "\n",
    "        df_res = pd.DataFrame(X_res, columns=columnas)\n",
    "        df_res[col_target] = y_res  # y_res ya es array, no .values\n",
    "\n",
    "        df_res.to_csv(path_salida, index=False)\n",
    "        print(f\"   ‚úÖ Guardado: {path_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Dataset: ecoli\n",
      "\n",
      "   ‚ñ∂ Ejecutando variante con IsolationForest = 0%\n",
      "‚ö†Ô∏è Caso base ya existe para ecoli. Usando archivos existentes.\n",
      "üü¶ Caso base generado (I0):\n",
      " - Train: ../datasets/datasets_aumentados/base/ecoli_I0_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/ecoli_test.csv\n",
      "üîß Aumentaci√≥n cl√°sica (en memoria) para: ecoli\n",
      "   X_train shape: (268, 7), y_train shape: (268,)\n",
      "   üîÅ Aplicando smote ...\n",
      "   ‚ö†Ô∏è smote no gener√≥ muestras sint√©ticas: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "      ‚Üí Se omite guardar smote_ecoli_train.csv para este dataset.\n",
      "   üîÅ Aplicando borderlinesmote ...\n",
      "   ‚ö†Ô∏è borderlinesmote no gener√≥ muestras sint√©ticas: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 4, n_samples = 3\n",
      "      ‚Üí Se omite guardar borderlinesmote_ecoli_train.csv para este dataset.\n",
      "   üîÅ Aplicando adasyn ...\n",
      "   ‚ö†Ô∏è adasyn no gener√≥ muestras sint√©ticas: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 2, n_samples = 2\n",
      "      ‚Üí Se omite guardar adasyn_ecoli_train.csv para este dataset.\n",
      "#01 ‚ûï Aumentando con D=25 | R=60 | P=entropia | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': 0.5916727785823274, 'riesgo_medio': 0.49553571428571425, 'riesgo_std': 0.1987986078411814, 'densidad_media': 0.3397345823575332, 'vecinos_validos_promedio': 1.4098360655737705, 'n_candidatas': 61, 'n_filtradas': 32, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.4659729074200057, 'umbral_densidad_global': 0.4659729074200057, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4098360655737705, 'cant_min_en_p_elegido': 0.8852459016393442, 'cant_vecinos_en_p_global_promedio': 1.4098360655737705, 'cant_min_en_p_global_promedio': 0.8852459016393442}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': 0.5916727785823274, 'riesgo_medio': 0.5178571428571428, 'riesgo_std': 0.23076514255928723, 'densidad_media': 0.48112659698025545, 'vecinos_validos_promedio': 1.4390243902439024, 'n_candidatas': 41, 'n_filtradas': 16, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.45037251073466894, 'umbral_densidad_global': 0.45037251073466894, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4390243902439024, 'cant_min_en_p_elegido': 1.024390243902439, 'cant_vecinos_en_p_global_promedio': 1.4390243902439024, 'cant_min_en_p_global_promedio': 1.024390243902439}\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D25_R60_Pentropia_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D25_R60_Pentropia_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D25_R60_Pentropia_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n",
      "#02 ‚ûï Aumentando con D=25 | R=60 | P=proporcion | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': None, 'riesgo_medio': 0.49553571428571425, 'riesgo_std': 0.1987986078411814, 'densidad_media': 0.3397345823575332, 'vecinos_validos_promedio': 1.4098360655737705, 'n_candidatas': 61, 'n_filtradas': 32, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.4659729074200057, 'umbral_densidad_global': 0.4659729074200057, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4098360655737705, 'cant_min_en_p_elegido': 0.8852459016393442, 'cant_vecinos_en_p_global_promedio': 1.4098360655737705, 'cant_min_en_p_global_promedio': 0.8852459016393442, 'pureza_eps': 0.14285714285714285, 'pureza_limite_inferior': 0.14285714285714285, 'pureza_limite_superior': 0.8571428571428572}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': None, 'riesgo_medio': 0.6607142857142857, 'riesgo_std': 0.12242240357859008, 'densidad_media': 0.31845238095238093, 'vecinos_validos_promedio': 1.4285714285714286, 'n_candidatas': 28, 'n_filtradas': 8, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.5714285714285714, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.47863680325038155, 'umbral_densidad_global': 0.47863680325038155, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4285714285714286, 'cant_min_en_p_elegido': 0.5, 'cant_vecinos_en_p_global_promedio': 1.4285714285714286, 'cant_min_en_p_global_promedio': 0.5, 'pureza_eps': 0.14285714285714285, 'pureza_limite_inferior': 0.14285714285714285, 'pureza_limite_superior': 0.8571428571428572}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': None, 'riesgo_medio': 0.5178571428571428, 'riesgo_std': 0.23076514255928723, 'densidad_media': 0.48112659698025545, 'vecinos_validos_promedio': 1.4390243902439024, 'n_candidatas': 41, 'n_filtradas': 16, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.45037251073466894, 'umbral_densidad_global': 0.45037251073466894, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4390243902439024, 'cant_min_en_p_elegido': 1.024390243902439, 'cant_vecinos_en_p_global_promedio': 1.4390243902439024, 'cant_min_en_p_global_promedio': 1.024390243902439, 'pureza_eps': 0.14285714285714285, 'pureza_limite_inferior': 0.14285714285714285, 'pureza_limite_superior': 0.8571428571428572}\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D25_R60_Pproporcion_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D25_R60_Pproporcion_I0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\Documents\\UNLu\\armado-tesina\\codigo\\notebooks\\../scripts\\pc_smote.py:173: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(lista, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D25_R60_Pproporcion_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n",
      "#03 ‚ûï Aumentando con D=50 | R=60 | P=entropia | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': 0.5916727785823274, 'riesgo_medio': 0.49553571428571425, 'riesgo_std': 0.1987986078411814, 'densidad_media': 0.3397345823575332, 'vecinos_validos_promedio': 1.4098360655737705, 'n_candidatas': 61, 'n_filtradas': 32, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.4659729074200057, 'umbral_densidad_global': 0.4659729074200057, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4098360655737705, 'cant_min_en_p_elegido': 0.8852459016393442, 'cant_vecinos_en_p_global_promedio': 1.4098360655737705, 'cant_min_en_p_global_promedio': 0.8852459016393442}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D50_R60_Pentropia_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D50_R60_Pentropia_I0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\Documents\\UNLu\\armado-tesina\\codigo\\notebooks\\../scripts\\pc_smote.py:173: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(lista, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D50_R60_Pentropia_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n",
      "#04 ‚ûï Aumentando con D=50 | R=60 | P=proporcion | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': None, 'riesgo_medio': 0.49553571428571425, 'riesgo_std': 0.1987986078411814, 'densidad_media': 0.3397345823575332, 'vecinos_validos_promedio': 1.4098360655737705, 'n_candidatas': 61, 'n_filtradas': 32, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.2857142857142857, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.4659729074200057, 'umbral_densidad_global': 0.4659729074200057, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4098360655737705, 'cant_min_en_p_elegido': 0.8852459016393442, 'cant_vecinos_en_p_global_promedio': 1.4098360655737705, 'cant_min_en_p_global_promedio': 0.8852459016393442, 'pureza_eps': 0.14285714285714285, 'pureza_limite_inferior': 0.14285714285714285, 'pureza_limite_superior': 0.8571428571428572}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self_meta: {'umbral_densidad': 0.0, 'umbral_entropia': None, 'riesgo_medio': 0.6607142857142857, 'riesgo_std': 0.12242240357859008, 'densidad_media': 0.31845238095238093, 'vecinos_validos_promedio': 1.4285714285714286, 'n_candidatas': 28, 'n_filtradas': 8, 'elapsed_ms': None, 'k_efectivo': 7, 'umbral_riesgo_min': 0.5714285714285714, 'percentil_densidad_distancias_elegido': 20.0, 'valor_percentil_global_elegido': 0.47863680325038155, 'umbral_densidad_global': 0.47863680325038155, 'k_global': 7, 'cant_vecinos_en_p_elegido': 1.4285714285714286, 'cant_min_en_p_elegido': 0.5, 'cant_vecinos_en_p_global_promedio': 1.4285714285714286, 'cant_min_en_p_global_promedio': 0.5, 'pureza_eps': 0.14285714285714285, 'pureza_limite_inferior': 0.14285714285714285, 'pureza_limite_superior': 0.8571428571428572}\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D50_R60_Pproporcion_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D50_R60_Pproporcion_I0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FamiliaNatelloMedina\\Documents\\UNLu\\armado-tesina\\codigo\\notebooks\\../scripts\\pc_smote.py:173: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(lista, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D50_R60_Pproporcion_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n",
      "#05 ‚ûï Aumentando con D=75 | R=60 | P=entropia | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D75_R60_Pentropia_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D75_R60_Pentropia_I0.csv\n",
      "‚ö†Ô∏è No hay log POR MUESTRA para exportar.\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D75_R60_Pentropia_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n",
      "#06 ‚ûï Aumentando con D=75 | R=60 | P=proporcion | I=0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.5714285714285714\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "self.riesgo recibido en el construct:  60.0\n",
      "Umbral por percentil riesgo: 0.2857142857142857\n",
      "‚úÖ Guardado exitoso:\n",
      " - Train: ..\\datasets\\datasets_aumentados\\pcsmote_ecoli_D75_R60_Pproporcion_I0_train.csv\n",
      "üìÅ Log por clase guardado en: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D75_R60_Pproporcion_I0.csv\n",
      "‚ö†Ô∏è No hay log POR MUESTRA para exportar.\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_clase\\log_pcsmote_ecoli_D75_R60_Pproporcion_I0.csv\n",
      "üìÑ Log exportado: ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_ecoli.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- GRID PCSMOTE ---\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [60]#[25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "\n",
    "# ac√° se definen TODAS las variantes de IsolationForest que se quieren comparar\n",
    "percentiles_isolation = [0] #[0, 1]  # ej.: 0% y 1%; aca se puede agregar 5, etc.\n",
    "\n",
    "combinaciones = list(product(\n",
    "    percentiles_densidad,\n",
    "    percentiles_riesgo,\n",
    "    criterios_pureza,\n",
    "))\n",
    "\n",
    "# --- Rutas base ---\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "RUTA_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "os.makedirs(RUTA_CLASICOS, exist_ok=True)\n",
    "\n",
    "datasets_a_ignorar = {\n",
    "    \"eurosat\",\n",
    "    \"shuttle\",\n",
    "    \"glass\",\n",
    "    \"heart\",\n",
    "    \"wdbc\",\n",
    "    # \"ecoli\",\n",
    "    \"iris\",\n",
    "    \"us_crime\"\n",
    "}\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset in datasets_a_ignorar:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Reset del acumulador de logs POR MUESTRA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Se hace UNA sola vez por dataset para acumular:\n",
    "    #   - todas las combinaciones D/R/P\n",
    "    #   - y todos los percentiles de isolation (I0, I1, etc.)\n",
    "    if nombre_dataset in PCSMOTE._acumulador_logs_por_muestra_por_dataset:\n",
    "        del PCSMOTE._acumulador_logs_por_muestra_por_dataset[nombre_dataset]\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Bucle sobre LOS DIFERENTES percentiles de Isolation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    for percentil_isolation_cleaner in percentiles_isolation:\n",
    "        print(f\"\\n   ‚ñ∂ Ejecutando variante con IsolationForest = {percentil_isolation_cleaner}%\")\n",
    "\n",
    "        # --- CASO BASE (√∫nica lectura del dataset crudo por variante de isolation) ---\n",
    "        base_train, base_test = generar_caso_base(\n",
    "            nombre_dataset,\n",
    "            config,\n",
    "            porcentaje_limpieza=percentil_isolation_cleaner,\n",
    "        )\n",
    "        print(f\"üü¶ Caso base generado (I{percentil_isolation_cleaner}):\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "        # Cargar en memoria el TRAIN base ya escalado (y limpiado si I>0)\n",
    "        col_target = config.get(\"col_target\", \"target\")\n",
    "        df_base_train = pd.read_csv(base_train)\n",
    "\n",
    "        if col_target not in df_base_train.columns:\n",
    "            raise ValueError(f\"La columna target '{col_target}' no est√° en {base_train}\")\n",
    "\n",
    "        # Versi√≥n DataFrame/Series para los cl√°sicos\n",
    "        X_train_df = df_base_train.drop(columns=[col_target])\n",
    "        y_train_series = df_base_train[col_target]\n",
    "\n",
    "        # Versi√≥n numpy para PCSMOTE\n",
    "        X_train_base = X_train_df.values\n",
    "        y_train_base = y_train_series.values\n",
    "\n",
    "        # Para nombrar archivos (sufijo I0, I1, etc.)\n",
    "        percentil_isolation_etiqueta = percentil_isolation_cleaner\n",
    "\n",
    "        # --- AUMENTACI√ìN CL√ÅSICA (SMOTE, BorderlineSMOTE, ADASYN) ---\n",
    "        generar_aumentaciones_clasicas_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            X_train=X_train_df,\n",
    "            y_train=y_train_series,\n",
    "            col_target=col_target,\n",
    "            ruta_clasicos=RUTA_CLASICOS,\n",
    "            overwrite=False,\n",
    "            # solo se usa para el nombre de archivo: smote_dataset_I0_train.csv, etc.\n",
    "            percentil_isolation_etiqueta=percentil_isolation_etiqueta,\n",
    "        )\n",
    "\n",
    "        # --- GRID DE PCSMOTE + ISOLATION CLEANER SOBRE ESTE TRAIN BASE ---        \n",
    "        for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "\n",
    "            print(\n",
    "                f\"#{idx:02d} ‚ûï Aumentando con \"\n",
    "                f\"D={pdens} | R={priesgo} | P={criterio} | I={percentil_isolation_cleaner}\"\n",
    "            )\n",
    "\n",
    "            path_train, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "                nombre_dataset=nombre_dataset,\n",
    "                X_train_base=X_train_base,\n",
    "                y_train_base=y_train_base,\n",
    "                percentil_densidad=pdens,\n",
    "                percentil_riesgo=priesgo,\n",
    "                criterio_pureza=criterio,\n",
    "                # solo para nombre del archivo pcsmote_{dataset}_..._I*_train.csv\n",
    "                percentil_isolation_etiqueta=percentil_isolation_etiqueta,\n",
    "                col_target=col_target,\n",
    "            )\n",
    "\n",
    "            if path_train is None or sampler is None:\n",
    "                print(\"‚ùå Fall√≥ la generaci√≥n.\")\n",
    "                continue\n",
    "\n",
    "            sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "            nombre_configuracion = (\n",
    "                f\"D{int(pdens)}_R{int(priesgo)}\"\n",
    "                f\"_{criterio}_I{percentil_isolation_cleaner}\"\n",
    "            )\n",
    "            sampler.nombre_configuracion = nombre_configuracion\n",
    "\n",
    "            print(f\"‚úÖ Guardado exitoso:\\n - Train: {path_train}\")\n",
    "\n",
    "            # --- rutas de logs (sanitizadas) ---\n",
    "            base_logs = Path(\"../datasets/datasets_aumentados/logs/pcsmote\")\n",
    "            p_tag = Utils.tag_p(criterio)  # \"Pentropia\"/\"Pproporcion\" \n",
    "\n",
    "            # Log por clase: se diferencia POR configuraci√≥n, incluyendo I\n",
    "            fname_clase = Utils.safe_token(\n",
    "                f\"log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_{p_tag}_I{percentil_isolation_cleaner}.csv\"\n",
    "            )\n",
    "\n",
    "            # Log por muestra: UN SOLO EXCEL POR DATASET\n",
    "            # Dentro del Excel se distinguen:\n",
    "            #   - configuraciones D/R/P/I por columna 'configuracion'\n",
    "            fname_muestra = Utils.safe_token(\n",
    "                f\"log_pcsmote_x_muestra_{nombre_dataset}.xlsx\"\n",
    "            )\n",
    "\n",
    "            log_path_x_clase = str(base_logs / \"por_clase\" / fname_clase)\n",
    "            log_path_x_muestras = str(base_logs / \"por_muestras\" / fname_muestra)\n",
    "\n",
    "            # Asegurar subdirectorios de logs\n",
    "            os.makedirs(base_logs / \"por_clase\", exist_ok=True)\n",
    "            os.makedirs(base_logs / \"por_muestras\", exist_ok=True)\n",
    "\n",
    "            # --- export logs (CSV/Excel) ---\n",
    "            sampler.exportar_log_csv(log_path_x_clase)\n",
    "            sampler.exportar_log_muestras_excel(log_path_x_muestras)\n",
    "\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_clase}\")\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_muestras}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
