{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from custom_samplers import PCSMOTEWrapper                     # Wrapper personalizado para la t√©cnica PCSMOTE\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "from graficador2d import Graficador2D                       # Clase para graficar resultados en 2D\n",
    "from isolation_cleaner import IsolationCleaner                 # Clase para limpieza de outliers con Isolation Forest\n",
    "from Utils import Utils                                       # Clase utilitaria con funciones auxiliares\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "from pathlib import Path\n",
    "\n",
    "import traceback\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- funci√≥n: generar caso base (train/test sin sobremuestreo) ---\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.30,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin PCSMOTE) para un dataset: guarda train y test en ruta_base.\n",
    "    Usa la misma l√≥gica de carga que el resto del pipeline (cargar_dataset + LabelEncoder opcional).\n",
    "\n",
    "    Retorna:\n",
    "        (path_train, path_test)\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        return path_train, path_test  # ya generado\n",
    "\n",
    "    # 1) Cargar dataset base con tu helper habitual\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\", \",\"),\n",
    "        header=config.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=config.get(\"tipo\", \"tabular\")\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar etiquetas num√©ricas si vienen como strings/objects\n",
    "    if getattr(y, \"dtype\", None) == object or (len(y) > 0 and isinstance(y[0], str)):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # 3) Split estratificado (mismo seed para reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # 4) Guardar CSVs\n",
    "    pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_train, index=False)\n",
    "    pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name=config.get(\"col_target\", \"target\"))], axis=1)\\\n",
    "      .to_csv(path_test, index=False)\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(nombre_dataset, config, percentil_densidad, percentil_riesgo, criterio_pureza, test_size=0.2):\n",
    "    print(f\"üìÇ Cargando dataset: {nombre_dataset}\")\n",
    "\n",
    "    try:\n",
    "        # 1) Cargar dataset original\n",
    "        X, y, clases = cargar_dataset(\n",
    "            path=config[\"path\"],\n",
    "            clase_minoria=config.get(\"clase_minoria\"),\n",
    "            col_features=config.get(\"col_features\"),\n",
    "            col_target=config.get(\"col_target\"),\n",
    "            sep=config.get(\"sep\", \",\"),\n",
    "            header=config.get(\"header\", None),\n",
    "            binarizar=False,\n",
    "            tipo=config.get(\"tipo\", \"tabular\"),\n",
    "            impute='median',\n",
    "            na_values=('?', 'NA', 'None')\n",
    "        )\n",
    "\n",
    "        # 2) Codificar etiquetas si son strings\n",
    "        le = None\n",
    "        if y.dtype == object or isinstance(y[0], str):\n",
    "            le = LabelEncoder().fit(y)\n",
    "            y = le.transform(y)\n",
    "\n",
    "        # 3) Si es un dataset de im√°genes, vectorizar\n",
    "        if config.get(\"tipo\") == \"imagen\":\n",
    "            X = X.reshape((X.shape[0], -1)).astype(np.float32)\n",
    "\n",
    "        # 4) Escalar TODO antes de dividir\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        # 5) Split estratificado\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # 5.1) IsolationForest (opcional, por clase por defecto)\n",
    "        X_train_clean, y_train_clean, info = IsolationCleaner.limpiarOutliers(\n",
    "            X_train, y_train,\n",
    "            contamination=\"auto\",\n",
    "            random_state=42,\n",
    "            normalizar_scores=True,      # opcional; dejalo True si quer√©s comparabilidad entre datasets\n",
    "            percentil_umbral=5, # elimino el 5% m√°s bajo de cada clase\n",
    "            devolver_info=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        print(\"Umbrales aplicados:\", info[\"umbral_por_clase\"])\n",
    "\n",
    "        # Sanity check pre-oversampling\n",
    "        from collections import Counter\n",
    "        print(\"üìä Train limpio (conteos):\", Counter(y_train_clean))\n",
    "\n",
    "        # 6) PCSMOTE sobre TRAIN LIMPIO\n",
    "        print(f\"üß¨ Aplicando PCSMOTE | Densidad: {percentil_densidad} | Riesgo: {percentil_riesgo} | Pureza: {criterio_pureza}\")\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42, #1 random_state\n",
    "            percentil_densidad=percentil_densidad, #2 percentil_densidad\n",
    "            percentil_entropia=75 if criterio_pureza == 'entropia' else None, #4 percentil_entropia\n",
    "            percentil_riesgo = percentil_riesgo, # aproxima el criterio de Borderline-SMOTE de ‚Äú‚â• 0.5‚Äù \n",
    "            criterio_pureza=criterio_pureza, #5 criterio_pureza\n",
    "            factor_equilibrio=config.get(\"factor_equilibrio\", 1), #6 factor_equilibrio\n",
    "            metric=config.get(\"metric\", \"euclidean\") #7 metrica elegida\n",
    "        )\n",
    "        sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "        # Descomentar si no quiero limpieza -- NO LIMPIEZA --\n",
    "        # X_train_clean, y_train_clean = X_train, y_train\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_train_res, y_train_res = sampler.fit_resample_multiclass(X_train_clean, y_train_clean)\n",
    "        else:\n",
    "            X_train_res, y_train_res = sampler.fit_resample(X_train_clean, y_train_clean)\n",
    "\n",
    "        print(\"üìä Train aumentado (conteos):\", Counter(y_train_res))\n",
    "\n",
    "        # 6.1) Plot: TRAIN LIMPIO vs TRAIN AUMENTADO (no X completo)\n",
    "        idx_removed = info[\"idx_removed\"]  # √≠ndices respecto de X_train\n",
    "\n",
    "        grafo = Graficador2D(\n",
    "            reductor=\"pca\",\n",
    "            semilla=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_riesgo=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            nombre_dataset=nombre_dataset,\n",
    "        )\n",
    "\n",
    "        X_syn, y_syn = sampler.get_sinteticas()\n",
    "        \n",
    "        grafo.trazar_original_clean_aumentado(\n",
    "            X_orig=X_train, y_orig=y_train,\n",
    "            idx_removed=idx_removed,\n",
    "            X_clean=X_train_clean, y_clean=y_train_clean,\n",
    "            X_res=X_train_res, y_res=y_train_res,\n",
    "            titulo=\"Train limpio vs. Train aumentado\",\n",
    "            nombres_clase=clases,\n",
    "            X_syn=X_syn, y_syn=y_syn,\n",
    "        )\n",
    "\n",
    "        # 7) Guardar datasets: train aumentado\n",
    "        print(\"üíæ Guardando datasets aumentados...\")\n",
    "\n",
    "        out_dir = (Path.cwd().parent / \"datasets\" / \"datasets_aumentados\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fname_train = f\"pcsmote_{nombre_dataset}_D{percentil_densidad}_R{percentil_riesgo}_{Utils.tag_p(criterio_pureza)}_train.csv\"\n",
    "        fname_train = Utils.safe_token(fname_train)         \n",
    "        p_train = (out_dir / fname_train)                   \n",
    "\n",
    "        df_train = pd.DataFrame(X_train_res)\n",
    "        df_train[\"target\"] = y_train_res\n",
    "\n",
    "        print(f\"Verificacion de ruta p_train - Train aumentado: {p_train}\")\n",
    "\n",
    "        with open(p_train, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "            df_train.to_csv(fh, index=False)\n",
    "\n",
    "        # ---------- TEST (sin tocar) ----------\n",
    "        fname_test = f\"pcsmote_{nombre_dataset}_D{percentil_densidad}_R{percentil_riesgo}_{Utils.tag_p(criterio_pureza)}_test.csv\"\n",
    "        fname_test = Utils.safe_token(fname_test)\n",
    "        p_test = (out_dir / fname_test)\n",
    "\n",
    "        df_test = pd.DataFrame(X_test)\n",
    "        df_test[\"target\"] = y_test\n",
    "\n",
    "        print(f\"Verificacion de ruta p_test - Test original: {p_test}\")\n",
    "        with open(p_test, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "            df_test.to_csv(fh, index=False)\n",
    "\n",
    "\n",
    "        return str(p_train), sampler    \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"‚ùå Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "combinaciones = list(product(percentiles_densidad, percentiles_riesgo, criterios_pureza))\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "umbrales_por_dataset = {\n",
    "    \"shuttle\": {\"1\": -0.108, \"2\": -0.446, \"3\":  0.006, \"4\":  0.125, \"5\":  0.138, \"6\": -0.779, \"7\": -0.921},\n",
    "    \"wdbc\":    {\"B\": -0.029, \"M\": -0.264},\n",
    "    \"glass\":   {\"1\": -0.859, \"2\": -0.319, \"3\": -0.982, \"5\": -0.429, \"7\": -0.824},\n",
    "    \"heart\":   {\"0\": -0.470, \"1\": -0.742, \"2\": -0.824, \"3\": -0.552, \"4\": -0.727},\n",
    "    \"iris\":    {\"setosa\": -0.776, \"versicolor\": -0.776, \"virginica\": -0.776},\n",
    "    \"ecoli\":   {\"imS\": -0.315, \"imL\": -0.432, \"imU\": -0.289, \"imT\": -0.401, \"imC\": -0.567, \"imW\": -0.524, \"imF\": -0.486},\n",
    "}\n",
    "\n",
    "datasets_a_ignorar = {\n",
    "        \"eurosat\", \n",
    "        \"shuttle\", \n",
    "        # \"glass\", \n",
    "        \"heart\", \n",
    "        \"wdbc\",\n",
    "        \"ecoli\",\n",
    "        \"iris\"\n",
    "        }\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset in datasets_a_ignorar:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # --- CASO BASE ---\n",
    "    base_train, base_test = generar_caso_base(nombre_dataset, config)\n",
    "    print(f\"üü¶ Caso base generado:\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "    # --- GRID DE PCSMOTE ---\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        print(f\"#{idx:02d} ‚ûï Aumentando con D={pdens} | R={priesgo} | P={criterio}\")\n",
    "\n",
    "        path_train, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            config=config,\n",
    "            percentil_densidad=pdens,\n",
    "            percentil_riesgo=priesgo,\n",
    "            criterio_pureza=criterio,\n",
    "            # umbrales_por_dataset=umbrales_por_dataset,\n",
    "        )\n",
    "\n",
    "        if path_train and sampler:\n",
    "            print(f\"‚úÖ Guardado exitoso:\\n - Train: {path_train}\")\n",
    "            # --- rutas de logs (sanitizadas) ---\n",
    "            base_logs = Path(\"../datasets/datasets_aumentados/logs/pcsmote\")\n",
    "            p_tag = Utils.tag_p(criterio)                          # \"Pentropia\"/\"Pproporcion\"\n",
    "            fname_clase   = Utils.safe_token(f\"log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_P{p_tag}.csv\")\n",
    "            fname_muestra = Utils.safe_token(f\"log_pcsmote_x_muestra_{nombre_dataset}_D{pdens}_R{priesgo}_P{p_tag}.xlsx\")\n",
    "\n",
    "            log_path_x_clase    = str(base_logs / \"por_clase\"   / fname_clase)\n",
    "            log_path_x_muestras = str(base_logs / \"por_muestras\"/ fname_muestra)\n",
    "\n",
    "            # --- export logs (CSV/Excel) ---\n",
    "            sampler.exportar_log_csv(log_path_x_clase)\n",
    "\n",
    "            # si quer√©s mantener la versi√≥n debug, OK; si no:\n",
    "            sampler.exportar_log_muestras_excel(log_path_x_muestras)\n",
    "            # sampler.exportar_log_muestras_excel_debug(log_path_x_muestras)\n",
    "\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_clase}\")\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_muestras}\")\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Fall√≥ la generaci√≥n.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
