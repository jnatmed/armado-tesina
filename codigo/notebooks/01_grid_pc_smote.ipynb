{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset                      # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                    # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                 # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from pc_smote import PCSMOTE                                   # Implementaci√≥n principal de PCSMOTE\n",
    "from graficador2d import Graficador2D                       # Clase para graficar resultados en 2D\n",
    "from isolation_cleaner import IsolationCleaner                 # Clase para limpieza de outliers con Isolation Forest\n",
    "from Utils import Utils                                       # Clase utilitaria con funciones auxiliares\n",
    "from limpiador import LimpiadorOutliers                                 # Clase para limpieza de datos\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "from pathlib import Path\n",
    "\n",
    "import traceback\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "RUTA_CLASICOS: str = \"../datasets/datasets_aumentados/resampler_clasicos/\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False,\n",
    "    porcentaje_limpieza: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin sobremuestreo) aplicando:\n",
    "      1) Split train/test\n",
    "      2) RobustScaler (fit SOLO en X_train, transform en X_train/X_test)\n",
    "      3) (Opcional) Limpieza de outliers con IsolationForest sobre X_train_scaled\n",
    "\n",
    "    IMPORTANTE:\n",
    "    - Aqu√≠ `porcentaje_limpieza` se usa para LIMPIAR realmente el conjunto\n",
    "      de entrenamiento con IsolationForest (si > 0).\n",
    "    - El valor de `porcentaje_limpieza` tambi√©n se refleja en el nombre del\n",
    "      archivo de TRAIN como sufijo `I{porcentaje_limpieza}` para dejar traza.\n",
    "    - El TEST nunca se limpia con IsolationForest, por eso su nombre no lleva\n",
    "      sufijo `I*`.\n",
    "\n",
    "    El resultado son dos CSV:\n",
    "      - {ruta_base}/{nombre_dataset}_I{porcentaje_limpieza}_train.csv\n",
    "        (train escalado y, si corresponde, limpiado con IFOR)\n",
    "      - {ruta_base}/{nombre_dataset}_test.csv\n",
    "        (test escalado, sin limpieza IFOR)\n",
    "\n",
    "    Estos archivos se usan como base tanto para:\n",
    "      - el caso base (sin sobremuestreo),\n",
    "      - las t√©cnicas cl√°sicas (SMOTE, BorderlineSMOTE, ADASYN),\n",
    "      - y PC-SMOTE.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    path_train = os.path.join(ruta_base, f\"{nombre_dataset}_I{porcentaje_limpieza}_train.csv\")\n",
    "    path_test  = os.path.join(ruta_base, f\"{nombre_dataset}_test.csv\")\n",
    "\n",
    "    if not overwrite and os.path.exists(path_train) and os.path.exists(path_test):\n",
    "        print(f\"‚ö†Ô∏è Caso base ya existe para {nombre_dataset}. Usando archivos existentes.\")\n",
    "        return path_train, path_test\n",
    "\n",
    "    # 1) Cargar dataset crudo seg√∫n config_datasets\n",
    "    df = cargar_dataset(nombre_dataset, config)\n",
    "\n",
    "    col_target   = config[\"col_target\"]\n",
    "    col_features = config[\"col_features\"]\n",
    "\n",
    "    X = df[col_features]\n",
    "    y = df[col_target]\n",
    "\n",
    "    # 2) Train / test split (estratificado)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    print(f\"[{nombre_dataset}] Split: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # 3) Escalado robusto (fit en train, transform en train y test)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # 4) (Opcional) Limpieza de outliers con IsolationForest SOLO sobre TRAIN\n",
    "    if porcentaje_limpieza is not None and porcentaje_limpieza > 0:\n",
    "        print(f\"[{nombre_dataset}] Aplicando IsolationForest sobre TRAIN (porcentaje={porcentaje_limpieza}%)\")\n",
    "\n",
    "        X_train_scaled, y_train, info_limpieza = IsolationCleaner.limpiarOutliers(\n",
    "            X=X_train_scaled,\n",
    "            y=y_train.values if isinstance(y_train, pd.Series) else y_train,\n",
    "\n",
    "            # par√°metros reales de la clase:\n",
    "            percentil_umbral=porcentaje_limpieza,   # este S√ç define el corte real\n",
    "            contamination=\"auto\",\n",
    "            n_estimators=200,\n",
    "            max_samples=\"auto\",\n",
    "            random_state=random_state,\n",
    "            bootstrap=False,\n",
    "            normalizar_scores=False,\n",
    "            devolver_info=True,                     # para obtener el diccionario info_limpieza\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        removed_total = info_limpieza.get(\"removed_total\", 0)\n",
    "        total_inicial = removed_total + len(y_train)\n",
    "        print(\n",
    "            f\"[{nombre_dataset}] Limpieza IF (percentil): \"\n",
    "            f\"removidos={removed_total} / total_inicial‚âà{total_inicial}\"\n",
    "        )\n",
    "        # --------------------------------------------------\n",
    "\n",
    "    # 5) Reconstruir DataFrames con columnas\n",
    "    df_train = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_train_scaled, columns=col_features),\n",
    "            pd.Series(\n",
    "                y_train,\n",
    "                name=col_target,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_test = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_test_scaled, columns=col_features),\n",
    "            pd.Series(\n",
    "                y_test.values if isinstance(y_test, pd.Series) else y_test,\n",
    "                name=col_target,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # 6) Guardar CSV base\n",
    "    df_train.to_csv(path_train, index=False)\n",
    "    df_test.to_csv(path_test, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Caso base generado para {nombre_dataset}\")\n",
    "    print(f\"   Train: {path_train}  (rows={df_train.shape[0]})\")\n",
    "    print(f\"   Test : {path_test}   (rows={df_test.shape[0]})\")\n",
    "\n",
    "    return path_train, path_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar(\n",
    "    nombre_dataset,\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    percentil_densidad,\n",
    "    percentil_riesgo,\n",
    "    criterio_pureza,\n",
    "    col_target=\"target\",\n",
    "    percentil_isolation_etiqueta:float = 0.0\n",
    "):\n",
    "    try:\n",
    "        # 1) PCSMOTE sobre el TRAIN limpio (ya escalado)\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            percentil_densidad=percentil_densidad,\n",
    "            percentil_entropia=75 if criterio_pureza == \"entropia\" else None,\n",
    "            percentil_riesgo=percentil_riesgo,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            factor_equilibrio=1,\n",
    "            metric=\"euclidean\",\n",
    "        )\n",
    "\n",
    "        if hasattr(sampler, \"fit_resample_multiclass\"):\n",
    "            X_res, y_res = sampler.fit_resample_multiclass(X_train_base, y_train_base)\n",
    "        else:\n",
    "            X_res, y_res = sampler.fit_resample(X_train_base, y_train_base)\n",
    "\n",
    "        # 2) Guardar TRAIN aumentado\n",
    "        out_dir = Path(\"../datasets/datasets_aumentados/\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fname_train = (\n",
    "            f\"pcsmote_{nombre_dataset}_\"\n",
    "            f\"D{percentil_densidad}_R{percentil_riesgo}_\"\n",
    "            f\"{Utils.tag_p(criterio_pureza)}_I{percentil_isolation_etiqueta}_train.csv\"\n",
    "        )\n",
    "        fname_train = Utils.safe_token(fname_train)\n",
    "        p_train = out_dir / fname_train\n",
    "\n",
    "        df_train = pd.DataFrame(X_res)\n",
    "        df_train[col_target] = y_res\n",
    "\n",
    "        df_train.to_csv(p_train, index=False)\n",
    "\n",
    "        return str(p_train), sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"‚ùå Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_aumentaciones_clasicas_y_guardar(\n",
    "    nombre_dataset: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    col_target: str,\n",
    "    ruta_clasicos: str = \"../datasets/datasets_aumentados/resampler_clasicos/\",\n",
    "    overwrite: bool = False,\n",
    "    percentil_isolation_etiqueta:float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera datasets aumentados con t√©cnicas cl√°sicas a partir de X_train, y_train:\n",
    "      - SMOTE\n",
    "      - BorderlineSMOTE\n",
    "      - ADASYN\n",
    "\n",
    "    Guarda los resultados en CSV en ruta_clasicos/ con nombres:\n",
    "      smote_{dataset}_train.csv\n",
    "      borderlinesmote_{dataset}_train.csv\n",
    "      adasyn_{dataset}_train.csv\n",
    "    \"\"\"\n",
    "    os.makedirs(ruta_clasicos, exist_ok=True)\n",
    "\n",
    "    columnas = list(X_train.columns)\n",
    "\n",
    "    print(f\"üîß Aumentaci√≥n cl√°sica (en memoria) para: {nombre_dataset}\")\n",
    "    print(f\"   X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "    resamplers = [\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"borderlinesmote\", BorderlineSMOTE(random_state=RANDOM_STATE, kind=\"borderline-1\")),\n",
    "        (\"adasyn\", ADASYN(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    "\n",
    "    for nombre_resampler, resampler in resamplers:\n",
    "        nombre_archivo = f\"{nombre_resampler}_{nombre_dataset}_I{percentil_isolation_etiqueta}_train.csv\"\n",
    "        path_salida = os.path.join(ruta_clasicos, nombre_archivo)\n",
    "\n",
    "        if not overwrite and os.path.exists(path_salida):\n",
    "            print(f\"   ‚ö™ Omitido ({nombre_resampler}), ya existe: {nombre_archivo}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   üîÅ Aplicando {nombre_resampler} ...\")\n",
    "\n",
    "        try:\n",
    "            # fit_resample trabaja con ndarrays\n",
    "            X_res, y_res = resampler.fit_resample(X_train.values, y_train.values)\n",
    "        except ValueError as e:\n",
    "            # Caso t√≠pico: ADASYN (u otro) decide no generar muestras\n",
    "            print(f\"   ‚ö†Ô∏è {nombre_resampler} no gener√≥ muestras sint√©ticas: {e}\")\n",
    "            print(f\"      ‚Üí Se omite guardar {nombre_resampler}_{nombre_dataset}_train.csv para este dataset.\")\n",
    "            continue\n",
    "\n",
    "        df_res = pd.DataFrame(X_res, columns=columnas)\n",
    "        df_res[col_target] = y_res  # y_res ya es array, no .values\n",
    "\n",
    "        df_res.to_csv(path_salida, index=False)\n",
    "        print(f\"   ‚úÖ Guardado: {path_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GRID PCSMOTE ---\n",
    "percentiles_densidad = [25, 50, 75]\n",
    "percentiles_riesgo = [25, 50, 75]\n",
    "criterios_pureza = [\"entropia\", \"proporcion\"]\n",
    "percentil_isolation_cleaner = 0\n",
    "\n",
    "combinaciones = list(product(\n",
    "    percentiles_densidad,\n",
    "    percentiles_riesgo,\n",
    "    criterios_pureza,\n",
    "))\n",
    "\n",
    "# --- Rutas base ---\n",
    "os.makedirs(\"../logs/\", exist_ok=True)\n",
    "\n",
    "RUTA_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "os.makedirs(RUTA_CLASICOS, exist_ok=True)\n",
    "\n",
    "datasets_a_ignorar = {\n",
    "    \"eurosat\",\n",
    "    \"shuttle\",\n",
    "    \"glass\",\n",
    "    \"heart\",\n",
    "    \"wdbc\",\n",
    "    # \"ecoli\",\n",
    "    \"iris\",\n",
    "    \"us_crime\"\n",
    "}\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset in datasets_a_ignorar:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    # --- CASO BASE (√∫nica lectura del dataset crudo) ---\n",
    "    base_train, base_test = generar_caso_base(nombre_dataset, config, porcentaje_limpieza=percentil_isolation_cleaner)\n",
    "    print(f\"üü¶ Caso base generado:\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "    # Cargar en memoria el TRAIN base ya escalado\n",
    "    col_target = config.get(\"col_target\", \"target\")\n",
    "    df_base_train = pd.read_csv(base_train)\n",
    "\n",
    "    if col_target not in df_base_train.columns:\n",
    "        raise ValueError(f\"La columna target '{col_target}' no est√° en {base_train}\")\n",
    "\n",
    "    # Versi√≥n DataFrame/Series para los cl√°sicos\n",
    "    X_train_df = df_base_train.drop(columns=[col_target])\n",
    "    y_train_series = df_base_train[col_target]\n",
    "\n",
    "    # Versi√≥n numpy para PCSMOTE (como ya lo usabas)\n",
    "    X_train_base = X_train_df.values\n",
    "    y_train_base = y_train_series.values\n",
    "\n",
    "    percentil_isolation_etiqueta  = percentil_isolation_cleaner\n",
    "\n",
    "    # --- AUMENTACI√ìN CL√ÅSICA (SMOTE, BorderlineSMOTE, ADASYN) ---\n",
    "    generar_aumentaciones_clasicas_y_guardar(\n",
    "        nombre_dataset=nombre_dataset,\n",
    "        X_train=X_train_df,\n",
    "        y_train=y_train_series,\n",
    "        col_target=col_target,\n",
    "        ruta_clasicos=RUTA_CLASICOS,\n",
    "        overwrite=False,\n",
    "        percentil_isolation_etiqueta=percentil_isolation_etiqueta, # se lo paso al metodo, que solo lo va a usar para nombrar los archivos, y queda porejemplo adasyn_ecoli_I0_train.csv         \n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # --- GRID DE PCSMOTE + ISOLATION CLEANER SOBRE ESTE TRAIN BASE ---\n",
    "    for idx, (pdens, priesgo, criterio) in enumerate(combinaciones, start=1):\n",
    "        print(\n",
    "            f\"#{idx:02d} ‚ûï Aumentando con \"\n",
    "            f\"D={pdens} | R={priesgo} | P={criterio} | I={percentil_isolation_cleaner}\"\n",
    "        )\n",
    "\n",
    "        path_train, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "            nombre_dataset=nombre_dataset,\n",
    "            X_train_base=X_train_base,\n",
    "            y_train_base=y_train_base,\n",
    "            percentil_densidad=pdens,\n",
    "            percentil_riesgo=priesgo,\n",
    "            criterio_pureza=criterio,\n",
    "            percentil_isolation_etiqueta=percentil_isolation_etiqueta, # se lo paso al metodo, que solo lo va a usar para nombrar los archivos, y queda porejemplo pcsmote_ecoli_D25_R75_Pproporcion_I0_train.csv \n",
    "            col_target=col_target,\n",
    "        )\n",
    "\n",
    "        sampler.nombre_dataset = nombre_dataset\n",
    "\n",
    "        nombre_configuracion = f\"D{int(pdens)}_R{int(priesgo)}\" \\\n",
    "                            f\"_{criterio}_I{percentil_isolation_cleaner}\"\n",
    "\n",
    "        sampler.nombre_configuracion = nombre_configuracion\n",
    "\n",
    "        if path_train and sampler:\n",
    "            print(f\"‚úÖ Guardado exitoso:\\n - Train: {path_train}\")\n",
    "\n",
    "            # --- rutas de logs (sanitizadas) ---\n",
    "            base_logs = Path(\"../datasets/datasets_aumentados/logs/pcsmote\")\n",
    "            p_tag = Utils.tag_p(criterio)  # \"Pentropia\"/\"Pproporcion\"\n",
    "\n",
    "            fname_clase = Utils.safe_token(\n",
    "                f\"log_pcsmote_{nombre_dataset}_D{pdens}_R{priesgo}_{p_tag}_I{percentil_isolation_cleaner}.csv\"\n",
    "            )\n",
    "            # Despu√©s (un solo archivo por dataset)\n",
    "            fname_muestra = Utils.safe_token(\n",
    "                f\"log_pcsmote_x_muestra_{nombre_dataset}.xlsx\"\n",
    "            )\n",
    "\n",
    "\n",
    "            log_path_x_clase = str(base_logs / \"por_clase\" / fname_clase)\n",
    "            log_path_x_muestras = str(base_logs / \"por_muestras\" / fname_muestra)\n",
    "\n",
    "            # Asegurar subdirectorios de logs\n",
    "            os.makedirs(base_logs / \"por_clase\", exist_ok=True)\n",
    "            os.makedirs(base_logs / \"por_muestras\", exist_ok=True)\n",
    "\n",
    "            # --- export logs (CSV/Excel) ---\n",
    "            sampler.exportar_log_csv(log_path_x_clase)\n",
    "            sampler.exportar_log_muestras_excel(log_path_x_muestras)\n",
    "\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_clase}\")\n",
    "            print(f\"üìÑ Log exportado: {log_path_x_muestras}\")\n",
    "        else:\n",
    "            print(\"‚ùå Fall√≥ la generaci√≥n.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
